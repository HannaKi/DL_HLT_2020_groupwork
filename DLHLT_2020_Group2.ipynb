{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HannaKi/DL_HLT_2020_groupwork/blob/master/DLHLT_2020_Group2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T80c1sLBZXZy"
   },
   "source": [
    "# Deep Learning in Human Language Technologies 2020\n",
    "# Text classification project 12.5.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CslrZI_PZXZ1"
   },
   "source": [
    "#### Group 2: Hanna Kitti, Dang Trinh Ha, Timo Jokela\n",
    "\n",
    "Contributions:\n",
    "This project was a joint effort by all the participants, all three of us dedicating a considerable amount of time for the various phases of the project. The amount of work done can be roughly estimated as: Kitti 35 %, Ha 30 %, Jokela 35 %.\n",
    "\n",
    "Over the course of solving the milestones we changed ideas and discussed different ways to solve separate sub tasks, problems and frustration that occasionally rose. This mutual support can not be evidently seen from the code but it definitely guided us to the final solutions and results we gained.\n",
    "\n",
    "What follows is a **very approximate** description of the contributions.  \n",
    "Lead: the person primarily responsible for the implementation  \n",
    "Support: Mentoring, problem solving, testing with alternative parameters  \n",
    "\n",
    "**Milestone 1:**  \n",
    "Jokela: Lead  \n",
    "Kitti: Support  \n",
    "Ha: Support  \n",
    "\n",
    "**Milestone 2:**  \n",
    "Kitti: Lead (Keras BERT), support  \n",
    "Jokela: Lead (PyTorch BERT), support  \n",
    "Ha: Support  \n",
    "\n",
    "**Milestone 3:**  \n",
    "Kitti: Lead  \n",
    "Jokela: Support  \n",
    "Ha: Support  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCUw35SnZXZ5"
   },
   "source": [
    "### System dependent configuration\n",
    "\n",
    "**NOTE: This notebook is composed of pieces obtained during several kernel instances.** Different platforms (both desktop and Colab) encountered memory allocation issues when attempting to execute the notebook during a single kernel instance, mainly due to BERT models being very resource hungry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9XY6Hc7AZXZ-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Are we running in a Colab environment?\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Do we want to omit some tedious tasks? (Embedding generation, GridSearchCV)\n",
    "USE_PRECOMPUTED_FILES = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "    !pip3 install keras-bert\n",
    "    !pip install scikit-multilearn\n",
    "    \n",
    "    # Download development data\n",
    "    !wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
    "    # Download test data\n",
    "    !wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
    "    # Download train data\n",
    "    !wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv\n",
    "  \n",
    "    # Download pretrained TurkuNLP FinBERT from: https://github.com/TurkuNLP/FinBERT\n",
    "    !wget -nc http://dl.turkunlp.org/finbert/bert-base-finnish-cased-v1.zip\n",
    "    # Give -n argument so that existing files aren't overwritten \n",
    "    !unzip -n bert-base-finnish-cased-v1.zip\n",
    "\n",
    "\n",
    "    if USE_PRECOMPUTED_FILES:\n",
    "        # Download pre-computed FastText embedding matrix \n",
    "        !wget https://seafile.utu.fi/f/75d03975f06d4784b23e/?dl=1 -O rnn_embedding_matrix.p\n",
    "        # Download pre-computed BOW GSCV results\n",
    "        !wget https://seafile.utu.fi/f/975f0bf59a2f48bab603/?dl=1 -O bow_grid_results.p\n",
    "    \n",
    "    bert_vocab_path = 'bert-base-finnish-cased-v1/vocab.txt'\n",
    "    bert_config_path = 'bert-base-finnish-cased-v1/bert_config.json'\n",
    "    bert_checkpoint_path = 'bert-base-finnish-cased-v1/bert_model.ckpt'\n",
    "    bow_model_filepath = 'bow_model.h5'\n",
    "    rnn_model_filepath = 'rnn_model.h5'\n",
    "    \n",
    "    embedding_matrix_file = Path('rnn_embedding_matrix.p')\n",
    "    bow_grid_file = Path('bow_grid_results.p')\n",
    "    \n",
    "    train_file = 'fincore-train.tsv'\n",
    "    dev_file = 'fincore-dev.tsv'\n",
    "    test_file = 'fincore-test.tsv'\n",
    "    \n",
    "    # Milestone 2: training batch size\n",
    "    MS2_BATCH_SIZE = 16\n",
    "    \n",
    "else:\n",
    "    bow_model_filepath = '/tmp/bow_model.h5'\n",
    "    rnn_model_filepath = '/tmp/rnn_model.h5'\n",
    "    \n",
    "    bert_base_dir = Path.home() / 'Documents/BERT/bert-base-finnish-cased-v1'\n",
    "    bert_vocab_path = str(bert_base_dir / 'vocab.txt')\n",
    "    bert_config_path = str(bert_base_dir / 'bert_config.json')\n",
    "    bert_checkpoint_path = str(bert_base_dir / 'bert_model.ckpt')\n",
    "    \n",
    "    embedding_matrix_file = Path.home() / 'Documents/DLHLT/rnn_embedding_matrix.p'\n",
    "    bow_grid_file = Path.home() / 'Documents/DLHLT/bow_grid_results.p'\n",
    "    train_file = Path.home() / 'Documents/DLHLT/fincore-train.tsv'\n",
    "    dev_file = Path.home() / 'Documents/DLHLT/fincore-dev.tsv'\n",
    "    test_file = Path.home() / 'Documents/DLHLT/fincore-test.tsv'\n",
    "    \n",
    "    # Milestone 2: training batch size\n",
    "    MS2_BATCH_SIZE = 4\n",
    "    \n",
    "keras_bert_results_file = Path.home() / 'Documents/DLHLT/keras_bert_results.p'\n",
    "pytorch_bert_results_file = Path.home() / 'Documents/DLHLT/pytorch_bert_results.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaGwlSMCZXaV"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuhNal3AZXaX"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTNIgqhrZXaZ"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import cuda\n",
    "from pytorch_pretrained_bert import (\n",
    "    BertTokenizer as PTBertTokenizer,  # prefix all with PT for consistency\n",
    "    BertConfig as PTBertConfig,\n",
    ")\n",
    "from pytorch_pretrained_bert import (\n",
    "    BertAdam as PTBertAdam, \n",
    "    BertForSequenceClassification as PTBertForSequenceClassification\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    multilabel_confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder, MultiLabelBinarizer, StandardScaler\n",
    ")\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqlzwZ-nZXan"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0CEeEPEZXaq"
   },
   "source": [
    "Data split\n",
    "\n",
    "- Training set: used for training the models\n",
    "- Validation (or model assessment) set: when comparing\n",
    "different models trained on training set, select one with lowest\n",
    "error on validation set\n",
    "- Test set: test the final hypothesis on test set to get unbiased\n",
    "error estimate for it\n",
    "\n",
    "After error estimation the final model is often trained on\n",
    "combined training, validation and test set, using best\n",
    "hyperparameters found during model selection\n",
    "- Complication: randomized optimization approaches where\n",
    "different runs with same hyperparameters can lead to very\n",
    "different quality solutions (e.g. neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mYEaM5nZXat"
   },
   "outputs": [],
   "source": [
    "orig_columns = ['reg', 'text']\n",
    "\n",
    "# load the data\n",
    "df_train = pd.read_csv(train_file, sep='\\t', names=orig_columns)\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_dev_full = pd.read_csv(dev_file, sep='\\t', names=orig_columns)\n",
    "df_dev_full = (df_dev_full.sample(frac=1, random_state=42)\n",
    "                          .reset_index(drop=True))\n",
    "\n",
    "df_test_full = pd.read_csv(test_file, sep='\\t', names=orig_columns)\n",
    "df_test_full = (df_test_full.sample(frac=1, random_state=42)\n",
    "                            .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhfiJUeuZXa5"
   },
   "source": [
    "The following classes of the test and development sets are not in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vV5y3TpZZXa9",
    "outputId": "f07ebbe9-097b-4d89-c0e5-e82dfe608388"
   },
   "outputs": [],
   "source": [
    "print(\"Dev/test classes not in the training set:\\n\")\n",
    "print(df_test_full[~df_test_full.reg.isin(df_train.reg.unique())].reg.unique())\n",
    "print(df_dev_full[~df_dev_full.reg.isin(df_train.reg.unique())].reg.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f65defOJZXbJ"
   },
   "source": [
    "Since we can not train the model to predict labels it has not seen during training phase, we will keep only rows in dev and test data which have the labels that appear also in training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLfMbupaZXbM"
   },
   "outputs": [],
   "source": [
    "df_test = df_test_full[\n",
    "    df_test_full.reg.isin(df_train.reg.unique())].reset_index(drop=True)\n",
    "df_dev = df_dev_full[\n",
    "    df_dev_full.reg.isin(df_train.reg.unique())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JBBtsNiZXba"
   },
   "source": [
    "Texts for all the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rzg4AQNYZXbc"
   },
   "outputs": [],
   "source": [
    "txt_train = [x for x in df_train.text]\n",
    "txt_dev = [x for x in df_dev.text]\n",
    "txt_test = [x for x in df_test.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lI7w7lvFZXbl"
   },
   "source": [
    "Encoded labels (registers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKBmHZDJZXbp"
   },
   "outputs": [],
   "source": [
    "# labels as numbers\n",
    "label_encoder = LabelEncoder()\n",
    "train_classes = label_encoder.fit_transform(df_train.reg)\n",
    "dev_classes = label_encoder.transform(df_dev.reg)\n",
    "test_classes = label_encoder.transform(df_test.reg)\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYwLbDo6ZXbz"
   },
   "source": [
    "#### Some initial thoughts\n",
    "\n",
    "We draw a chart for visualizing the distribution of different kinds of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWJbq2hyZXb0",
    "outputId": "d662ae3e-dd0b-41f6-fc38-8acc776b66d0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (df, name) in enumerate(zip([df_train, df_dev, df_test],\n",
    "                                   ['Train', 'Dev', 'Test'])):\n",
    "    ax[i].hist(df.reg.value_counts().values, bins=len(df.reg.unique()))\n",
    "    ax[i].set_title(name, fontsize=14)\n",
    "    ax[i].set_xlabel(\"Samples in a class\", fontsize=14)\n",
    "    ax[i].set_ylabel(\"Number of classes\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZhzxnvCZXcA"
   },
   "source": [
    "We immediately observe that the number of classes with just one sample is very high, whereas there are lot less classes that have several representatives. That is, the distribution of labels is highly skewed in all the data sets. This could be fixed with data augmentation but since we have no experience, we settle for the data provided. Also further manual annotation to gain more data would probably improve model performance in all the upcoming classifiers.\n",
    "\n",
    "Especially predicting the labels which have only a few instances in the data will be a challenging task for the classifiers since they might not be able to truly learn the characteristics of the label by one or two instances.\n",
    "\n",
    "Small classes (labels) also hinder using powerful hyperparameter optimizers, such as GridSearch, in optimal way: Stratification for K-folds in inner and outer loops is impossible because of the scarcity of the instances in the smallest classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCO9RF_QZXcC"
   },
   "source": [
    "## Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CwCByRhZXcF"
   },
   "source": [
    "For the first two models we construct vocabularies of alternative sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNoyrBPkZXcG",
    "outputId": "f6b61672-f069-4725-802e-3a528c585a08"
   },
   "outputs": [],
   "source": [
    "print(\"Generating vocabularies...\")\n",
    "max_features_options = [1000, 10000, 20000, 30000, 40000, 50000]\n",
    "ms1_vocabs = {}\n",
    "for mf in max_features_options:\n",
    "    # a vectorizer and the feature matrices for this particular max_features\n",
    "    # NOTE: outside of this notebook other params were tested; see baseline\n",
    "    vectorizer = CountVectorizer(max_features=mf,\n",
    "                                 binary=False,\n",
    "                                 ngram_range=(1, 1))\n",
    "    ms1_vocabs[mf] = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'train_fm': vectorizer.fit_transform(txt_train),\n",
    "        'dev_fm': vectorizer.transform(txt_dev),\n",
    "        'test_fm':  vectorizer.transform(txt_test)\n",
    "    }\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7bfTVuTdZXcR"
   },
   "source": [
    "### 1.0: Baseline (Naive Bayes)\n",
    "\n",
    "Naive Bayes classifier to be used as a baseline. It is trained on the training data for each separate dataset (different _max_features_ params for the CountVectorizer). Additionally accuracy on the permutated test sets are computed for the purpose of ensuring that performance with actual data differs from performance with junk data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79K0eKdyZXcU",
    "outputId": "539cb305-ca57-4a7f-9530-5ad4c10ab5bb"
   },
   "outputs": [],
   "source": [
    "def mnb_compute(X, y, mnb):\n",
    "    predictions = mnb.predict(X)\n",
    "    return (np.sum(predictions == y) / len(y), predictions)\n",
    "\n",
    "bayes_results = {}\n",
    "print(\"Computing stats for the MNB models...\")\n",
    "for max_feats, vocab in ms1_vocabs.items():\n",
    "    # train using the training set\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(vocab['train_fm'], df_train.reg)\n",
    "\n",
    "    # the accuracy of the trained model on all the sets\n",
    "    train_accuracy, _ = mnb_compute(vocab['train_fm'], df_train.reg, mnb)\n",
    "    dev_accuracy, _ = mnb_compute(vocab['dev_fm'], df_dev.reg, mnb)\n",
    "    test_accuracy, test_preds = mnb_compute(vocab['test_fm'], df_test.reg, mnb)\n",
    "    \n",
    "    # accuracy on permutated test set\n",
    "    permutated = np.random.permutation(df_test.reg)\n",
    "    perm_accuracy, _ = mnb_compute(vocab['test_fm'], permutated, mnb)\n",
    "    \n",
    "    bayes_results[max_feats] = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'dev_accuracy': dev_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_predictions': test_preds,\n",
    "        'perm_accuracy': perm_accuracy,\n",
    "    }\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4yap8UwZXch",
    "outputId": "d36a142f-692b-46a3-9145-c63cc9713119"
   },
   "outputs": [],
   "source": [
    "for mf, stats in bayes_results.items():\n",
    "    print(f\"\\nMax features: {mf}\\nACCURACY: \"\n",
    "          f\"Train {stats['train_accuracy']:.2f}  \"\n",
    "          f\"Dev {stats['dev_accuracy']:.2f}  \"\n",
    "          f\"Test {stats['test_accuracy']:.2f}  \"\n",
    "          f\"Permutated {stats['perm_accuracy']:.2f}  \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOcIE581ZXcw"
   },
   "source": [
    "**NOTE**: Outside of this notebook other params (*binary, ngram*) for the CountVectorizer were also tested. Neither here nor with the BOW model had they any practical effect whatsoever. Also, the value of the *max_features* parameter seems to be the most effective approximately in the range of 20 - 40 k, after which the accuracy begins to slowly degrade (possibly due to more features contributing only noise). \n",
    "\n",
    "**Having several different models for different parameter sets proved to be very demanding RAM-wise (particularly for BOW)**, and therefore, to keep the hardware requirements in check, we reduced the number of different *max_feature*s values to three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsWebVD3ZXc0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# limit the number of different max_features params\n",
    "max_features_options = [1000, 10000, 30000]\n",
    "ms1_vocabs.pop(20000)\n",
    "ms1_vocabs.pop(40000)\n",
    "ms1_vocabs.pop(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4flXldb8ZXdB"
   },
   "source": [
    "### 1.1: BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdfcQVFxZXdD"
   },
   "source": [
    "We tried several optimizers, of which SGD and Adam seemed to be the most effective. Therefore we decided to cover only them in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvxKjE0mZXdE"
   },
   "outputs": [],
   "source": [
    "keras_optimizers = {\n",
    "    'SGD': optimizers.SGD,\n",
    "    'Adam': optimizers.Adam,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZpNT6CcZXdJ"
   },
   "source": [
    "Next we define the function responsible for building the bow model.\n",
    "\n",
    "\n",
    "**NOTE**: Outside of this notebook several alternative model constructions were tested. These included other forms of regularization (e.g. dropout), different setups for hidden layer(s) and using pre-computed weights due to the data being imbalanced. None of these had any practical effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZvrdyg2ZXdM"
   },
   "outputs": [],
   "source": [
    "def build_bow_model(\n",
    "        feature_matrix, activation=None, optimizer=None, learning_rate=None\n",
    "    ):\n",
    "    \"\"\" Builds the BOW model according to the parameters \"\"\"\n",
    "    doc_count, feature_count = feature_matrix.shape\n",
    "    inp = Input(shape=(feature_count,))\n",
    "    hidden = Dense(250, activation=activation)(inp)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(hidden)\n",
    "    model = Model(inputs=[inp], outputs=[output])\n",
    "    optimizer = optimizer(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXpe7nSxZXdW"
   },
   "source": [
    "We also performed GridSearchCV for the following parameter combinations:\n",
    "- max_features: 1000, 10000, 30000\n",
    "- optimizers: SGD, Adam\n",
    "- activation: tanh, ReLU\n",
    "- learning rate:  0.00098, 0.00310, 0.00984, 0.03125, 0.09921, 0.31498, 1.0\n",
    "\n",
    "\n",
    "**NOTE**: This is an extremely taxing process both time- and resource-wise (allocating in total 20 GB of RAM + some swap on the side). Therefore the full computation does not have to be performed here, and instead only list some of the best results, with which we will proceed. Set *DO_BOW_GSCV* to **True** to run the full operation or to **False** to load the precomputed data.\n",
    "\n",
    "Given the availability of sufficient resources, it would also be an option to optimize several other factors (not necessarily using GridSearchCV if not applicable but by other means) such as hidden layer composition or dropout rate (if used). However, due to limited time and hardware we have to settle for the options mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZXGSQz3ZXdc"
   },
   "outputs": [],
   "source": [
    "# set to True to perform GridSearchCV again\n",
    "DO_BOW_GSCV = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Zbz2t1aZXdj",
    "outputId": "b57bb044-045f-4b34-a7e9-c0915fdc01d7"
   },
   "outputs": [],
   "source": [
    "if DO_BOW_GSCV or not bow_grid_file.is_file():\n",
    "    mc = ModelCheckpoint(filepath=bow_model_filepath,\n",
    "                         monitor='accuracy',\n",
    "                         verbose=0,\n",
    "                         save_best_only=True,\n",
    "                         mode='auto')\n",
    "\n",
    "    es = EarlyStopping(monitor='accuracy',\n",
    "                       patience=20,\n",
    "                       verbose=0,\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "    bow_grid_params = {\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'optimizer': list(keras_optimizers.values()), \n",
    "        'learning_rate': np.logspace(-10, 0, num = 7, base = 2).tolist()\n",
    "    }\n",
    "    grid_results = {}\n",
    "    for mf in max_features_options:\n",
    "        print(f\"BOW: GridSearchCV for param max_features={mf}\")\n",
    "        feature_matrix = ms1_vocabs[mf]['train_fm']\n",
    "\n",
    "        def f(activation=None, optimizer=None, learning_rate=None):\n",
    "            return build_bow_model(\n",
    "                feature_matrix,\n",
    "                activation=activation, \n",
    "                optimizer=optimizer,\n",
    "                learning_rate=learning_rate\n",
    "            )\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            estimator=KerasClassifier(\n",
    "                build_fn=f, epochs=40, batch_size=32, verbose=0),\n",
    "            param_grid=bow_grid_params,\n",
    "            n_jobs=1,\n",
    "            cv=3,\n",
    "            verbose=0,\n",
    "        )\n",
    "        grid_results[mf] = grid.fit(\n",
    "            feature_matrix.toarray(), train_classes, callbacks=[es, mc])\n",
    "    print(\"Done.\")\n",
    "\n",
    "    bow_grid_results = {}\n",
    "    for mf in max_features_options:\n",
    "        res = grid_results[mf]\n",
    "        bow_grid_results[mf] = {\n",
    "            'params': res.cv_results_['params'],\n",
    "            'rank_test_score': res.cv_results_['rank_test_score'],\n",
    "            'mean_test_score': res.cv_results_['mean_test_score']\n",
    "        }\n",
    "    pickle.dump(bow_grid_results, open(bow_grid_file, 'wb'))\n",
    "else:\n",
    "    bow_grid_results = pickle.load(open(bow_grid_file, 'rb'))\n",
    "    print(\"BOW: Loaded GridSearchCV results from a file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyFUCD_XZXdr"
   },
   "source": [
    "The following parameter sets ranked the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHYv3hfmZXds",
    "outputId": "0b4cd0ba-82a6-44c7-d9f9-fec85cf693dc"
   },
   "outputs": [],
   "source": [
    "bow_params = {}  # For storing the best models for final comparison\n",
    "for mf in max_features_options:\n",
    "    print(f\"\\n{'--' * 10}\\nBest for max_features={mf}:\")\n",
    "    res = bow_grid_results[mf]\n",
    "    rank = list(res['rank_test_score'])\n",
    "    idx_best = [rank.index(x) if x in rank else rank.index(x - 1)\n",
    "                for x in range(1, 4)]\n",
    "    best = [res['params'][x] for x in idx_best]\n",
    "    bow_params[mf] = best\n",
    "    scores = [res['mean_test_score'][x] for x in idx_best]\n",
    "    for i, r in enumerate(best):\n",
    "        print(f\"\\n{['First:', 'Second:', 'Third:'][i]} (score {scores[i]:.3f})\\n\")\n",
    "        pp.pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dynxrVPxZXdy"
   },
   "source": [
    "As can be seen, *tanh* does consistently better than *ReLU*, at least in the context of GridSearchCV. As for the other params, the results are more varied, even though the scores between the parameter sets do not vary considerably. In any case, a line has to be drawn somewhere, and therefore we will perform the full training cycle only with the param sets defined above.\n",
    "\n",
    "In addition, for regularization purposes, early stopping will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpaK445MZXdz",
    "outputId": "7a7c2519-debd-4791-ee5b-ade77b3f1f13"
   },
   "outputs": [],
   "source": [
    "# Early Stopping settings\n",
    "mc = ModelCheckpoint(filepath=bow_model_filepath,\n",
    "                     monitor='val_accuracy',\n",
    "                     verbose=0,\n",
    "                     save_best_only=True,\n",
    "                     mode='auto')\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy',\n",
    "                   patience=20,\n",
    "                   verbose=0,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "# The parameters that are used in the training process\n",
    "fit_params = {\n",
    "    'batch_size': 32,\n",
    "    'verbose': 0,\n",
    "    'epochs': 100,\n",
    "    'callbacks': [mc, es],\n",
    "}\n",
    "\n",
    "def train_bow_model(max_features, params, train_classes=train_classes):\n",
    "    \"\"\" Returns a trained BOW model and results for the given parameters \"\"\"\n",
    "    \n",
    "    # Get the correct vocabulary and define the validation set\n",
    "    vocab = ms1_vocabs[max_features]\n",
    "    fit_params['validation_data'] = (vocab['dev_fm'], dev_classes)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_bow_model(vocab['train_fm'], **params)\n",
    "    hist = model.fit(vocab['train_fm'].toarray(), train_classes, **fit_params)\n",
    "\n",
    "    # compute predictions for all the sets\n",
    "    train_preds = np.argmax(model.predict(vocab['train_fm'].toarray()), axis=1)\n",
    "    train_acc = np.sum(np.equal(train_classes, train_preds)) / len(train_classes)\n",
    "    \n",
    "    dev_preds = np.argmax(model.predict(vocab['dev_fm'].toarray()), axis=1)\n",
    "    dev_acc = np.sum(np.equal(dev_classes, dev_preds)) / len(dev_classes)\n",
    "    \n",
    "    test_preds = np.argmax(model.predict(vocab['test_fm'].toarray()), axis=1)\n",
    "    test_acc = np.sum(np.equal(test_classes, test_preds)) / len(test_classes)\n",
    "    \n",
    "    return {\n",
    "        'hist': hist.history,\n",
    "        'optimizer': params['optimizer'].__name__,\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'train_accuracy': train_acc,\n",
    "        'validation_accuracy': dev_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_predictions': label_encoder.inverse_transform(test_preds),\n",
    "        'true_test_labels': label_encoder.inverse_transform(test_classes),\n",
    "    }\n",
    "    \n",
    "bow_results = {}\n",
    "for mf in max_features_options:\n",
    "    bow_results[mf] = []\n",
    "    print(f\"\\nTraining BOW with max_features={mf}\")\n",
    "    for i, param_set in enumerate(bow_params[mf]):\n",
    "        print(f\"Param set {i + 1}.\", end=\" \")\n",
    "        bow_results[mf].append(train_bow_model(mf, param_set))\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csptN4TIZXd7"
   },
   "source": [
    "Then it's time for some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zdq0dzO_ZXd8",
    "outputId": "e0b4757a-b108-4615-cafb-44e75e774388"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(15, 18), sharey=True, tight_layout=True)\n",
    "plt.suptitle(\"BOW results (param sets by GSCV)\\n\\n\", va='baseline', fontsize=20)\n",
    "for i, (mf, res_mf) in enumerate(bow_results.items()):\n",
    "    for j, res in enumerate(res_mf):\n",
    "        hist = res['hist']\n",
    "        ax[i][j].set_title(f\"MAX FEATURES: {mf}\\n\"\n",
    "                        f\"Learning rate: {res['learning_rate']:.3f};    \"\n",
    "                        f\"Optimizer: {res['optimizer']}\",\n",
    "                        fontsize=12)\n",
    "        ax[i][j].plot(hist['accuracy'], label='training')\n",
    "        ax[i][j].plot(hist['val_accuracy'], label='validation')\n",
    "        ax[i][j].set_xlabel(\n",
    "            f\"ACCURACY:\\n Train: {res['train_accuracy']:.2f}    \"\n",
    "            f\"Validation: {res['validation_accuracy']:.2f}    \"\n",
    "            f\"Test: {res['test_accuracy']:.2f}\\n\",\n",
    "            fontsize=12\n",
    "        )\n",
    "        ax[i][j].legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuF4rq2IZXeB"
   },
   "source": [
    "As can be seen the results are downright lackluster. Optimizer-wise models with *Adam* do not seem to learn anything at all (or they learn everything they can during the first epoch). The exact value of *max_features* also seems to be somewhat irrelevant as long as it is within reasonable limits.\n",
    "\n",
    "It should also be noted that the results displayed are clearly above results obtained from non-signal data. That is, if we permutate the training labels (effectively training with junk data) we obtain the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7D_w-RbZXeD",
    "outputId": "45bca101-cac0-4e7e-c67c-8c62b0dbf0fb"
   },
   "outputs": [],
   "source": [
    "permutated_bow = train_bow_model(\n",
    "    10000,\n",
    "    bow_params[10000][0],\n",
    "    train_classes=np.random.permutation(train_classes))\n",
    "print(\"Accuracies for a model trained with permutated data \"\n",
    "      \"(best param set for max_features = 10000):\\n\"\n",
    "      f\"Train: {permutated_bow['train_accuracy']:.3f}\\t\"\n",
    "      f\"Dev: {permutated_bow['validation_accuracy']:.3f}\\t\"\n",
    "      f\"Test: {permutated_bow['test_accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzwqO94rZXeP"
   },
   "source": [
    "That is, the test set accuracy has dropped from over 0.5 to approximately 0.1. Of course this is just a single permutation, and to improve confidence we'd have to repeat the test at least a thousand times to obtain a proper p-value, but that is outside the scope of this report.\n",
    "\n",
    "**We will inspect the predictions more closely in Milestone 2.2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFIqNNzDZXeQ"
   },
   "source": [
    "### 1.2: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yMXHuLFZXeR"
   },
   "source": [
    "Next up is the RNN model. In order to limit the number of parameters that need optimizing we first opted for using FastText embeddings on the embedding layer (see https://fasttext.cc/docs/en/crawl-vectors.html). The model would therefore be constructed by making the embedding layer of shape *vocabulary size X FastText vector size*.\n",
    "\n",
    "We generated the embedding matrix as follows, using a vocabulary size of 100000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAyb6AKiZXeT",
    "outputId": "41414423-4b57-4fc0-b486-06450c3cbf8d"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300  # FastText vector size\n",
    "vocab_size = 100000\n",
    "sequence_length = 250\n",
    "\n",
    "\n",
    "if embedding_matrix_file.is_file():\n",
    "    embedding_matrix = pickle.load(open(embedding_matrix_file, 'rb'))\n",
    "    print(\"Loaded the embedding matrix from a file.\")\n",
    "else:\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        !wget -nc http://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fi.300.vec.gz\n",
    "        !gunzip cc.fi.300.vec.gz    \n",
    "        fasttext_file = 'cc.fi.300.vec'\n",
    "    else:\n",
    "        fasttext_file = Path.home() / 'Documents/Fasttext/cc.fi.300.vec'\n",
    "        \n",
    "    tok = Tokenizer(num_words=vocab_size, lower=False, split=' ', char_level=False)\n",
    "    tok.fit_on_texts(txt_train)\n",
    "\n",
    "    print(f\"Generating the embedding matrix..\", end=\"\")\n",
    "\n",
    "    embedding_matrix = np.zeros([vocab_size, embedding_dim])\n",
    "\n",
    "    with open(fasttext_file) as word_vecs:\n",
    "        next(word_vecs)  # skip the first line (contains stats)\n",
    "\n",
    "        for i, line in enumerate(word_vecs):\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "            items = line.strip().split(' ')\n",
    "            word = items[0]\n",
    "\n",
    "            # If the word does not have an embedding it will be omitted =>\n",
    "            # the corresponding row of the embedding matrix is 0-filled\n",
    "            try:\n",
    "                pos = tok.word_index[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # If the word is not among the 100000 most common ones, omit it.\n",
    "            if pos >= vocab_size:\n",
    "                continue\n",
    "\n",
    "            # Set row for this particular word in the matrix.\n",
    "            embedding = (np.array([float(j) for j in items[1:]]))\n",
    "            embedding_matrix[pos, :] = embedding\n",
    "\n",
    "    pickle.dump(embedding_matrix, open(embedding_matrix_file, 'wb'))\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this approach is not ideal. The number of 0 filled rows (embeddings not found) is quite high. Therefore we allow for the layer to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of 0-filled rows in the embedding matrix:\",\n",
    "      len(np.where(~embedding_matrix.any(axis=1))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MR4ckSsZXeZ"
   },
   "source": [
    "We will use Keras tools for text tokenization and sequence padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsPGo0fGZXea"
   },
   "outputs": [],
   "source": [
    "def get_padded_sequences():\n",
    "    \"\"\" \n",
    "    Constructs tokenized and padded sequences from the texts.\n",
    "    Default 'pre' padding.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=vocab_size, lower=True, split=' ', char_level=False\n",
    "    )\n",
    "    \n",
    "    params = {'maxlen': sequence_length, 'value': 0}\n",
    "    \n",
    "    tokenizer.fit_on_texts(txt_train)\n",
    "    \n",
    "    train_seq = tokenizer.texts_to_sequences(txt_train)\n",
    "    train_seq = pad_sequences(train_seq, **params)\n",
    "    \n",
    "    dev_seq = tokenizer.texts_to_sequences(txt_dev)\n",
    "    dev_seq = pad_sequences(dev_seq, **params)\n",
    "    test_seq = tokenizer.texts_to_sequences(txt_test)\n",
    "    test_seq = pad_sequences(test_seq, **params)\n",
    "    \n",
    "    return train_seq, dev_seq, test_seq\n",
    "\n",
    "# Get the sequences\n",
    "train_seq, dev_seq, test_seq = get_padded_sequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4c3dsNWZXee"
   },
   "source": [
    "The following function is used to construct the model. There are a lot of options that could/should be tested, but evaluating all the aspects would take more time and resources than we can afford. **We initially implemented a GSCV routine for RNN also**, but found it to be too taxing and time-consuming for the project at hand.\n",
    "\n",
    "Some considerations:\n",
    "- dropout (*dropout, recurrent_dropout*) will be omitted; only early stopping is relied on for regularization purposes\n",
    "- outside of this notebook we tried several alternative approaches, all of which resulted in **no observable improvement**:\n",
    "    - adding a BiLSTM layer\n",
    "    - with/without FastText embeddings\n",
    "    - different sequence lengths\n",
    "    - different vocabulary sizes\n",
    "    - different number of RNN units\n",
    "\n",
    "Of course specific choices can be made to achieve considerably weaker performance. The results displayed here are approximately the best we could reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYwXLLtGZXeg"
   },
   "outputs": [],
   "source": [
    "def build_rnn_model(learning_rate=None, emb_trainable=True):\n",
    "    \"\"\" Builds the RNN model according to the params \"\"\"\n",
    "    rnn_units = 100\n",
    "    em = embedding_matrix[:vocab_size]\n",
    "    new_em = np.empty_like(em)\n",
    "    new_em[:] = em\n",
    "    weights = [new_em]\n",
    "    input_ = Input(shape=(sequence_length,))\n",
    "    # We will also allow further training of the embedding layer\n",
    "    embedding = Embedding(\n",
    "        vocab_size, embedding_dim, weights=weights, trainable=emb_trainable\n",
    "    )(input_)\n",
    "    rnn = LSTM(rnn_units, return_sequences=False)(embedding)\n",
    "    output = Dense(num_classes, activation='softmax')(rnn)\n",
    "    model =  Model(inputs=[input_], outputs=[output])\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqyOklkRZXel"
   },
   "source": [
    "The training setup for the RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alephzTHZXem",
    "outputId": "067103d4-aee8-4b8c-83b6-64b985ab467e"
   },
   "outputs": [],
   "source": [
    "# Early Stopping settings\n",
    "mc_rnn = ModelCheckpoint(filepath=rnn_model_filepath,\n",
    "                         monitor='val_accuracy',\n",
    "                         verbose=0,\n",
    "                         save_best_only=True,\n",
    "                         mode='auto')\n",
    "    \n",
    "es_rnn = EarlyStopping(monitor='val_accuracy',\n",
    "                       patience=5,  # 20 => longer training time, same accuracy\n",
    "                       verbose=0,\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "# The parameters that are used in the training process\n",
    "fit_params = {\n",
    "    'batch_size': 100,\n",
    "    'verbose': 0,\n",
    "    'epochs': 100,\n",
    "    'callbacks': [mc_rnn, es_rnn],\n",
    "    'validation_data': (dev_seq, dev_classes)\n",
    "}\n",
    "\n",
    "def train_rnn_model(learning_rate, fit_params=fit_params, emb_trainable=True):\n",
    "    \"\"\" Returns a trained BOW model and results for the given parameters \"\"\"\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_rnn_model(learning_rate, emb_trainable=emb_trainable)\n",
    "    hist = model.fit(train_seq, train_classes, **fit_params)\n",
    "\n",
    "    # compute predictions for all the sets\n",
    "    train_preds = np.argmax(model.predict(train_seq), axis=1)\n",
    "    train_acc = np.sum(np.equal(train_classes, train_preds)) / len(train_classes)\n",
    "    \n",
    "    dev_preds = np.argmax(model.predict(dev_seq), axis=1)\n",
    "    dev_acc = np.sum(np.equal(dev_classes, dev_preds)) / len(dev_classes)\n",
    "    \n",
    "    test_preds = np.argmax(model.predict(test_seq), axis=1)\n",
    "    test_acc = np.sum(np.equal(test_classes, test_preds)) / len(test_classes)\n",
    "    \n",
    "    return {\n",
    "        'hist': hist.history,\n",
    "        'learning_rate': learning_rate,\n",
    "        'train_accuracy': train_acc,\n",
    "        'validation_accuracy': dev_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_predictions': label_encoder.inverse_transform(test_preds),\n",
    "        'true_test_labels': label_encoder.inverse_transform(test_classes),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results = {}\n",
    "for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    print(f\"\\nTraining LSTM with learning_rate={lr}\")\n",
    "    rnn_results[lr] = train_rnn_model(lr)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S205R-TZXep",
    "outputId": "7e0017e5-d701-4f24-e257-4ba31a79e6fa"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5), sharey=True, tight_layout=True)\n",
    "plt.suptitle(\"LSTM results\\n\\n\", va='baseline', fontsize=20)\n",
    "for i, (lr, res) in enumerate(rnn_results.items()):\n",
    "    hist = res['hist']\n",
    "    ax[i].set_title(f\"Learning rate: {res['learning_rate']:.4f}\", fontsize=12)\n",
    "    ax[i].plot(hist['accuracy'], label='training')\n",
    "    ax[i].plot(hist['val_accuracy'], label='validation')\n",
    "    ax[i].set_xlabel(\n",
    "        f\"ACCURACY:\\n Train: {res['train_accuracy']:.2f}    \"\n",
    "        f\"Validation: {res['validation_accuracy']:.2f}    \"\n",
    "        f\"Test: {res['test_accuracy']:.2f}\\n\",\n",
    "        fontsize=12\n",
    "    )\n",
    "    ax[i].legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9qzoqXaZXeu"
   },
   "source": [
    "The results are considerably weaker than with most of the BOW models. Depending on the actual parameters the accuracy on the test set could occasionally be improved by 1-2 %, but no setup allowed the model to reach even the 40 % limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDd23yyVZXey"
   },
   "source": [
    "**We will inspect the predictions more closely in Milestone 2.2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9N6ALu9aZXez"
   },
   "source": [
    "## Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ko5tLG5ZXe0"
   },
   "source": [
    "### 2.1: BERT\n",
    "\n",
    "We had initially some trouble in getting BERT working properly, and therefore decided to use two alternative approaches: Keras and PyTorch. Some massive problems remained (highly likely relating to the order in which libraries are imported) and we settled for stylewise suboptimal decision to import keras-BERT and relating functions as close to deployment as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYolkabkZXe0"
   },
   "outputs": [],
   "source": [
    "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
    "\n",
    "# Maximum number of examples to read\n",
    "MAX_EXAMPLES = 1500\n",
    "\n",
    "# Maximum length of input sequence in tokens\n",
    "INPUT_LENGTH = 250\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Optimizer learning rate\n",
    "LEARNING_RATE = 0.00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmaIoojpZXe4"
   },
   "source": [
    "**NOTE**: We initially trained both of the models for considerably longer, up to 40 epochs for the Keras version (limited by early stopping). However, the learning effectively stopped after approximately 3-4 epochs (after that insignificant improvement at best). Therefore we cut down the number of training epochs considerably to speed up the execution of the submitted version of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OYvQvkXZXe6"
   },
   "source": [
    "### 2.1.1: BERT Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBpu7izhZXe7"
   },
   "source": [
    "To avoid out of memory we have to limit the input size to MAX_EXAMPLES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j11hmNLDZXe8"
   },
   "outputs": [],
   "source": [
    "def truncate_data(data, MAX_EXAMPLES):\n",
    "    if len(data) > MAX_EXAMPLES: # truncate data if needed to avoid OOM\n",
    "        # should take stratified subsample?\n",
    "        print('Note: truncating examples from {} to {}'\n",
    "              .format(len(data), MAX_EXAMPLES))\n",
    "        data = data[:MAX_EXAMPLES]\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peb4QleRZXfC",
    "outputId": "7502d79b-f275-45fc-d795-491b265994ce"
   },
   "outputs": [],
   "source": [
    "# We use data sets which only have labels that appear in the training data\n",
    "# All the datas will be truncated if needed\n",
    "\n",
    "train_ = truncate_data(df_train, MAX_EXAMPLES)\n",
    "dev_ = truncate_data(df_dev, MAX_EXAMPLES)\n",
    "test_ = truncate_data(df_test, MAX_EXAMPLES)\n",
    "\n",
    "frames = [train_, dev_, test_]\n",
    "for d in frames:\n",
    "    print(d.shape)\n",
    "#  print(d.head())\n",
    "\n",
    "train_reg = train_['reg']\n",
    "\n",
    "test_ = test_[test_['reg'].isin(train_reg.tolist())]\n",
    "dev_ = dev_[dev_['reg'].isin(train_reg.tolist())] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmy1FoS2ZXfE"
   },
   "source": [
    "Set some needed variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZyHACfnZXfE"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1' \n",
    "model_is_cased = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhFFXvG1ZXfI"
   },
   "source": [
    "We load the BERT vocabulary and model configurations. Vocabulary contains tags and suffixes needed to utilize our own input texts efficeintly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn6gxZ2eZXfJ"
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "with open(bert_vocab_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
    "        \n",
    "with open(bert_config_path) as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5Md3RXoZXfM"
   },
   "source": [
    "Then we create BERT tokenizer. To do so, we'll need a mapping from vocabulary items to their corresponding integer indices. We do this conventionally using enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBjhYHZ_ZXfM"
   },
   "outputs": [],
   "source": [
    "from keras_bert import Tokenizer as BERT_Tokenizer\n",
    "# Create mapping from vocabulary items to their indices in the vocabulary\n",
    "token_dict = { v: i for i, v in enumerate(vocab) }\n",
    "\n",
    "tokenizer = BERT_Tokenizer(token_dict, cased=model_is_cased) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4Yt8NxYZXfP"
   },
   "source": [
    "We will train the label encoder with the labels the truncated training data contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTaTEtkaZXfP"
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
    "train_label = train_['reg'].tolist() \n",
    "\n",
    "label_encoder.fit(train_label)\n",
    "# encoded labels. Y holds the labels the model will learn to predict.\n",
    "Y = label_encoder.transform(train_label) \n",
    "\n",
    "num_labels = len(set(Y)) # Take note of how many unique labels there are in the data\n",
    "\n",
    "# development data\n",
    "dev_label = dev_['reg']\n",
    "y = label_encoder.transform(dev_label)\n",
    "\n",
    "# test data\n",
    "test_label = test_['reg'].tolist()\n",
    "true_labels = label_encoder.transform(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wsMAikVZXfS"
   },
   "source": [
    "Then we tokenize input data. We keep token indices and segment ids in separate lists and store as numpy arrays. X here is the final vectorized form of the input we'll be providing to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjF32x1CZXfT"
   },
   "outputs": [],
   "source": [
    "def make_model_inputs(text):\n",
    "    token_indices, segment_ids = [], []\n",
    "    for text in text:\n",
    "        # tokenizer.encode() returns a sequence of token indices\n",
    "        # and a sequence of segment IDs. BERT expects both as input,\n",
    "        # even if the segments IDs are just all zeros (like here).\n",
    "        tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
    "        token_indices.append(tid)\n",
    "        segment_ids.append(sid)\n",
    "    # Format input as list of two numpy arrays\n",
    "    inp = [np.array(token_indices), np.array(segment_ids)] \n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VbkSRN5ZXfW"
   },
   "outputs": [],
   "source": [
    "X = make_model_inputs(train_['text'])\n",
    "x = make_model_inputs(dev_['text'])\n",
    "test_inp = make_model_inputs(test_['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bp5ygpEmZXfZ"
   },
   "source": [
    "Then we load pretrained BERT model\n",
    "\n",
    "We'll use the keras-bert function load_trained_model_from_checkpoint to load the model from the checkpoint we downloaded earlier.\n",
    "\n",
    "Explanation for a few parameters from keras-bert documentation:\n",
    "\n",
    "- training: If training, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
    "- trainable: Whether the model is trainable. The default value is the same with training.\n",
    "\n",
    "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use training=False but trainable=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwAVuEFBZXfZ"
   },
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "pretrained_model = load_trained_model_from_checkpoint(\n",
    "    config_file = bert_config_path,\n",
    "    checkpoint_file = bert_checkpoint_path,\n",
    "    training = False, # ignore MLM and NSP parts of the model\n",
    "    trainable = True,\n",
    "    seq_len = INPUT_LENGTH # define the size of input layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bhv5_PHZXfe",
    "outputId": "b9bebf9f-1d2b-4aad-98e4-4a3c5e5f7790"
   },
   "outputs": [],
   "source": [
    "print(pretrained_model.inputs)\n",
    "print(pretrained_model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFR0RDmTZXfh"
   },
   "source": [
    "Size of the input layer is as we defined with varaible INPUT_LENGTH. But the size of the output layer (250) does not match our label count. This must be fixed. This will be done y wrapping the pretrained model. We will grasp the model output and plug our own output layer on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2a0QDaoZXfi",
    "outputId": "3ca2d097-cff1-4863-808d-098f80fbee64"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "bert_out = pretrained_model.outputs[0][:,0]\n",
    "\n",
    "# by calling the output layer we end up calling all the other layers (the model)\n",
    "out = Dense(num_labels, activation='softmax')(bert_out) \n",
    "model = Model(\n",
    "    inputs=pretrained_model.inputs,\n",
    "    outputs=[out]\n",
    ")\n",
    "\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md3dCAsfZXfl"
   },
   "source": [
    "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We will copy parameters used in the original BERT work and Deep Learning in Human Language Teghnology -course.\n",
    "\n",
    "In the beginning of Milestone 2 we also set some more parameters to make the model work. These parameters have been optimized by exploring different values. We found out, that the most significant parameter for model performance is learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_qc4EoXZXfm"
   },
   "outputs": [],
   "source": [
    "from keras_bert import calc_train_steps, AdamWarmup\n",
    "\n",
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=len(train_['text']),\n",
    "    batch_size=MS2_BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(\n",
    "    total_steps,\n",
    "    warmup_steps,\n",
    "    lr=LEARNING_RATE,\n",
    "    epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    weight_decay_pattern=[\n",
    "        'embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ol8bRZEFZXfp"
   },
   "source": [
    "Then we compile and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gP972Gh1ZXfq"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy', # encoded labels!\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHQOhHz5ZXft"
   },
   "source": [
    "**NOTE**: It is impossible for at least one the desktop PCs in use to continue through this notebook uninterrupted. Particularly, handling both Keras-BERT and PyTorch-BERT during the same run proved difficult. Therefore we opted to try resetting the GPU state by using the Numba library and as a backup storing the results as pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nn1-t8S7ZXfu",
    "outputId": "63051e31-094e-4ac5-958a-c3bbf2f59def"
   },
   "outputs": [],
   "source": [
    "stop_cb = EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\"\"\"\n",
    "mc_cb = ModelCheckpoint(\n",
    "    filepath='models/BERT_multiclass.h5',\n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    verbose=0,save_best_only=True, mode='auto'\n",
    ")\n",
    "\"\"\"\n",
    "if not IN_COLAB and keras_bert_results_file.is_file():\n",
    "    # The desktop machine in use fails to allocate memory for all of the\n",
    "    # models => load precomputed results\n",
    "    keras_bert_results = pickle.load(open(keras_bert_results_file, 'rb'))\n",
    "    print(\"Loaded Keras Bert results from a file.\")\n",
    "    kb_predicted_labels = keras_bert_results['predictions']\n",
    "    history = None\n",
    "else:\n",
    "    keras_bert_results = None\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        Y,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=MS2_BATCH_SIZE,\n",
    "        validation_data=(x, y), \n",
    "        callbacks=[stop_cb]\n",
    "    )\n",
    "    \n",
    "    # np.argmax gives the index which has the highest value e.g. class\n",
    "    predictions = np.argmax(model.predict(test_inp), axis=1) \n",
    "    # inverse transform numerical labels to text\n",
    "    kb_predicted_labels = label_encoder.inverse_transform(predictions) \n",
    "\n",
    "    # Store the results in a file in case memory allocation problems arise\n",
    "    keras_bert_results = {\n",
    "        'history': history.history,\n",
    "        'predictions': kb_predicted_labels,\n",
    "        'true_labels': label_encoder.inverse_transform(true_labels)\n",
    "    }\n",
    "    \n",
    "    if not IN_COLAB:\n",
    "        pickle.dump(keras_bert_results, open(keras_bert_results_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABX_VqhiZXfy",
    "outputId": "b04d406b-17c5-4bed-a0ce-1877b48522d0"
   },
   "outputs": [],
   "source": [
    "# Explore training performance\n",
    "def plot_history(history):\n",
    "    plt.plot(history['sparse_categorical_accuracy'],\n",
    "             label=\"Training set accuracy\")\n",
    "    plt.plot(history['val_sparse_categorical_accuracy'],\n",
    "             label=\"Validation set accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Keras BERT\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(keras_bert_results['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSvHdmYCZXf1",
    "outputId": "27d1204d-17cc-46d5-dd45-a39ecfc25266"
   },
   "outputs": [],
   "source": [
    "# Compare predicted labels to true labels\n",
    "kb_acc = round(accuracy_score(test_label, kb_predicted_labels)*100,1)\n",
    "print(f\"Classification accuracy: {kb_acc} percent\")\n",
    "keras_bert_results['accuracy'] = kb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzEjpyv2ZXf7"
   },
   "source": [
    "Reset the GPU in low VRAM desktops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EiJq50-sZXf8"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtRaVUtQZXgA"
   },
   "source": [
    "### 2.1.2: BERT PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaJdE3pRZXgA"
   },
   "source": [
    "For comparison purposes here is also the PyTorch implementation of the process.\n",
    "\n",
    "First we detect if a GPU is available. Hopefully so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVrkwSzBZXgB"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMqZzmubZXgF"
   },
   "source": [
    "Next we tokenize and pad the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAlA7ouoZXgF"
   },
   "outputs": [],
   "source": [
    "# Tokenize the training sets\n",
    "b_sentences_train = [\"[CLS] \" + \" [SEP] \".join(query.split(' . ')) + \" [SEP]\" for query in txt_train]\n",
    "b_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-finnish-cased-v1\")\n",
    "b_tokenized_train = [b_tokenizer.tokenize(sent) for sent in b_sentences_train]\n",
    "\n",
    "b_sentences_dev = [\"[CLS] \" + \" [SEP] \".join(query.split(' . ')) + \" [SEP]\" for query in txt_dev]\n",
    "b_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-finnish-cased-v1\")\n",
    "b_tokenized_dev = [b_tokenizer.tokenize(sent) for sent in b_sentences_dev]\n",
    "\n",
    "b_sentences_test = [\"[CLS] \" + \" [SEP] \".join(query.split(' . ')) + \" [SEP]\" for query in txt_test]\n",
    "b_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-finnish-cased-v1\")\n",
    "b_tokenized_test = [b_tokenizer.tokenize(sent) for sent in b_sentences_test]\n",
    "\n",
    "# Pad the inputs \n",
    "b_train_inputs = torch.tensor(pad_sequences(\n",
    "    [b_tokenizer.convert_tokens_to_ids(txt) for txt in b_tokenized_train],\n",
    "    maxlen=INPUT_LENGTH, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "))\n",
    "b_train_labels = torch.tensor(train_classes)\n",
    "\n",
    "b_dev_inputs = torch.tensor(pad_sequences(\n",
    "    [b_tokenizer.convert_tokens_to_ids(txt) for txt in b_tokenized_dev],\n",
    "    maxlen=INPUT_LENGTH, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "))\n",
    "b_dev_labels = torch.tensor(dev_classes)\n",
    "\n",
    "b_test_inputs = torch.tensor(pad_sequences(\n",
    "    [b_tokenizer.convert_tokens_to_ids(txt) for txt in b_tokenized_test],\n",
    "    maxlen=INPUT_LENGTH, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "))\n",
    "b_test_labels = torch.tensor(test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jt-EUVb6ZXgI"
   },
   "source": [
    "Then we construct the PyTorch Dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0E_C2i-QZXgK"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(b_train_inputs, b_train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=MS2_BATCH_SIZE * 2, shuffle=True)\n",
    "\n",
    "dev_data = TensorDataset(b_dev_inputs, b_dev_labels)\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=32)\n",
    "\n",
    "test_data = TensorDataset(b_test_inputs, b_test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oA1-saoBZXgO"
   },
   "source": [
    "Huggingface transformers have a direct support for the cased FinBERT model so we load that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_IUJyj9ZXgO"
   },
   "outputs": [],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-finnish-cased-v1\", num_labels=len(set(train_classes))\n",
    ") \n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8fTpyYdZXgR"
   },
   "source": [
    "Now we can actually define the training loop and run it. Note that due to time and resource issues the model is only run for 3 epochs. This is enough for the model to reach its ultimate accuracy before any overfitting starts to take place (verified manually, and very similar to the Keras version). No early stopping was implemented for this scenario.\n",
    "\n",
    "**NOTE**: Epoch 0 scores are computed before any training (for recording the initial non-trained level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWv17rpzZXgS"
   },
   "outputs": [],
   "source": [
    "# Parameters as suggested in\n",
    "# https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = PTBertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=.1)\n",
    "\n",
    "# For storing loss and accuracy\n",
    "phases = ['train', 'validation']\n",
    "pytorch_bert_results = {'history': {'train': [], 'validation': []}}\n",
    "\n",
    "if not IN_COLAB and pytorch_bert_results_file.is_file():\n",
    "    pytorch_bert_results = pickle.load(open(pytorch_bert_results_file, 'rb'))\n",
    "    EPOCHS = 0\n",
    "    print(\"Loaded PyTorch BERT results from a file.\")\n",
    "else:\n",
    "    EPOCHS = 4\n",
    "    print(\"Training PyTorch BERT\")\n",
    "    \n",
    "# The training loop\n",
    "for i in range(EPOCHS):  \n",
    "    # Training phase: use training mode\n",
    "    model.train()  \n",
    "    \n",
    "    if i > 0:\n",
    "        # For every batch:\n",
    "        for batch in train_dataloader:\n",
    "            # Get batch data and transfer to the correct device (GPU hopefully)\n",
    "            b_input_ids, b_labels = batch\n",
    "            b_input_ids, b_labels = b_input_ids.to(device), b_labels.to(device)\n",
    "\n",
    "            # Clear out the accumulated gradients of the previous batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, labels=b_labels)\n",
    "\n",
    "            # Get loss\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Backward pass, update parameters and do gradient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    # Validation phase: use evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate validation data for one epoch\n",
    "    for phase, eval_dl in zip(phases, (train_dataloader, dev_dataloader)):\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        eval_steps, eval_examples = 0, 0\n",
    "        \n",
    "        for batch in eval_dl:\n",
    "            b_input_ids, b_labels = batch\n",
    "            b_input_ids, b_labels = b_input_ids.to(device), b_labels.to(device)\n",
    "\n",
    "            # Evaluating: no reason to compute gradients\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids)[0]\n",
    "\n",
    "            # Before computing accuracy etc. the data has to be extracted from GPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            labels_flat = label_ids.flatten()\n",
    "            batch_acc = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "  \n",
    "            eval_accuracy += batch_acc\n",
    "            eval_steps += 1\n",
    "        \n",
    "        pytorch_bert_results['history'][phase].append(eval_accuracy / eval_steps)\n",
    "    tr_acc = pytorch_bert_results['history']['train'][-1]\n",
    "    dev_acc = pytorch_bert_results['history']['validation'][-1]\n",
    "    print(f\"Epoch {i}, training accuracy: {tr_acc:.3f}\\t\"\n",
    "          f\"validation accuracy: {dev_acc:.3f}\")\n",
    "       \n",
    "print(\"Done.\")\n",
    "    \n",
    "if EPOCHS:\n",
    "    model.eval()\n",
    "    pt_predictions = []\n",
    "    pt_actual = []\n",
    "    test_steps = 0\n",
    "    test_accuracy = 0\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, labels = batch\n",
    "        b_input_ids, labels = b_input_ids.to(device), labels.numpy()\n",
    "\n",
    "        # Evaluating: no reason to compute gradients\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        batch_predictions = np.argmax(logits, axis=1).flatten()\n",
    "        test_accuracy += (np.sum(batch_predictions == labels) / len(labels))\n",
    "        pt_predictions.append(batch_predictions)\n",
    "        pt_actual.append(labels)\n",
    "        test_steps += 1\n",
    "\n",
    "    pytorch_bert_results.update({\n",
    "        'predictions': pt_predictions, \n",
    "        'true_labels': pt_actual,\n",
    "        'test_accuracy': test_accuracy / test_steps\n",
    "    })\n",
    "\n",
    "    if not IN_COLAB:\n",
    "        pickle.dump(pytorch_bert_results, open(pytorch_bert_results_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l111lxUqZXgZ"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pytorch_bert_results['history']['train'], label='Train')\n",
    "plt.plot(pytorch_bert_results['history']['validation'], label='Validation')\n",
    "plt.title(\"PyTorch BERT\", fontsize=16)\n",
    "plt.xlabel(\"Test set accuracy: \"\n",
    "             f\"{pytorch_bert_results['test_accuracy'] * 100:.2f} %\",\n",
    "           fontsize=14)\n",
    "for p_x, p_y in enumerate(pytorch_bert_results['history']['validation']):\n",
    "    plt.text(p_x, p_y, f'{p_y:.3f}\\n')\n",
    "    plt.plot(p_x, p_y, c='r', marker='x')\n",
    "plt.margins(0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_f6gjOjyZXgh"
   },
   "source": [
    "### 2.2: Error analysis\n",
    "\n",
    "We now investigate the aggregated results produced by all the models trained in Milestone 1 and 2. **For each implementation we will pick the best performing model** (or one of them if the differences are insignificant).\n",
    "\n",
    "First, we want to display confusion matrices, but the number of labels in the full sets are too high. Therefore we implement the following method which only considers the most common labels in the **test set**:\n",
    "- exclude any classes for which the count of the representative samples is less than ten\n",
    "- eclude any predictions to such classes also\n",
    "\n",
    "That is, we only investigate the predictions **from the most common classes to the most common classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOfrA8fSZXgh"
   },
   "outputs": [],
   "source": [
    "def clean_confusion_matrix(flat_true_labels, flat_predictions):\n",
    "    \"\"\" Returns a confusion matrix for the common classes \"\"\"\n",
    "    # These labels will be omitted\n",
    "    omitted = [label for label, count in \n",
    "               collections.Counter(flat_true_labels).items() if count < 10]\n",
    "    preds = []\n",
    "    reals = []\n",
    "    for real, pred in zip(flat_true_labels, flat_predictions):\n",
    "        if real not in omitted and pred not in omitted:\n",
    "            preds.append(pred)\n",
    "            reals.append(real)\n",
    "    columns = sorted(set(reals).union(set(preds)))\n",
    "    conf_matrix = pd.DataFrame(confusion_matrix(reals, preds),\n",
    "      columns=columns,\n",
    "      index=columns\n",
    "    )\n",
    "        \n",
    "    return conf_matrix\n",
    "\n",
    "def draw_heatmap(test_classes, predictions, accuracy, title):\n",
    "    \"\"\" Draws the heatmap based on the predictions \"\"\"\n",
    "    ccm = clean_confusion_matrix(test_classes, predictions)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(ccm, annot=True, fmt='d')\n",
    "    plt.title(f\"{title}, most common classes\\n\", fontsize=20)\n",
    "    plt.ylabel('Actual\\n', fontsize=16)\n",
    "    plt.xlabel(f'Predicted\\n\\nAccuracy score: {accuracy:.2f} %', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6or67w3ZXgj",
    "outputId": "b4cdfbe1-3130-48b9-dc43-477ec6393ff0"
   },
   "outputs": [],
   "source": [
    "best_bayes = bayes_results[30000]\n",
    "draw_heatmap(\n",
    "    df_test.reg,\n",
    "    best_bayes['test_predictions'],\n",
    "    best_bayes['test_accuracy'] * 100,\n",
    "    'Naive Bayes (max_features=30000)',\n",
    ")\n",
    "cr_bayes = classification_report(\n",
    "    df_test.reg,\n",
    "    best_bayes['test_predictions'], \n",
    "    labels=df_test.reg.unique(),\n",
    "    zero_division=0 \n",
    ")\n",
    "cr_bayes = sorted(cr_bayes.split('\\n')[2:-5], key=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0JRiu6BZXgn",
    "outputId": "89a866c6-497f-4a84-a4dd-555d4de21a71"
   },
   "outputs": [],
   "source": [
    "best_bow = bow_results[30000][1]\n",
    "draw_heatmap(\n",
    "    best_bow['true_test_labels'],\n",
    "    best_bow['test_predictions'],\n",
    "    best_bow['test_accuracy'] * 100,\n",
    "    'BOW (max_features=30000, learning_rate=0.003, optimizer=Adam)'\n",
    ")\n",
    "cr_bow = classification_report(\n",
    "    best_bow['true_test_labels'],\n",
    "    best_bow['test_predictions'],\n",
    "    labels=np.unique(best_bow['true_test_labels']),\n",
    "    zero_division=0 \n",
    ")\n",
    "cr_bow = sorted(cr_bow.split('\\n')[2:-5], key=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjw-znJTZXgq",
    "outputId": "79d26781-0cba-497e-82f8-ced1a0dd2144"
   },
   "outputs": [],
   "source": [
    "best_rnn = rnn_results[0.001]\n",
    "draw_heatmap(\n",
    "    best_rnn['true_test_labels'],\n",
    "    best_rnn['test_predictions'],\n",
    "    best_rnn['test_accuracy'] * 100,\n",
    "    'LSTM (learning_rate=0.001)'\n",
    ")\n",
    "cr_rnn = classification_report(\n",
    "    best_rnn['true_test_labels'],\n",
    "    best_rnn['test_predictions'],\n",
    "    labels=np.unique(best_rnn['true_test_labels']),\n",
    "    zero_division=0 \n",
    ")\n",
    "cr_rnn = sorted(cr_rnn.split('\\n')[2:-5], key=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tVHtfzeZXgu",
    "outputId": "c2e76cfe-ec7f-430d-9cbb-b03fdecdbdb6"
   },
   "outputs": [],
   "source": [
    "draw_heatmap(\n",
    "    keras_bert_results['true_labels'],\n",
    "    keras_bert_results['predictions'],\n",
    "    keras_bert_results['accuracy'],\n",
    "    'Keras BERT'\n",
    ")\n",
    "cr_kb = classification_report(\n",
    "    keras_bert_results['true_labels'],\n",
    "    keras_bert_results['predictions'],\n",
    "    labels=np.unique(keras_bert_results['true_labels']),\n",
    "    zero_division=0 \n",
    ")\n",
    "\n",
    "cr_kb = sorted(cr_kb.split('\\n')[2:-5], key=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XC-5Np3ZXgy",
    "outputId": "a2f9221c-d2ea-4439-e918-30edc2984c01"
   },
   "outputs": [],
   "source": [
    "pt_label_encoder = LabelEncoder()\n",
    "pt_label_encoder.fit_transform(df_train.reg)\n",
    "pt_reals = pt_label_encoder.inverse_transform(\n",
    "    [x for lst in pytorch_bert_results['true_labels'] for x in lst]\n",
    ")\n",
    "pt_preds = pt_label_encoder.inverse_transform(\n",
    "    [x for lst in pytorch_bert_results['predictions'] for x in lst]\n",
    ")\n",
    "draw_heatmap(\n",
    "    pt_reals,\n",
    "    pt_preds,\n",
    "    pytorch_bert_results['test_accuracy'],\n",
    "    'Pytorch BERT',\n",
    ")\n",
    "cr_ptb = classification_report(\n",
    "    pt_reals,\n",
    "    pt_preds,\n",
    "    labels=np.unique(pt_reals),\n",
    "    zero_division=0 \n",
    ")\n",
    "cr_ptb = sorted(cr_ptb.split('\\n')[2:-5], key=lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap analysis\n",
    "\n",
    "It comes as no surprise that the models perform in a fairly similar fashion (with LSTM being the most notable exception). It is even less of a surprise that the models do best when predicting the majority classes (particularly *MT OS* shining as a beacon for attention in the middle of every map).\n",
    "\n",
    "It can also be observed that at least on a superficial level, all of the models make some fairly similar mistakes (for example  making the mistake of predicting actual *DS IG, description with intent to sell*  as *DT IN, description of a thing*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\", \"Classification report: Milestone 1 models\".rjust(70, ' '), \"\\n\")\n",
    "for i, (r1, r2, r3) in enumerate(zip(cr_bayes, cr_bow, cr_rnn)):\n",
    "    if i % 3 == 0:\n",
    "        print('-' * 110)\n",
    "        print('precision'.rjust(30, ' '), 'recall'.rjust(25, ' '),\n",
    "              'f1'.rjust(20, ' '), 'support'.rjust(25, ' '))\n",
    "        for j in [33, 25, 25]: \n",
    "            print('BAYES    BOW   RNN'.rjust(j, ' '), end='')\n",
    "        print()\n",
    "    name = ''\n",
    "    nl = 0\n",
    "    for cell in r1.split():\n",
    "        if not cell.isalpha():\n",
    "            break\n",
    "        name += cell + ' '\n",
    "        nl += 1\n",
    "    s1, s2, s3 = r1.split()[nl:], r2.split()[nl:], r3.split()[nl:]\n",
    "    print(name.ljust(16, ' '), end='')\n",
    "    print(f\"{s1[0].ljust(6, ' ')} {s2[0].ljust(6, ' ')} {s3[0].ljust(6, ' ')}\",\n",
    "          end='  |  ')\n",
    "    print(f\"{s1[1].ljust(6, ' ')} {s2[1].ljust(6, ' ')} {s3[1].ljust(6, ' ')}\",\n",
    "          end='  |  ')\n",
    "    print(f\"{s1[2].ljust(6, ' ')} {s2[2].ljust(6, ' ')} {s3[2].ljust(6, ' ')}\",\n",
    "          end='  |  ')\n",
    "    print(f\"{s1[3].rjust(10, ' ').ljust(6, ' ')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error analysis for Milestone 1 models**\n",
    "The classification report confirms our initial observations. To put it bluntly, the higher larger the class, the better the performance. \n",
    "All of the models fail to predict the minority classes, but that is to be excpected considering the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2_cr = {}\n",
    "for i, cr in enumerate([cr_kb, cr_ptb]):\n",
    "    for crr in cr:\n",
    "        name = ''\n",
    "        nl = 0\n",
    "        for cell in crr.split():\n",
    "            if not cell.isalpha():\n",
    "                break\n",
    "            name += cell + ' '\n",
    "            nl += 1\n",
    "        res = crr.split()[nl:]\n",
    "        if i == 0:\n",
    "            ms2_cr[name] = [res, None]\n",
    "        if i == 1:\n",
    "            if name not in ms2_cr:\n",
    "                ms2_cr[name] = [None, res]\n",
    "            else:\n",
    "                keras_res = ms2_cr[name][0]\n",
    "                ms2_cr[name] = [keras_res, res]\n",
    "                \n",
    "print(\"\\n\", \"Classification report: Milestone 2 models\".rjust(60, ' '), \"\\n\")\n",
    "for i, (name, res) in enumerate(sorted(ms2_cr.items(), key=lambda x: x[0])):\n",
    "    if i % 3 == 0:\n",
    "        print('-' * 110)\n",
    "        print('precision'.rjust(30, ' '), 'recall'.rjust(12, ' '),\n",
    "              'f1'.rjust(15, ' '), 'support'.rjust(17, ' '))\n",
    "        for j in [30, 18, 18]: \n",
    "            print('Keras  PyTorch'.rjust(j, ' '), end='')\n",
    "        print()\n",
    "    print(name.ljust(16, ' '), end='')\n",
    "    r0 = res[0] or ['', '', '']\n",
    "    r1 = res[1] or ['', '', '']\n",
    "    print(f\"{r0[0].ljust(6, ' ')} {r1[0].ljust(6, ' ')}\", end='  |  ')\n",
    "    print(f\"{r0[1].ljust(6, ' ')} {r1[1].ljust(6, ' ')}\", end='  |  ')\n",
    "    print(f\"{r0[2].ljust(6, ' ')} {r1[2].ljust(6, ' ')}\", end='  |  ')\n",
    "    print(f\"{r1[3].rjust(5, ' ').ljust(6, ' ')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpC1Tg4Fa259"
   },
   "source": [
    "**Error analysis for multi class BERT.**\n",
    "\n",
    "From the classification report below we can see, that for multi class BERT precision (proportion of relevant documents among those returned, True Positives/True Positives+False Positives), recall (proportion of relevant documents found, True Positives/True Positives+False Negatives) and their balanced combination F1-score go hand in hand from label to label. Still the accuracies differ and we can see, that predicting the biggest labels is easiest for the model.\n",
    "\n",
    "It also seems that the Keras and PyTorch version perform in a quite similar fashion, which is of course the expected outcome due to the same base architecture and prelearning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the whales. And the GPU.\n",
    "if not IN_COLAB:\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMGKzcgmnel0"
   },
   "source": [
    "# Milestone 3.1: Bert (multi-LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGktxIL7nbuM"
   },
   "source": [
    "For milestone 3 we will train two multi-label classifiers, one using non-deep contextual representations, the other using Bert. In this setting, each label is assigned independently.\n",
    "\n",
    "According to the task \"each label is assigned independently\". We interpreted this to mean, that high level and sublevel labels have no connection and did not explore possible (and presumable) connections.\n",
    "However, we skimmed how to improve multilabel classification. According to [scikit-multilearn](http: //scikit. ml/labelrelations.html): \"Multi-label classification tends to have problems with overfitting and underfitting classifiers when the label space is large, especially in problem transformation approaches. A well known approach to remedy this is to split the problem into subproblems with smaller label subsets to improve the generalization quality.\"\n",
    "In our case we could improve the classification by training the separate models for high leve and sublevel registers, since the labels are not independent.\n",
    "\n",
    "Besides of this a priori knowledge relations between classes could be explored by scikit-multilearn tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "044qdQRmoJsm"
   },
   "source": [
    "Data preparation for milestone 3\n",
    "\n",
    "Since we do multi label classification we are dealing with pure highlevel and sublevel registers instead of combinations. This means we have significantly less unique labels (for example 'NA OP' becomes 'NA' and 'OP') and we have to prepare the data differently from the previous milestones.\n",
    "\n",
    "For convenience we manipulate a combined data set, where column 'data' holds the information, to which original dataset each row belongs to We will separate all the registers and one hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbOy2ffHo85A"
   },
   "outputs": [],
   "source": [
    "# prepare combined data set for data manipulation\n",
    "\n",
    "df_train['data'] = 'train'\n",
    "df_dev_full['data'] = 'dev'\n",
    "df_test_full['data'] = 'test'\n",
    "\n",
    "frames = [df_train, df_dev_full, df_test_full]\n",
    "df = pd.concat(frames).reset_index(drop=True) # get rid of the old indexes # this data used for milestone 3\n",
    "df = df.rename(columns={\"reg\": \"label\"}) # note here a technical detail: column names have been changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dQjCfcqoiUH"
   },
   "outputs": [],
   "source": [
    "# Custom function to separate all the labels for one hot encoding\n",
    "def from_registers(col):\n",
    "  lst = col.tolist() # column to list\n",
    "  registers = [[]] # list of lists, as long as the original column\n",
    "  idx = 0\n",
    "  for element in lst:    # NA OP\n",
    "      parts = element.split(' ') # NA #OP\n",
    "      for i in parts: \n",
    "        if i == '': # omit empty strings\n",
    "          continue\n",
    "        registers[idx].append(i) # append to list in list\n",
    "      if len(lst)-1 > idx : # if we have more labels to separate\n",
    "        idx = idx+1 \n",
    "        registers.insert(idx, []) # add new empty list to list\n",
    "  return registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "vmzr2ujroy9J",
    "outputId": "2a425473-9b62-4b66-bebc-4bf2d8444f83"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "all_regs=from_registers(df['label']) # use custom made function to list the registers\n",
    "df['regs'] = all_regs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzRXc0S2omkv"
   },
   "source": [
    "Next we will turn those listed registers (column 'regs') to one hot encodings with sklearn MultiLabelBinarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "f6r8tAriqqi_",
    "outputId": "3e77e2e6-d0bb-484d-e47c-8832d7475d97"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "dummies = pd.DataFrame(mlb.fit_transform(df['regs']), columns=mlb.classes_, index=df.index)\n",
    "# join the dummies to original data frame\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAXbLGxAr0CH"
   },
   "outputs": [],
   "source": [
    "# separate train, test and dev datas from one hot encoded combined dataset\n",
    "\n",
    "train = df[df['data'] == 'train'] \n",
    "test = df[df['data'] == 'test'] \n",
    "dev = df[df['data'] == 'dev']\n",
    "\n",
    "# separate X (features) and Y (labels) for training data and x and y for development data\n",
    "\n",
    "# train\n",
    "X_ = train['text'] # features\n",
    "Y_ = train.drop(['regs', 'label','text', 'data'], axis=1) # labels\n",
    "\n",
    "# dev\n",
    "x_ = dev['text']\n",
    "y_ = dev.drop(['regs', 'label','text', 'data'], axis=1)\n",
    "\n",
    "# test\n",
    "test_feats_ = test['text']\n",
    "true_labels_ = test.drop(['regs', 'label','text', 'data'], axis=1) # true labels (one-hot encoded) for testing the model predictions\n",
    "true_regs_ = test['regs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zi2eqnOjvmcV"
   },
   "source": [
    "## 3.1.1 Classifier 1: Suppor Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBk7KcZGsSg1"
   },
   "source": [
    "Now we prepare the first classifier, Linear support vector machine (SVM).\n",
    "\n",
    "To do multilabel classification we wrap the SVM with [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier).\n",
    "\n",
    "\n",
    "For this milestone we followed [scikit tutorial](https://https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) and this [tutorial](https://www.datatechnotes.com/2020/03/multi-output-classification-with-multioutputclassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQGRbTc2samc"
   },
   "outputs": [],
   "source": [
    "# form feature matrixes\n",
    "vectorizer = CountVectorizer(max_features = 20000, ngram_range = (1,1)) \n",
    "\n",
    "train_feature_matrix = vectorizer.fit_transform(X_)\n",
    "dev_feature_matrix = vectorizer.transform(x_)\n",
    "test_feature_matrix = vectorizer.transform(test_feats_)\n",
    "\n",
    "# Scale input data for SVM\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "train_fm = scaler.fit_transform(train_feature_matrix)\n",
    "dev_fm = scaler.transform(dev_feature_matrix)\n",
    "test_fm = scaler.transform(test_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ip98hqWas3jJ",
    "outputId": "5858999e-ae7b-46aa-dcd2-a03d82e9b751"
   },
   "outputs": [],
   "source": [
    "labels = Y_.columns.values.tolist() # labels/registers\n",
    "\n",
    "estimator = SGDClassifier(random_state=42, alpha=0.0001, max_iter=40, early_stopping=True)\n",
    "\n",
    "SGDC_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(estimator))\n",
    "            ])\n",
    "\n",
    "SGDC_cnfs = []\n",
    "\n",
    "for category in labels:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model\n",
    "    SGDC_pipeline.fit(train_feature_matrix, Y_[category])\n",
    "    # compute test accuracy\n",
    "    prediction = SGDC_pipeline.predict(test_fm)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(true_labels_[category], prediction)))\n",
    "    cnf = confusion_matrix(true_labels_[category], prediction)\n",
    "    SGDC_cnfs.append(cnf)\n",
    "\n",
    "print(\"Confusion matrices saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7SQ-j1svaBo"
   },
   "source": [
    "Abowe we have the label wise accuracies we will analyze the results at chapter 3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t2kB9AC5vhjl"
   },
   "source": [
    "## 3.1.2 Classifier 2: Bert (multi-LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkBpnM_JwRN1"
   },
   "source": [
    "First we repeat some data preparation steps taken with BERT in chapter 2 but this time for the one hot encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kCsg4EEjv448"
   },
   "outputs": [],
   "source": [
    "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
    "# Maximum number of examples to read \n",
    "# Here we acknowledge, that stratified sample would be preferable to keep the original label distribution\n",
    "MAX_EXAMPLES = 1500\n",
    "\n",
    "# Maximum length of input sequence in tokens\n",
    "INPUT_LENGTH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "c6HRlbDav-zb",
    "outputId": "a973df2b-076c-4376-a252-a959f1e7997a"
   },
   "outputs": [],
   "source": [
    "# To avoid OOM, truncate one hot encoded data\n",
    "\n",
    "train = truncate_data(train, MAX_EXAMPLES)\n",
    "dev = truncate_data(dev, MAX_EXAMPLES)\n",
    "test = truncate_data(test, MAX_EXAMPLES)\n",
    "\n",
    "frames = [train, dev, test]\n",
    "for d in frames:\n",
    "  print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "KynvvHB7wOyo",
    "outputId": "b997b501-e455-4634-fc11-b8d34eff5195"
   },
   "outputs": [],
   "source": [
    "# remember, INPUT_LENGTH is a parameter for make_model_inputs -function. \n",
    "# We also use BERT tokenizer here.\n",
    "\n",
    "X = make_model_inputs(train['text'])\n",
    "x = make_model_inputs(dev['text'])\n",
    "test_inp = make_model_inputs(test['text'])\n",
    "\n",
    "print(X[0].shape)\n",
    "print(x[0].shape)\n",
    "print(test_inp[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJPmQ3AfwhwI"
   },
   "source": [
    "Extract one hot label encodings from the truncated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvBQvIc9wnBS"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "Y = train.drop(['regs', 'label','text', 'data'], axis=1)\n",
    "\n",
    "# dev\n",
    "y = dev.drop(['regs','label','text', 'data'], axis=1)\n",
    "\n",
    "# test\n",
    "true_labels = test.drop(['regs', 'label','text', 'data'], axis=1).reset_index(drop=True) # these are used for evaluating model predictions\n",
    "true_regs = test['regs'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ec4obARvwtv2"
   },
   "source": [
    "Now the data is prepared for multilabel BERT. We will initialize the hyperparameters, set up the optimizer utilizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl73HMhpwuDH"
   },
   "outputs": [],
   "source": [
    "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
    "# Number of epochs to train for\n",
    "EPOCHS = 30\n",
    "\n",
    "# Optimizer learning rate\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ElZcAUaZw8Cq"
   },
   "outputs": [],
   "source": [
    "# Set up Adam optimizer for BERT justa as in milestone 2 but with multilabel BERT hyperparameters\n",
    "\n",
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=len(train['text']),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(\n",
    "    total_steps,\n",
    "    warmup_steps,\n",
    "    lr=LEARNING_RATE,\n",
    "    epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1p5FUr_xTvf"
   },
   "source": [
    "Again we will wrap the model. We reload the pretrained model with multilabel BERT hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ge3ab63xhwf"
   },
   "outputs": [],
   "source": [
    "pretrained_model = load_trained_model_from_checkpoint(\n",
    "    config_file = bert_config_path,\n",
    "    checkpoint_file = bert_checkpoint_path,\n",
    "    training = False, # ignore MLM [masked language modeling] and NSP [next sentence prediction] parts of the model\n",
    "    trainable = True,\n",
    "    seq_len = INPUT_LENGTH # define the size of input layer\n",
    ")\n",
    "\n",
    "bert_out = pretrained_model.outputs[0][:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDfrTn38xtN1"
   },
   "source": [
    "In multi-label classification instead of softmax() we use sigmoid() activation to get the probabilities for each label separatedly. \n",
    "\n",
    "For testing choosing the metrics and loss we followed these tutorials and guides [1](https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d), [2](https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/) and [3](https://https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "9Ag4i8Yzx5F_",
    "outputId": "2d6d9bdb-9dea-4fe2-907f-2682bbc6c371"
   },
   "outputs": [],
   "source": [
    "num_labels = len(mlb.classes_) # size of the output layer\n",
    "\n",
    "out = Dense(num_labels, activation='sigmoid')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
    "model = Model(                                          # sigmoid gives us independent probabilities for each class \n",
    "    inputs=pretrained_model.inputs,\n",
    "    outputs=[out]\n",
    ")\n",
    "\n",
    "# Now size of the last layer matches the number of the labels and activation (sigmoid) our multilabel purposes\n",
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChhMKZpCyFGk"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy', # one hot encoded labels!\n",
    "    metrics=['categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eDoR8gMyGFt"
   },
   "source": [
    "As a loss function we use the binary_crossentropy instead of categorical_crossentropy loss. This is because we want to penalize each output node independently. Categorical crossentropy was also trialed but it resulted in significantly worse results than binary crossentropy (Classification accuracy:  17.7 percent with lr 0.00001 and batch size 16). Hamming loss might also have been a suitable candidate, but it does not have Keras implementation at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "id": "nCOoZn4XyQbj",
    "outputId": "7b0b507e-5827-4427-b48b-7665f17a7ec2"
   },
   "outputs": [],
   "source": [
    "stop_cb = EarlyStopping(monitor = 'val_categorical_accuracy', patience= 8, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "mc_cb = ModelCheckpoint(filepath='models/BERT_multilabel.h5', monitor='val_categorical_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(x, y),\n",
    "    callbacks=[stop_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iO2M2FfydCY"
   },
   "source": [
    "Inspect the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "UOfLO3sRycI4",
    "outputId": "cfd23f1a-6d98-4102-fd14-c714da70fa07"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['categorical_accuracy'],label=\"Training set accuracy\")\n",
    "    plt.plot(history.history['val_categorical_accuracy'],label=\"Validation set accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d_fKatjylxK"
   },
   "source": [
    "Since training accuracy is only approximately 60% it seems the model can't fully utilize training data. Also BATCH_SIZEs 8 and 16 were tried but best results were gained with batch size 4. \n",
    "\n",
    "Accuracy as a metrics is problematic in multilabel classification. And in our case the data is highly biased: Most of the true occurrences fall in a couple of categories and we have many labels with only a few items belonging to them. Thus predicting for an input text NOT to belong to a certain class improves accuracy. We take a closer look at the model performance in chapter 3.2.\n",
    "\n",
    "Activation function could also be optimized. By creating a custom made activation function we could shift and optimize the classification threshold of 50% (default value of sigmoid activation). In the other hand this would lead in favoring some labels over the others, which might not be desireable.\n",
    "\n",
    "Now we will move on and predict: For each input we gain independet probabilities for belonging to each of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "OZXcWdkHzs1F",
    "outputId": "bdcaad3f-3bfd-45c7-9895-c471e948be3a"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_inp, batch_size = BATCH_SIZE)\n",
    "# Since we do multiclass, sum of probabilities is over 1.\n",
    "print(predictions) \n",
    "print(sum(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee2fDXnZ0GmV"
   },
   "source": [
    "We will transform predictions to one hot encodings with sigmoid threshold, 50%. This means we select those labels the model predicts with 50% or higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "FgUwg-zq0KJe",
    "outputId": "8370073c-eae4-4d3a-db3f-7d7081b89f8c"
   },
   "outputs": [],
   "source": [
    "preds = (predictions >= 0.50).astype(np.uint8) \n",
    "print(\"Predictions are binary sequences:\\n\", preds[0]) \n",
    "\n",
    "predicted_labels = mlb.inverse_transform(preds) # inverse transform gives the registers\n",
    "print(\"Inverse transform gives the predicted register name(s):\\n\" ,predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "FJ9-v9Zl03CI",
    "outputId": "44845ea1-a20c-4de4-b138-d54de0776742"
   },
   "outputs": [],
   "source": [
    "# Now we will utilize scikit-multilearn library and create confusion matrices for all the labels\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "cnfs = multilabel_confusion_matrix(true_labels, preds)\n",
    "cnfs.shape # object contains cnf matrix for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkdiiVDU1Ro6"
   },
   "source": [
    "# Milestone 3.2: Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFRnPMI41VYa"
   },
   "source": [
    "Now we will compare the results of the two multilabel classifiers. We will analyze the predictions in terms of label-specific differences. For this purpose use the confusion matrices generated for the both predictions.\n",
    "\n",
    "We loop over all the labels and plot binary confusion matrices of the both models side by side. Label specific model evaluation metrics will also be printed.\n",
    "\n",
    "In the confusion matrices we hope to see high numbers on the main diagonal: In such a case the model has predicted well for the instances to belong or not to belong to the relevant class.\n",
    "\n",
    "By taking a closer look at the matrices and the performance metrics we can see, how highly accuracy and true positive rate correlate from label to label. Predicting 0 (not belonging to a class) has been an encouraged strategy for both of our classifiers since accuracy has been the metrics. Since each text truly belongs only to a few classes, the probability of not belonging to most of them is high but for the classifiers this implies bad performance.\n",
    "\n",
    "Our SGDC seems to be eager to falsely predict the labels even in cases where the label does not belong to. In other words, false positive rate is high from label to label. But also recall (proportion of true positive labels found) is higher than for BERT.\n",
    "\n",
    "To further try to improve the multilabel BERT we also trained it having recall as metrics. This anyhow did not improve the model performance.\n",
    "\n",
    "Since BERT is trained with less training data (1500 instances compared to over 5000) we concentrate the model comparison to labels which appeared in BERT training data close to or over 200 times. These were OS, IN, MT, NE and NA, Na being single label appearing more over than 500 times in the BERT training data. In all these labels BERT has rather high precision and recall, while SGDC keeps producing false positives.\n",
    "\n",
    "Thus we can state, that BERT performs better in multilabel classification. The model would highly benefit from larger training data but because of the memory restrictions we could not afford exceeding the set limit of 1500 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "idPlmYy42PBw",
    "outputId": "f21c570c-3b19-450a-8c55-28956f506860"
   },
   "outputs": [],
   "source": [
    "# for the plot and analysis code from \n",
    "# https://stackoverflow.com/questions/40887753/display-matrix-values-and-colormap and \n",
    "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "# was utilized\n",
    "\n",
    "idx = 0\n",
    "for label in mlb.classes_:\n",
    "\n",
    "  # Set up pic frame: two subplots\n",
    "  plt.subplots_adjust(wspace=0.8, hspace=0.6)\n",
    "  fig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "  ax1, ax2 = axes\n",
    "  label = mlb.classes_[idx]\n",
    "\n",
    "  # BERT confusion matrix:\n",
    "  matrix = cnfs[idx]\n",
    "\n",
    "  FP = matrix[0,1].astype(float) # False positives\n",
    "  TP = matrix[1,1].astype(float) # True positives\n",
    "  FN = matrix[1,0].astype(float) # False negatives\n",
    "  TN = matrix[0,0].astype(float) # True negatives\n",
    "  \n",
    "  FPR = round(FP/(FP+TN) * 100, 1) # False positive rate\n",
    "  TPR = round(TP/(TP+FN) * 100, 1) # True positive rate\n",
    "  TNR = round(TN/(TN+FP) * 100, 1) # Specificity or true negative rate\n",
    "  PRE = round(TP/(TP+FP) * 100, 1) # Precision or positive predictive value\n",
    "  REC = round(TP/(TP+FN) * 100, 1) # Recall\n",
    "  ACC = round((TP+TN)/(TP+FP+FN+TN) * 100, 1) # Overall accuracy\n",
    "  NPV = round(TN/(TN+FN) * 100, 1) # Negative predictive value\n",
    "  FNR = round(FN/(TP+FN) * 100, 1) # False negative rate\n",
    "  FDR = round(FP/(TP+FP) * 100, 1) # False discovery rate\n",
    "  #F1 = round((2*((PRE*REC)/(PRE+REC)))* 100, 1) # Balance between precision and recall\n",
    "\n",
    "  for i in range(2):\n",
    "    for j in range(2):\n",
    "        c = matrix[j,i]\n",
    "        ax1.text(i, j, str(c), va='center', ha='center')\n",
    "\n",
    "  im1 = ax1.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "  ax1.set_title(\"\\n BERT performance for label \\\"{}\\\" \\n True positive rate {}\\n False positive rate {}\\n True negative rate {}\\n False negative rate {}\\n Precision {}\\n Recall {}\\n False discovery rate {}\\n False negative rate {}\\n Accuracy {}\\n\". format(label, TPR, FPR, TNR, FNR, PRE, REC, FDR, FNR, ACC), fontsize=16)\n",
    "  \n",
    "  # Second matrix (SGDC)\n",
    "  matrix = SGDC_cnfs[idx]\n",
    "\n",
    "  FP = matrix[0,1].astype(float) # False positives\n",
    "  TP = matrix[1,1].astype(float) # True positives\n",
    "  FN = matrix[1,0].astype(float) # False negatives\n",
    "  TN = matrix[0,0].astype(float) # True negatives\n",
    "  \n",
    "  FPR = round(FP/(FP+TN) * 100, 1) # False positive rate\n",
    "  TPR = round(TP/(TP+FN) * 100, 1) # True positive rate\n",
    "  TNR = round(TN/(TN+FP) * 100, 1) # Specificity or true negative rate\n",
    "  FNR = round(FN/(TP+FN) * 100, 1) # False negative rate\n",
    "  PRE = round(TP/(TP+FP) * 100, 1) # Precision or positive predictive value\n",
    "  REC = round(TP/(TP+FN) * 100, 1) # Recall: proportion of relevant documents found\n",
    "  ACC = round((TP+TN)/(TP+FP+FN+TN) * 100, 1) # Overall accuracy\n",
    "  NPV = round(TN/(TN+FN) * 100, 1) # Negative predictive value\n",
    "  FDR = round(FP/(TP+FP) * 100, 1) # False discovery rate\n",
    "  #F1 = round((2*((PRE*REC)/(PRE+REC)))* 100, 1) # Balance between precision and recall\n",
    "\n",
    "  im2 = ax2.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "  ax2.set_title(\"\\n SGDC performance for label \\\"{}\\\" \\n True positive rate {}\\n False positive rate {}\\n True negative rate {}\\n False negative rate {}\\n Precision {}\\n Recall {}\\n False discovery rate {}\\n False negative rate {}\\n Accuracy {}\\n\". format(label, TPR, FPR, TNR, FNR, PRE, REC, FDR, FNR, ACC), fontsize=16)\n",
    "\n",
    "  for i in range(2):\n",
    "    for j in range(2):\n",
    "      c = matrix[j,i]\n",
    "      ax2.text(i, j, str(c), va='center', ha='center')\n",
    "\n",
    "  for axis in axes:\n",
    "    axis.set_ylabel(\"True\", fontsize = 16)\n",
    "    axis.set_xlabel(\"\\nPredicted\", fontsize = 16)\n",
    "\n",
    "  idx = idx +1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "DLHLT_2020_Group2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langtech",
   "language": "python",
   "name": "langtech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
