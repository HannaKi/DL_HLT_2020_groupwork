{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"langtech","language":"python","name":"langtech"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"Copy of DLHLT2020.ipynb","provenance":[{"file_id":"https://github.com/tjokela/Text_Classification_DLHLT/blob/master/DLHLT2020.ipynb","timestamp":1586783797968}]}},"cells":[{"cell_type":"code","metadata":{"id":"yRSctb4Gj1Oa","colab_type":"code","colab":{}},"source":["import libvoikko\n","\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.corpus import stopwords\n","from pathlib import Path\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import LabelEncoder\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Dense, Embedding, Input, LSTM, SimpleRNN\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmRYQtz-j1PN","colab_type":"code","colab":{}},"source":["register_map = {\n","    'NA': {\n","        'description': 'Narrative',\n","        'sub': {\n","            'NE': 'New reports / news blogs', \n","            'SR': 'Sports reports',\n","            'PB': 'Personal blog', \n","            'HA': 'Historical article',\n","            'FC': 'Fiction', \n","            'TB': 'Travel blog',\n","            'CB': 'Community blogs', \n","            'OA': 'Online article',\n","        }\n","    },\n","    'OP': {\n","        'description': 'Opinion',\n","        'sub': {\n","            'OB': 'Personal opinion blogs', \n","            'RV': 'Reviews',\n","            'RS': 'Religious blogs/sermons',\n","            'AV': 'Advice',\n","        }\n","    },\n","    'IN': {\n","        'description': 'Informal description',\n","        'sub': {\n","            'JD': 'Job description', \n","            'FA': 'FAQs',\n","            'DT': 'Description of a thing', \n","            'IB': 'Information blogs',\n","            'DP': 'Description of a person',\n","            'RA': 'Research articles',\n","            'LT': 'Legal terms / conditions',\n","            'CM': 'Course materials',\n","            'EN': 'Encyclopedia articles',\n","            'RP': 'Report',\n","        }\n","    },\n","    'ID': {\n","        'description': 'Interactive discussion',\n","        'sub': {\n","            'DF': 'Discussion forums',\n","            'QA': 'Question-answer forums',\n","        }\n","    },\n","    'HI': {\n","        'description': 'How-to/instructions',\n","        'sub': {\n","            'RE': 'Recipes',\n","        }\n","    },\n","    'IP': {\n","        'description': 'Informational persuasion',\n","        'sub': {\n","        }\n","    },\n","    'IG': {\n","        'description': 'Informational persuasion',\n","        'sub': {\n","            'DS': 'Description with intent to sell',\n","            'EB': 'News-opinion blogs / editorials',\n","        }\n","    },\n","    'LY': {\n","        'description': 'Lyrical',\n","        'sub': {\n","            'PO': 'Poems',\n","            'SL': 'Songs',\n","        }\n","    },\n","    'SP': {\n","        'description': 'Spoken',\n","        'sub': {\n","            'IT': 'Interviews', \n","            'FS': 'Formal speeches',\n","        }\n","    },\n","    'OS': {\n","        'description': 'Others',\n","        'sub': {\n","             'MT': 'Machine-translated / generated texts',\n","        }\n","    }\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bsfzy1Bj1P6","colab_type":"code","colab":{}},"source":["# high level registers\n","hl_reg = list(register_map.keys())\n","# sub level registers\n","sl_reg = [s for sublist in register_map.values() for s in sublist['sub'].keys()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6w-HlevRj1QJ","colab_type":"code","colab":{}},"source":["class Transformer:\n","    def __init__(self):\n","        self.voikko = libvoikko.Voikko(u\"fi\")\n","        \n","    def as_baseform(self, word):\n","        \"\"\" A lemmatized form of the word \"\"\"\n","        try:\n","            return self.voikko.analyze(word)[0]['BASEFORM']\n","        except IndexError:\n","            return word\n","        \n","    def suggest(self, word):\n","        \"\"\" Spell-checked form of the word \"\"\"\n","        try:\n","            return self.voikko.suggest(word)[0]\n","        except IndexError:\n","            return word\n","        \n","    def transform(self, word):\n","        return self.as_baseform(self.suggest(word))\n","\n","transformer = Transformer()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZI0lEUFj1QS","colab_type":"code","colab":{},"outputId":"b37199da-d1d8-48a2-b0b4-ca643de6b325"},"source":["df_train_file = Path('./df_train.p')\n","df_dev_file = Path('./df_dev.p')\n","df_test_file = Path('./df_test.p')\n","\n","if df_train_file.is_file() and df_dev_file.is_file() and df_test_file.is_file():\n","    df_train = pickle.load(open(df_train_file, 'rb'))\n","    df_dev = pickle.load(open(df_dev_file, 'rb'))\n","    df_test = pickle.load(open(df_test_file, 'rb'))\n","    print(\"DataFrames loaded from files.\")\n","else:\n","    print(\"Generating DataFrames. This WILL take time.\")\n","    orig_columns = ['reg', 'text']\n","\n","    df_train = pd.read_csv('fincore-train.tsv', sep='\\t', names=orig_columns)\n","    df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n","    df_train = df_train.reindex(columns=orig_columns + hl_reg + sl_reg)\n","\n","    df_dev = pd.read_csv('fincore-dev.tsv', sep='\\t', names=orig_columns)\n","    df_dev = df_dev.sample(frac=1, random_state=42).reset_index(drop=True)\n","    df_dev = df_dev.reindex(columns=orig_columns + hl_reg + sl_reg)\n","\n","    df_test = pd.read_csv('fincore-test.tsv', sep='\\t', names=orig_columns)\n","    df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n","    df_test = df_test.reindex(columns=orig_columns + hl_reg + sl_reg)\n","\n","    # nested loops go brrrrrrrr.....\n","    for df in [df_train, df_dev, df_test]:\n","        # warning: insanely slow !!!\n","        df['transformed'] = df['text'].apply(lambda text:\n","            ' '.join([transformer.transform(word) for word in text.split(' ')\n","                      if word not in stopwords.words('finnish')]))\n","        df['reg'] = df['reg'].apply(lambda x: x.strip(' ').replace(' ', '_'))\n","        df[hl_reg + sl_reg] = 0\n","        for idx, row in df.iterrows():\n","            for reg in row['reg'].split('_'):\n","                if not reg:\n","                    continue\n","                df.iloc[idx, df.columns.get_loc(reg)] = 1\n","    pickle.dump(df_train, open(df_train_file, 'wb'))\n","    pickle.dump(df_dev, open(df_dev_file, 'wb'))\n","    pickle.dump(df_test, open(df_test_file, 'wb'))\n","    print(\"Done.\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["DataFrames loaded from files.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ncDq7uhrj1Qk","colab_type":"code","colab":{},"outputId":"74585369-583f-4d6d-a3a8-af230098aa45"},"source":["df_train.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reg</th>\n","      <th>text</th>\n","      <th>NA</th>\n","      <th>OP</th>\n","      <th>IN</th>\n","      <th>ID</th>\n","      <th>HI</th>\n","      <th>IP</th>\n","      <th>IG</th>\n","      <th>LY</th>\n","      <th>...</th>\n","      <th>QA</th>\n","      <th>RE</th>\n","      <th>DS</th>\n","      <th>EB</th>\n","      <th>PO</th>\n","      <th>SL</th>\n","      <th>IT</th>\n","      <th>FS</th>\n","      <th>MT</th>\n","      <th>transformed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CB_NA</td>\n","      <td>lauantai Lauantai oli taas mukavan kylmä päiv...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>lauantai lauantai taas mukava kylmä Päivä ( +...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>DT_IN</td>\n","      <td>Aurinkolämmöllä tarkoitetaan järjestelmää , j...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>aurinkolämpö tarkoittaa järjestelmä , käyttää...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>PB_NA</td>\n","      <td>Sivut torstai 14. kesäkuuta 2012 Vaihde vapaa...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>sivu torstai 14 kesäkuu 2012 vaihde vapaa kes...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HI</td>\n","      <td>Valitse kieli : Hae rahoitusta EEP-rahoitusta...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>valita kieliä : hakea rahoitus EEP-rahoitusta...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PB_NA</td>\n","      <td>Friday , 13 May 2011 Näin aika kuluu Vanhoja ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Friday , 13 Kay 2011 nähdä aika kulua vanha k...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 45 columns</p>\n","</div>"],"text/plain":["     reg                                               text  NA  OP  IN  ID  \\\n","0  CB_NA   lauantai Lauantai oli taas mukavan kylmä päiv...   1   0   0   0   \n","1  DT_IN   Aurinkolämmöllä tarkoitetaan järjestelmää , j...   0   0   1   0   \n","2  PB_NA   Sivut torstai 14. kesäkuuta 2012 Vaihde vapaa...   1   0   0   0   \n","3     HI   Valitse kieli : Hae rahoitusta EEP-rahoitusta...   0   0   0   0   \n","4  PB_NA   Friday , 13 May 2011 Näin aika kuluu Vanhoja ...   1   0   0   0   \n","\n","   HI  IP  IG  LY  ...  QA  RE  DS  EB  PO  SL  IT  FS  MT  \\\n","0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   \n","1   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   \n","2   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   \n","3   1   0   0   0  ...   0   0   0   0   0   0   0   0   0   \n","4   0   0   0   0  ...   0   0   0   0   0   0   0   0   0   \n","\n","                                         transformed  \n","0   lauantai lauantai taas mukava kylmä Päivä ( +...  \n","1   aurinkolämpö tarkoittaa järjestelmä , käyttää...  \n","2   sivu torstai 14 kesäkuu 2012 vaihde vapaa kes...  \n","3   valita kieliä : hakea rahoitus EEP-rahoitusta...  \n","4   Friday , 13 Kay 2011 nähdä aika kulua vanha k...  \n","\n","[5 rows x 45 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"7a3C-Q1fj1Q0","colab_type":"text"},"source":["## Milestone 1"]},{"cell_type":"code","metadata":{"id":"tQs4ZNPUj1Q3","colab_type":"code","colab":{},"outputId":"dd481ce5-941f-41e0-f5fc-8816e8791b93"},"source":["# only dev / test rows the labels of which are present in the training data\n","train_labels = set(df_train['reg'])\n","df_dev_ms1 = df_dev[df_dev['reg'].isin(train_labels)]\n","df_test_ms1 = df_test[df_test['reg'].isin(train_labels)]\n","\n","texts = {\n","    'normal': {\n","        'train': [x for x in df_train['text']],\n","        'dev': [x for x in df_dev_ms1['text']],\n","        'test': [x for x in df_test_ms1['text']],\n","    },\n","    'transformed': {\n","        'train': [x for x in df_train['transformed']],\n","        'dev': [x for x in df_dev_ms1['transformed']],\n","        'test': [x for x in df_test_ms1['transformed']],\n","    }\n","}\n","\n","label_encoder = LabelEncoder()\n","train_class_numbers = label_encoder.fit_transform(df_train['reg'])\n","dev_class_numbers = label_encoder.transform(df_dev_ms1['reg'])\n","test_class_numbers = label_encoder.transform(df_test_ms1['reg'])\n","\n","ms1_data_file = Path('./ms1_data.p')\n","if ms1_data_file.is_file():\n","    ms1_data = pickle.load(open(ms1_data_file, 'rb'))\n","    print(\"Loaded Milestone 1 data from a file.\")\n","else:\n","    print(\"Generating Milestone 1 data.\")\n","    ms1_data = {}\n","    for txt in ['normal', 'transformed']:\n","        ms1_data[txt] = {}\n","        for mf in [1000, 5000, 10000, 20000]:\n","            vectorizer = CountVectorizer(max_features=mf,\n","                                         binary=True,\n","                                         ngram_range=(1, 1))\n","            train_feature_matrix = vectorizer.fit_transform(texts[txt]['train'])\n","            dev_feature_matrix = vectorizer.transform(texts[txt]['dev'])\n","            test_feature_matrix = vectorizer.transform(texts[txt]['test'])\n","            ms1_data[txt][fm] = {\n","                'vectorizer': vectorizer,\n","                'train_feature_matrix': train_feature_matrix,\n","                'dev_feature_matrix': dev_feature_matrix,\n","                'test_feature_matrix': test_feature_matrix,\n","            }\n","    pickle.dump(ms1_data, open(ms1_data_file, 'wb'))\n","    print(\"Done.\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded Milestone 1 data from a file.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"--JghpNGj1RA","colab_type":"text"},"source":["### 1.0: Baseline (naive Bayes)"]},{"cell_type":"code","metadata":{"id":"HbLsMoT9j1RF","colab_type":"code","colab":{}},"source":["def mnb_compute(X, y, mnb):\n","    predictions = mnb.predict(X)\n","    return (np.sum(predictions == y) / len(y), predictions)\n","\n","bayes_results = {}\n","for txt, sets in ms1_data.items():\n","    bayes_results[txt] = {}\n","    for max_feats, stats in sets.items():\n","        mnb = MultinomialNB()\n","        mnb.fit(stats['train_feature_matrix'], df_train['reg'])\n","        \n","        train_accuracy, _ = mnb_compute(\n","            stats['train_feature_matrix'], df_train['reg'], mnb)\n","        \n","        dev_accuracy, _ = mnb_compute(\n","            stats['dev_feature_matrix'], df_dev_ms1['reg'], mnb)\n","        \n","        test_accuracy, test_predictions = mnb_compute(\n","            stats['test_feature_matrix'], df_test_ms1['reg'], mnb)\n","        \n","        # accuracy on permutated test set\n","        perm_accuracy, _ = mnb_compute(\n","            stats['test_feature_matrix'],\n","            np.random.permutation(df_test_ms1['reg']),\n","            mnb\n","        )\n","        \n","        bayes_results[txt][max_feats] = {\n","            'model': mnb,\n","            'train_accuracy': train_accuracy,\n","            'dev_accuracy': dev_accuracy,\n","            'test_accuracy': test_accuracy,\n","            'test_predictions': test_predictions,\n","            'perm_accuracy': perm_accuracy,\n","        }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_op0t9dkj1RO","colab_type":"code","colab":{},"outputId":"f01020bd-540d-4e83-a017-7d998513d432"},"source":["for txt, sets in bayes_results.items():\n","    print(f\"\\nResults for {txt} texts.\")\n","    for mf, stats in sets.items():\n","        print(f\"Max features: {mf}\",\n","              f\"\\tTrain Accuracy: {stats['train_accuracy']:.2f}\",\n","              f\"\\tDev accuracy: {stats['dev_accuracy']:.2f}\",\n","              f\"\\tTest accuracy: {stats['test_accuracy']:.2f}\"\n","              f\"\\tPermutated testset accuracy: {stats['perm_accuracy']:.2f}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Results for normal texts.\n","Max features: 1000 \tTrain Accuracy: 0.68 \tDev accuracy: 0.53 \tTest accuracy: 0.50\tPermutated testset accuracy: 0.07\n","Max features: 5000 \tTrain Accuracy: 0.79 \tDev accuracy: 0.59 \tTest accuracy: 0.55\tPermutated testset accuracy: 0.06\n","Max features: 10000 \tTrain Accuracy: 0.81 \tDev accuracy: 0.59 \tTest accuracy: 0.56\tPermutated testset accuracy: 0.09\n","Max features: 20000 \tTrain Accuracy: 0.81 \tDev accuracy: 0.56 \tTest accuracy: 0.55\tPermutated testset accuracy: 0.09\n","\n","Results for transformed texts.\n","Max features: 1000 \tTrain Accuracy: 0.69 \tDev accuracy: 0.51 \tTest accuracy: 0.51\tPermutated testset accuracy: 0.06\n","Max features: 5000 \tTrain Accuracy: 0.76 \tDev accuracy: 0.56 \tTest accuracy: 0.56\tPermutated testset accuracy: 0.07\n","Max features: 10000 \tTrain Accuracy: 0.76 \tDev accuracy: 0.56 \tTest accuracy: 0.54\tPermutated testset accuracy: 0.08\n","Max features: 20000 \tTrain Accuracy: 0.73 \tDev accuracy: 0.53 \tTest accuracy: 0.51\tPermutated testset accuracy: 0.09\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1OXua3HHj1Ra","colab_type":"text"},"source":["### 1.1: BOW"]},{"cell_type":"code","metadata":{"id":"S2T61c5Dj1Rd","colab_type":"code","colab":{},"outputId":"1f118af8-f281-4ad5-c126-5486331af4f9"},"source":["bow_results = {}\n","\n","example_count = len(df_train)\n","class_count = len(label_encoder.classes_)\n","\n","for txt, sets in ms1_data.items():\n","    bow_results[txt] = {}\n","    for max_feats, stats in sets.items():\n","        bow_results[txt][max_feats] = {}\n","        for optimizer in ['sgd', 'adam']:\n","            print(f\"Training BOW for {txt} text \"\n","                  f\"with params: optimizer = {optimizer}, \"\n","                  f\"count vectorizer max_features = {max_feats}\") \n","            \n","            train_fm = stats['train_feature_matrix']\n","            dev_fm = stats['dev_feature_matrix']\n","            test_fm = stats['test_feature_matrix']\n","\n","            inp = Input(shape=(max_feats,))\n","            hidden = Dense(300, activation=\"relu\")(inp)\n","            outp = Dense(class_count, activation='softmax')(hidden)\n","            model = Model(inputs=[inp], outputs=[outp])\n","\n","            model.compile(optimizer=optimizer,\n","                          loss='sparse_categorical_crossentropy',\n","                          metrics=['accuracy'])\n","\n","            mc = ModelCheckpoint(filepath='/tmp/bow_model.h5',\n","                                 monitor='val_loss',\n","                                 verbose=0,\n","                                 save_best_only=True,\n","                                 mode='auto')\n","\n","            es = EarlyStopping(monitor='val_loss',\n","                               patience=10,\n","                               verbose=0,\n","                               restore_best_weights=True)\n","            \n","            hist = model.fit(train_fm.toarray(),\n","                             train_class_numbers,\n","                             batch_size=32,\n","                             verbose=0,\n","                             epochs=100,\n","                             callbacks=[mc, es],\n","                             validation_data=(dev_fm.toarray(), dev_class_numbers))\n","            \n","            bow_results[txt][max_feats][optimizer] = {\n","                'model': model,\n","                'hist': hist,\n","                'results': {}\n","            }\n","            \n","            for name, fm_cls in zip (['Train', 'Dev', 'Test'],\n","                                     [(train_fm, train_class_numbers),\n","                                      (dev_fm, dev_class_numbers),\n","                                      (test_fm, test_class_numbers)]):\n","                fm, cls_num = fm_cls\n","                preds = model.predict(fm.toarray())\n","                acc = (np.sum(np.equal(cls_num, np.argmax(preds, axis=1)))\n","                       / len(cls_num))\n","                bow_results[txt][max_feats][optimizer]['results'][name] = {\n","                    'accuracy': acc,\n","                    'preds': preds\n","                }\n","                \n","print(\"Done.\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training BOW for normal text with params: optimizer = sgd, count vectorizer max_features = 1000\n","Training BOW for normal text with params: optimizer = adam, count vectorizer max_features = 1000\n","Training BOW for normal text with params: optimizer = sgd, count vectorizer max_features = 5000\n","Training BOW for normal text with params: optimizer = adam, count vectorizer max_features = 5000\n","Training BOW for normal text with params: optimizer = sgd, count vectorizer max_features = 10000\n","Training BOW for normal text with params: optimizer = adam, count vectorizer max_features = 10000\n","Training BOW for normal text with params: optimizer = sgd, count vectorizer max_features = 20000\n","Training BOW for normal text with params: optimizer = adam, count vectorizer max_features = 20000\n","Training BOW for transformed text with params: optimizer = sgd, count vectorizer max_features = 1000\n","Training BOW for transformed text with params: optimizer = adam, count vectorizer max_features = 1000\n","Training BOW for transformed text with params: optimizer = sgd, count vectorizer max_features = 5000\n","Training BOW for transformed text with params: optimizer = adam, count vectorizer max_features = 5000\n","Training BOW for transformed text with params: optimizer = sgd, count vectorizer max_features = 10000\n","Training BOW for transformed text with params: optimizer = adam, count vectorizer max_features = 10000\n","Training BOW for transformed text with params: optimizer = sgd, count vectorizer max_features = 20000\n","Training BOW for transformed text with params: optimizer = adam, count vectorizer max_features = 20000\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PeuFDN5Bj1Rq","colab_type":"code","colab":{},"outputId":"fafbef58-e764-46ac-d8c6-2704fb29cfcc"},"source":["for txt, sets in bow_results.items():\n","    print(f\"\\nResults for {txt} texts.\")\n","    for mf, optim_stats in sets.items():\n","        for optim, stats in optim_stats.items():\n","            print(f\"Max features: {mf}\",\n","                  f\"\\tOptimizer: {optim.ljust(4, ' ')}\",\n","                  f\"\\tTrain Accuracy: {stats['results']['Train']['accuracy']:.2f}\",\n","                  f\"\\tDev accuracy: {stats['results']['Dev']['accuracy']:.2f}\",\n","                  f\"\\tTest accuracy: {stats['results']['Test']['accuracy']:.2f}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Results for normal texts.\n","Max features: 1000 \tOptimizer: sgd  \tTrain Accuracy: 0.69 \tDev accuracy: 0.49 \tTest accuracy: 0.50\n","Max features: 1000 \tOptimizer: adam \tTrain Accuracy: 0.83 \tDev accuracy: 0.51 \tTest accuracy: 0.51\n","Max features: 5000 \tOptimizer: sgd  \tTrain Accuracy: 0.82 \tDev accuracy: 0.55 \tTest accuracy: 0.53\n","Max features: 5000 \tOptimizer: adam \tTrain Accuracy: 0.92 \tDev accuracy: 0.56 \tTest accuracy: 0.55\n","Max features: 10000 \tOptimizer: sgd  \tTrain Accuracy: 0.86 \tDev accuracy: 0.56 \tTest accuracy: 0.54\n","Max features: 10000 \tOptimizer: adam \tTrain Accuracy: 0.96 \tDev accuracy: 0.57 \tTest accuracy: 0.57\n","Max features: 20000 \tOptimizer: sgd  \tTrain Accuracy: 0.87 \tDev accuracy: 0.56 \tTest accuracy: 0.54\n","Max features: 20000 \tOptimizer: adam \tTrain Accuracy: 0.98 \tDev accuracy: 0.58 \tTest accuracy: 0.56\n","\n","Results for transformed texts.\n","Max features: 1000 \tOptimizer: sgd  \tTrain Accuracy: 0.72 \tDev accuracy: 0.54 \tTest accuracy: 0.52\n","Max features: 1000 \tOptimizer: adam \tTrain Accuracy: 0.74 \tDev accuracy: 0.54 \tTest accuracy: 0.54\n","Max features: 5000 \tOptimizer: sgd  \tTrain Accuracy: 0.82 \tDev accuracy: 0.56 \tTest accuracy: 0.54\n","Max features: 5000 \tOptimizer: adam \tTrain Accuracy: 0.93 \tDev accuracy: 0.56 \tTest accuracy: 0.56\n","Max features: 10000 \tOptimizer: sgd  \tTrain Accuracy: 0.84 \tDev accuracy: 0.56 \tTest accuracy: 0.55\n","Max features: 10000 \tOptimizer: adam \tTrain Accuracy: 0.96 \tDev accuracy: 0.57 \tTest accuracy: 0.57\n","Max features: 20000 \tOptimizer: sgd  \tTrain Accuracy: 0.86 \tDev accuracy: 0.57 \tTest accuracy: 0.55\n","Max features: 20000 \tOptimizer: adam \tTrain Accuracy: 0.98 \tDev accuracy: 0.57 \tTest accuracy: 0.57\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3LR5v97Cj1R3","colab_type":"text"},"source":["### 1.2: RNN"]},{"cell_type":"code","metadata":{"id":"3bSRohI0j1R4","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(num_words=100000, lower=True, split=' ', char_level=False)\n","tokenizer.fit_on_texts(texts['normal']['train'])\n","train_seq = tokenizer.texts_to_sequences(texts['normal']['train'])\n","dev_seq = tokenizer.texts_to_sequences(texts['normal']['dev'])\n","test_seq = tokenizer.texts_to_sequences(texts['normal']['test'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bz9WhdSxj1SF","colab_type":"code","colab":{}},"source":["train_lens = [len(s) for s in train_seq]\n","dev_lens = [len(s) for s in dev_seq]\n","test_lens = [len(s) for s in test_seq]\n","\n","train_max_len, train_min_len, train_mean_len = max(train_lens), min(train_lens), int(np.mean(train_lens))\n","dev_max_len, dev_min_len, dev_mean_len = max(dev_lens), min(dev_lens), int(np.mean(dev_lens))\n","test_max_len, test_min_len, test_mean_len = max(test_lens), min(test_lens), int(np.mean(test_lens))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4GdzRPLEj1SM","colab_type":"code","colab":{},"outputId":"47f1cc30-8645-4424-cc24-03352f9e1be5"},"source":["print(train_max_len, dev_max_len, test_max_len)\n","print(train_min_len, dev_min_len, test_min_len)\n","print(train_mean_len, dev_mean_len, test_mean_len)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["81038 13906 81724\n","0 19 0\n","584 530 564\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"be6VHDivj1Sa","colab_type":"code","colab":{}},"source":["train_seq = pad_sequences(train_seq,\n","                          250,\n","                          padding='post', \n","                          truncating='post',\n","                          value=0)\n","\n","dev_seq = pad_sequences(dev_seq,\n","                        250,\n","                        padding='post', \n","                        truncating='post',\n","                        value=0)\n","\n","test_seq = pad_sequences(test_seq,\n","                         250,\n","                         padding='post', \n","                         truncating='post',\n","                         value=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANBwJCSTj1St","colab_type":"code","colab":{}},"source":["def build_rnn_model(RNN_class, \n","                    sequence_length, \n","                    vocab_size,\n","                    num_classes,\n","                    embedding_dim=250,\n","                    rnn_units=50):\n","    \n","    input_ = Input(shape=(sequence_length,))\n","    embedding = Embedding(vocab_size, embedding_dim)(input_)\n","    # return_sequences=False is the default\n","    rnn = RNN_class(rnn_units, return_sequences=False)(embedding)\n","    output = Dense(num_classes, activation='softmax')(rnn)\n","    return Model(inputs=[input_], outputs=[output])  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6kOPzVmj1S0","colab_type":"code","colab":{},"outputId":"04d175a4-c60f-43f7-9bdb-214bfcd9cf62"},"source":["lstm_model = build_rnn_model(LSTM, 250, tokenizer.num_words, len(train_labels), embedding_dim=250)\n","lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","lstm_history = lstm_model.fit(train_seq, train_class_numbers, epochs=3, batch_size=1, validation_split=0.1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 4765 samples, validate on 530 samples\n","Epoch 1/3\n","4765/4765 [==============================] - 916s 192ms/sample - loss: 3.0709 - accuracy: 0.2086 - val_loss: 2.8131 - val_accuracy: 0.2660\n","Epoch 2/3\n","4765/4765 [==============================] - 924s 194ms/sample - loss: 2.5369 - accuracy: 0.3362 - val_loss: 2.6573 - val_accuracy: 0.3245\n","Epoch 3/3\n","4765/4765 [==============================] - 903s 189ms/sample - loss: 1.9225 - accuracy: 0.5047 - val_loss: 2.5919 - val_accuracy: 0.3774\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KLlTy9u9j1S6","colab_type":"code","colab":{}},"source":["\"\"\"\n","simple_model = build_rnn_model(SimpleRNN, train_max_len, tokenizer.num_words, len(train_labels))\n","simple_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","simple_history = simple_model.fit(train_seq, train_class_numbers, epochs=3, batch_size=1, validation_split=0.1, verbose=1)\n","\"\"\"\n","pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBXwSel4j1TB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}