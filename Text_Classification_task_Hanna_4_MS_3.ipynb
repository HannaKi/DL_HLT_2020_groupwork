{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Text_Classification_task_Hyperp_opt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PfqG5963ZhDa",
        "-7j5rqbNzm3O",
        "DTcr2XZ8zg1H",
        "V3HXbVzAF6UH",
        "9vn7FWLFGeJQ",
        "zIycPktw_m39",
        "tPskSfawiO5I",
        "NYSi-7hwqqTs",
        "i-HcrqoxqxiI",
        "Y1bZmgfNnWlm"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/DL_HLT_2020_groupwork/blob/master/Text_Classification_task_Hanna_4_MS_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VGkCcicEVPSF"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "This project is about text classification. You will develop a text classification system that identifies different kinds of online texts, such as news, blogs and opinionated texts. We will refer to these text categories as registers. If you want to learn more about online registers and their automatic identification, you can read, e.g., our paper [Toward Multilingual Identification of Online Registers] (https://www.aclweb.org/anthology/W19-6130/).\n",
        "\n",
        "# Data and register labels\n",
        "The data for this project consist of ~7500 documents with manual annotations on their register. You can download it from http://dl.turkunlp.org/TKO_8965-projects/classification/ . The documents are based on a (almost) random sample of the Finnish Internet. The registers are identified using a relatively detailed, hierarchical taxonomy. The taxonomy consists of 8 main categories that are divided into a large number of subregisters. The taxonomy is described at the end of this page. The table includes also the abbreviations that are used in the data.\n",
        "\n",
        "The challenge with online documents is that it is not always easy to identify the specific registers categories of the documents. Furthermore, another issue is that a document may display characteristics of several registers. For instance, a blog post may simultaneously seem like a product review. To deal with these challenges, we have followed the following guidelines:\n",
        "* For each document, the annotators have aimed at marking the specific subregister category. When this is possible, the document has two register labels: the subregister label and the main register label to which the subregister belongs. For instance, a document annotated as a news article would have the label NE for News and the corresponding higher level register label NA for Narrative. \n",
        "* In some cases, the document does not seem to fit any of the subregisters. In this case, the document can be given only one label: the main register label, such as NA for Narrative. \n",
        "* Some documents may display characteristics of several register categories. In this case, the annotator can mark several register labels for one single document. Consequently, the document may have up to four labels. This would be the case case if a document is annotated both as a Personal blog (subregister label PB + corresponding higher level register label NA) and Review (subregister label RV + corresponding higher level register label OP).\n",
        "\n",
        "\n",
        "# Register classes and abbreviations\n",
        "\n",
        "NA Narrative\n",
        "\n",
        "* NE NA    New reports / news blogs\n",
        "* SR NA    Sports reports\n",
        "* PB NA    Personal blog\n",
        "* HA NA    Historical article\n",
        "* FC NA    Fiction\n",
        "* TB NA    Travel blog\n",
        "* CB NA    Community blogs\n",
        "* OA NA    Online article\n",
        "\n",
        "OP  Opinion\n",
        "* OB OP  Personal opinion blogs\n",
        "* RV OP  Reviews\n",
        "* RS OP  Religious blogs/sermons\n",
        "* AV OP  Advice\n",
        "\n",
        "IN Informational description\n",
        "* JD IN  Job description\n",
        "* FA IN  FAQs\n",
        "* DT IN  Description of a thing\n",
        "* IB IN  Information blogs\n",
        "* DP IN  Description of a person\n",
        "* RA IN  Research articles\n",
        "* LT IN  Legal terms / conditions\n",
        "* CM IN  Course materials\n",
        "* EN IN  Encyclopedia articles\n",
        "* RP IN  Report\n",
        "\n",
        "ID Interactive discussion\n",
        "* DF ID  Discussion forums\n",
        "* QA ID  Question-answer forums\n",
        "\n",
        "HI  How-to/instructions\n",
        "* RE HI  Recipes\n",
        "\n",
        "IP IG  Informational persuasion\n",
        "* DS IG  Description with intent to sell\n",
        "* EB IG  News-opinion blogs / editorials\n",
        "\n",
        "Lyrical LY\n",
        "* PO LY  Poems\n",
        "* SL LY  Songs\n",
        "\n",
        "Spoken SP\n",
        "* IT SP Interviews\n",
        "* FS SP Formal speeches\n",
        "\n",
        "Others OS\n",
        "* MT OS Machine-translated / generated texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1_c9eZnWlG",
        "colab_type": "text"
      },
      "source": [
        "# Preparations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1s5CYypylU1",
        "colab_type": "text"
      },
      "source": [
        "Import packages, set up TF version 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsWUzLj5NOG",
        "colab_type": "code",
        "outputId": "86159980-3fb7-4ec5-8a2b-25db417ae40a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get rid of old tf at some point!\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# to run with old tf with which the code was made\n",
        "# The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.\n",
        "# https://colab.research.google.com/notebooks/tensorflow_version.ipynb\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvqKCrreThHK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ab422dd0-05a6-4ca5-a7ea-0240c2836cb0"
      },
      "source": [
        "print(tensorflow.keras.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kmNg_YSM4rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "aeadd8a9-c335-4afd-a1dd-b3b6ec067ab1"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svG58YMGXsNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "20812e80-6a44-448a-aae4-a86195a9306e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7bPGqEaPC7",
        "colab_type": "text"
      },
      "source": [
        "For saving the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERs_vfbfabkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%script bash\n",
        "mkdir -p models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIHMK6DxXy6r",
        "colab_type": "text"
      },
      "source": [
        "Download and open data, explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHGSBx5nWlM",
        "colab_type": "code",
        "outputId": "4c09af33-801d-4f75-8675-b020e242ed67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "# Download development data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
        "# Download test data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
        "# Download train data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-09 13:01:44--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4035578 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘fincore-dev.tsv.3’\n",
            "\n",
            "\rfincore-dev.tsv.3     0%[                    ]       0  --.-KB/s               \rfincore-dev.tsv.3     1%[                    ]  44.77K   161KB/s               \rfincore-dev.tsv.3     3%[                    ] 124.19K   222KB/s               \rfincore-dev.tsv.3    11%[=>                  ] 439.29K   522KB/s               \rfincore-dev.tsv.3    40%[=======>            ]   1.56M  1.38MB/s               \rfincore-dev.tsv.3   100%[===================>]   3.85M  3.03MB/s    in 1.3s    \n",
            "\n",
            "2020-05-09 13:01:45 (3.03 MB/s) - ‘fincore-dev.tsv.3’ saved [4035578/4035578]\n",
            "\n",
            "--2020-05-09 13:01:46--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8512687 (8.1M) [application/octet-stream]\n",
            "Saving to: ‘fincore-test.tsv.3’\n",
            "\n",
            "fincore-test.tsv.3  100%[===================>]   8.12M  6.58MB/s    in 1.2s    \n",
            "\n",
            "2020-05-09 13:01:48 (6.58 MB/s) - ‘fincore-test.tsv.3’ saved [8512687/8512687]\n",
            "\n",
            "--2020-05-09 13:01:49--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29379580 (28M) [application/octet-stream]\n",
            "Saving to: ‘fincore-train.tsv.3’\n",
            "\n",
            "fincore-train.tsv.3 100%[===================>]  28.02M  11.9MB/s    in 2.4s    \n",
            "\n",
            "2020-05-09 13:01:51 (11.9 MB/s) - ‘fincore-train.tsv.3’ saved [29379580/29379580]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8TezCY6pR1",
        "colab_type": "text"
      },
      "source": [
        "Data split\n",
        "\n",
        "- Training set: used for training the models\n",
        "- Validation (or model assessment) set: when comparing\n",
        "di\u000berent models trained on training set, select one with lowest\n",
        "error on validation set\n",
        "- Test set: test the \fnal hypothesis on test set to get unbiased\n",
        "error estimate for it\n",
        "\n",
        "After error estimation the \fnal model is often trained on\n",
        "combined training, validation and test set, using best\n",
        "hyperparameters found during model selection\n",
        "- Complication: randomized optimization approaches where\n",
        "di\u000berent runs with same hyperparameters can lead to very\n",
        "di\u000berent quality solutions (e.g. neural network)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KciwyumAEz_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "9b1ef2c3-2405-424c-9052-e5bb32d73849"
      },
      "source": [
        "train = pd.read_csv('fincore-train.tsv', sep='\\t', header=None)\n",
        "\n",
        "train = train.sample(frac=1, random_state = 4) # suffle the data\n",
        "train.columns = ['label','text']\n",
        "print(\"train data\")\n",
        "print(train.head())\n",
        "print(train.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(train['label'].value_counts())\n",
        "\n",
        "dev = pd.read_csv('fincore-dev.tsv', sep='\\t', header=None)\n",
        "dev.columns = ['label','text']\n",
        "print(\"dev data\")\n",
        "print(dev.head())\n",
        "print(dev.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(dev['label'].value_counts())\n",
        "\n",
        "test = pd.read_csv('fincore-test.tsv', sep='\\t', header=None)\n",
        "test.columns = ['label','text']\n",
        "print(\"test data\")\n",
        "print(test.head())\n",
        "print(test.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(test['label'].value_counts())\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data\n",
            "       label                                               text\n",
            "3982  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "2640  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...\n",
            "119   NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...\n",
            "4916  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...\n",
            "775   MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...\n",
            "(5295, 2)\n",
            "dev data\n",
            "    label                                               text\n",
            "0  OA NA    Luonnonhoito Maaperän siemenpankkia avattiin ...\n",
            "1  DS IG    • Jokainen ripsi on erittäin kevyt ja muodolt...\n",
            "2  DS IG    Mukavuudet Hotel Dila Vain muutaman metrin pä...\n",
            "3  DF ID    Vastaa viestiin Otsikko Viesti ensin omaishoi...\n",
            "4  OA NA    Dinosaur Jr 30.5.2010 Tavastia , Helsinki 198...\n",
            "(756, 2)\n",
            "test data\n",
            "    label                                               text\n",
            "0    HI     Tehkää nollaleimaus . Jos rekisteröinti onnis...\n",
            "1    NA     1 kommenttia : Syyslomallelähtijät kirjoitti ...\n",
            "2  DT IN    Ammattikoulutuksen perustana on ajatus siitä ...\n",
            "3  DP IN    Ulkonäkö : Silveriä voisi kuvata tietyllä tap...\n",
            "4  DT IN    Laulupelimannien puheenjohtajina ovat toimine...\n",
            "(1513, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwxqapN9J7PH",
        "colab_type": "code",
        "outputId": "90393bdc-0776-4745-be90-a5cc42d44e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Prepare stratified data sets for training, development and testing:\n",
        "# Stratification aims to ensure that all the data sets (train, development and test) have the same distribution of labels. \n",
        "# This minimizes chances that a model has to try to predict labes it has not seen during training.\n",
        "\n",
        "# test-train-split causes error: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
        "# Best solution: More data\n",
        "# Second best solution: If you cannot have another dataset, you will have to play with what you have. Suggestion: remove the sample that has the lonely target. \n",
        "\n",
        "# Join all the data and re-divide it with stratification and to illustrate how skewed the data is!\n",
        "train_e = train\n",
        "train_e['data'] = 'train'\n",
        "\n",
        "dev_e = dev\n",
        "dev_e['data'] = 'dev'\n",
        "\n",
        "test_e = test\n",
        "test_e['data'] = 'test'\n",
        "\n",
        "frames = [train_e, dev_e, test_e]\n",
        "df = pd.concat(frames).reset_index(drop=True) # get rid of the old indexes # this data used for milestone 3\n",
        "\n",
        "# Separating out the target\n",
        "y = df['label'] # pd df\n",
        "# # Separating out the features\n",
        "X = df['text'] # pd df\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "nums = dict(zip(unique, counts))\n",
        "\n",
        "df.head()\n",
        "\n",
        "#pprint(sorted(nums.items(), key = lambda kv:(kv[1], kv[0])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWSDvMthCZFl",
        "colab_type": "code",
        "outputId": "69f5a5ad-08d2-4b6a-a06e-6afc9f89769f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "plt.figure(figsize=(26, 6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.countplot(x=\"label\", hue=\"data\", data=df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6e207ab9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1872x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuGhgHP5DJkZ",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the distribution of labels is highly skewed. This might be fixed with data augmentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzVzQOvux5lJ",
        "colab_type": "text"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "Since we now know that 'MT OS ' is the most common label in the data with 957 occurences, we can set a naive baseline prediction: We will predict that an unlabeled new text belongs to this biggest class. In this case our prediction accuracy is the pure share of the biggest class in the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vU1dkUOyhQH",
        "colab_type": "code",
        "outputId": "53ba4644-eec0-4780-f1b7-fb852f2c5656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification baseline: \", round((957/df.shape[0])*100,1), \"percent\") # here the original data without modifications for training of classificators "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification baseline:  12.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_AQXZLHkXz",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.1: Bag-of-words classifier (multi-class)\n",
        "\n",
        "Train a bag-of-words classifier to predict the register categories. In this milestone, the setting is multi-class, so the register label combinations form the classes, e.g. NA_NE and NA_NE_OP_OB. \n",
        "\n",
        "- Evaluate your model and report your results with different hyperparameters\n",
        "- Ideas to try:\n",
        "  - Different activation functions\n",
        "  - Altering the learning rate\n",
        "  - Use different optimizers\n",
        "  - Adjusting the vocabulary size of the embeddings\n",
        "\n",
        "- Activation functions and optimizers supported by Keras can be found here: https://keras.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FGFhSxaVye",
        "colab_type": "text"
      },
      "source": [
        "Bow classifier is only interested in the multiplicity or appearance of words (or to be precise n-garms). Hence we loose the textual context and order of the words (n-grams). This inevitably leads to some information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLA0Alu_y6c2",
        "colab_type": "text"
      },
      "source": [
        "## Data filtering  for milestones 1 and 2\n",
        "\n",
        "Since we can not train the model to predict labels it has not seen during training phase, we will keep only rows in dev and test data which have the labels that appear also in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j80A7RBn0KOY",
        "colab_type": "code",
        "outputId": "408f92d9-3a6b-4198-9c30-de0d2490bc37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "# Optional way to treat data if stratification is bad idea...\n",
        "\n",
        "# Gather features and labels of the data\n",
        "\n",
        "# Separate text and the associated label\n",
        "train_text = train['text']\n",
        "train_labels = train['label']\n",
        "\n",
        "# print(train_text.head())\n",
        "# print(train_labels.head())\n",
        "# print()\n",
        "\n",
        "dev_text = dev['text']\n",
        "dev_labels = dev['label']\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "all_labels = pd.concat(labels)\n",
        "\n",
        "print(all_labels.head(10))\n",
        "print()\n",
        "print(\"Number of unique labels in data: \", len(all_labels.unique()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3982    DS IG \n",
            "2640    RS OP \n",
            "119     NE NA \n",
            "4916    SR NA \n",
            "775     MT OS \n",
            "5264      NA  \n",
            "5040    DP IN \n",
            "3446    DS IG \n",
            "216     DF ID \n",
            "863     DT IN \n",
            "Name: label, dtype: object\n",
            "\n",
            "Number of unique labels in data:  119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8zX0o3bxaA8",
        "colab_type": "code",
        "outputId": "b024ebfb-fb36-437e-8718-dc96e7816cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_labels)))\n",
        "print(\"-development data: \", len(np.unique(dev_labels)))\n",
        "print(\"-test data: \", len(np.unique(test_labels)))\n",
        "print()\n",
        "inter = np.intersect1d(train_labels, dev_labels)\n",
        "inter2 = np.intersect1d(train_labels, test_labels)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  58\n",
            "-test data:  70\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RtQipNUc-7S",
        "colab_type": "code",
        "outputId": "200cb2b6-abdf-44fa-b867-485ed8224e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Filter out labels not found in training data\n",
        "\n",
        "test_ = test[test['label'].isin(train_labels.tolist())]\n",
        "dev_ = dev[dev['label'].isin(train_labels.tolist())] \n",
        "\n",
        "print(test_.shape)\n",
        "print(dev_.shape)\n",
        "\n",
        "dev_text = dev_['text'] # development data for milestones 1 and 2\n",
        "dev_labels = dev_['label']\n",
        "\n",
        "test_text = test_['text'] # test data for milestones 1 and 2\n",
        "test_labels = test_['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "labels = pd.concat(labels)\n",
        "\n",
        "print(\"Number of unique labels in data: \", len(labels.unique()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 3)\n",
            "(750, 3)\n",
            "Number of unique labels in data:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7VZBC6QCWVi",
        "colab_type": "code",
        "outputId": "4e8677d2-fea4-478e-c77f-0fe04a77043c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_labels)))\n",
        "print(\"-development data: \", len(np.unique(dev_labels)))\n",
        "print(\"-test data: \", len(np.unique(test_labels)))\n",
        "print()\n",
        "inter = np.intersect1d(train_labels, dev_labels)\n",
        "inter2 = np.intersect1d(train_labels, test_labels)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  52\n",
            "-test data:  57\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ3qKWoKbeEU",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation, step  1: form feature matrix\n",
        "\n",
        "We will use CountVectorizer / TfidfVectorizer from sklearn package to transform out text data to numerical format with which our classifier is able to deal with. CountVectorizer converts the collection of text documents (our training data) to a matrix of token counts. Since we are only interested in whether a particular word of the vocabulary is in a single document or not, our vectorizer is set on \"binary\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt1fNJ7ovGge",
        "colab_type": "code",
        "outputId": "03cd8a9a-96f6-4aac-d53f-91c6444a19f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 20000, ngram_range = (1,1)) # Grid search fails: OOM when allocating tensor of shape [85000,200]\n",
        "\n",
        "# form feature matrix\n",
        "train_feature_matrix = vectorizer.fit_transform(train_text)\n",
        "dev_feature_matrix = vectorizer.transform(dev_text)\n",
        "test_feature_matrix = vectorizer.transform(test_text)\n",
        "\n",
        "print(\"shape of the training data: \", train_feature_matrix.shape)\n",
        "print(\"shape of the development data: \", dev_feature_matrix.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5295, 20000)\n",
            "shape of the development data:  (750, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymmdQP4aq2E",
        "colab_type": "text"
      },
      "source": [
        "The shape of the feature matrix tells us that we have XXX items (documents) in our training data. The number of unique n-grams exceeds XXXX but we are including only the first XXXX most common of them. Since our CountVectorizer has parameter setting \"ngram_range = 1, 1\" this means we are forming the vector with unigrams, separate words or charachters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTY0uwjd25w",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation, step 2: Label encoding and one hot encoding\n",
        "\n",
        "Next we will encode the labels. This means transforming the textual labels to numeric values, which our model is able to deal with. This step is made with LabelEncoder class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_yBCz62umP",
        "colab_type": "code",
        "outputId": "b8cc60d2-a574-414f-9c0a-d52881612a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "# label encoding \n",
        "\n",
        "label_encoder = LabelEncoder() # Create the instance of LabelEncoder we use to turn class labels into integers\n",
        "\n",
        "train_numbers = label_encoder.fit_transform(train_labels) # encode labels to integers\n",
        "dev_numbers = label_encoder.transform(dev_labels) \n",
        "test_numbers = label_encoder.transform(test_labels) \n",
        "\n",
        "print(\"Inverse transform gives unique labels in each data set: \", label_encoder.inverse_transform(train_numbers))\n",
        "print(\"Sanity checks, do we have as many labels and texts in our data sets?\")\n",
        "print(\"Train data: \", len(train_numbers), len(train_text))\n",
        "print(\"Dev data: \", len(dev_numbers), len(dev_text))\n",
        "print(\"Test data: \", len(test_numbers), len(test_text))\n",
        "\n",
        "# one hot encoding\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_hot=one_hot_encoder.fit_transform(train_numbers.reshape(-1,1))\n",
        "dev_hot=one_hot_encoder.transform(dev_numbers.reshape(-1,1))\n",
        "test_hot=one_hot_encoder.transform(test_numbers.reshape(-1,1))\n",
        "print()\n",
        "print(train_hot.shape)\n",
        "print(dev_hot.shape)\n",
        "print(test_hot.shape)\n",
        "train_hot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inverse transform gives unique labels in each data set:  ['DS IG ' 'RS OP ' 'NE NA ' ... 'DS IG ' 'MT OS ' 'MT OS ']\n",
            "Sanity checks, do we have as many labels and texts in our data sets?\n",
            "Train data:  5295 5295\n",
            "Dev data:  750 750\n",
            "Test data:  1500 1500\n",
            "\n",
            "(5295, 100)\n",
            "(750, 100)\n",
            "(1500, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3RDtU7J2di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LabelEncoderin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode tee koodaus\n",
        "# print(\"Koodaukset: \", list(le.classes_))\n",
        "# print()\n",
        "# le.transform([\"tokyo\", \"tokyo\", \"paris\"]) # käytä koodausta arvojen transformointiin --> numeeriset arvot\n",
        "# num = le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
        "# print(\"Sovitetulla encoderilla tuotetut numeeriset arvot muuttujalistasta: \", num)\n",
        "# print()\n",
        "# print(\"Inverse transform: \", list(le.inverse_transform([2, 2, 1])))\n",
        "# print()\n",
        "# test = le.fit_transform([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit kolmella\", test)\n",
        "\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"helsinki\"])\n",
        "# #print(list(le.classes_))\n",
        "\n",
        "# test = le.fit_transform([\"paris\", \"helsinki\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit neljällä\", test)\n",
        "\n",
        "# print(list(le.inverse_transform([2, 2, 1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9_7r6FeRS5",
        "colab_type": "text"
      },
      "source": [
        "Now the data is prepared for milestone 1.\n",
        "\n",
        "## Cross validation and hyperparameter optimization with Gridsearch\n",
        "\n",
        "Join train and dev data for CV grid search.\n",
        "\n",
        "Approach: Select prominent optimizers and optimize rest of the hyperparams for it. Since other hyperparams affect the optimizer performance it is not a \"fair competition\" to optimize optimizers with all the rest hyperparams set to some values. We have to deal with memory constraints also. This significantly limits the set of hyperparametrs we can optimize at once.\n",
        "\n",
        "Here Adam and SGD are explored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAoJ6ZArns0j",
        "colab_type": "code",
        "outputId": "41058cb0-fcc8-48c0-ec40-966da64018cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "# features \n",
        "frames = [train, dev_]\n",
        "joint = pd.concat(frames)\n",
        "joint_feature_matrix = vectorizer.fit_transform(joint['text'])\n",
        "\n",
        "# labels, one hot encoded\n",
        "print(type(train_hot))\n",
        "print(train_hot.shape)\n",
        "print(dev_hot.shape)\n",
        "joint_labels= np.concatenate((train_hot, dev_hot))\n",
        "print(joint_labels.shape)\n",
        "\n",
        "print(joint_feature_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(5295, 100)\n",
            "(750, 100)\n",
            "(6045, 100)\n",
            "(6045, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWUSL5YoSARM",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer: SGD\n",
        "\n",
        "### Step 1.1 Build and compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cdWVH9iQpVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_count, feature_count = joint_feature_matrix.shape\n",
        "class_count = len(labels.unique())\n",
        "\n",
        "def create_bow_model_SGD(learning_rate=0.01, nodes=200):\n",
        "  inp = Input(shape=(feature_count,))\n",
        "  hidden1 = Dense(nodes, activation=\"relu\")(inp)\n",
        " # hidden2 = Dense(nodes, activation=\"relu\")(hidden1) # multiple hidden layers possible but slow down hyperparameter optimization\n",
        "  dropout = Dropout(0.3)(hidden1)\n",
        "  outp = Dense(class_count, activation='softmax')(dropout)\n",
        "  model = Model(inputs=[inp], outputs=[outp])\n",
        "  optimizer = optimizers.SGD(learning_rate=learning_rate) # also hyperparameter momentum could be optimized\n",
        "  model.compile(optimizer = optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkzqF6nkT98F",
        "colab_type": "code",
        "outputId": "0dc59dca-742d-4d3c-e07b-d4ff3d5ccff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "model = create_bow_model_SGD()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               4000200   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "=================================================================\n",
            "Total params: 4,020,300\n",
            "Trainable params: 4,020,300\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjQl3cVXXota",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.2: GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNXj8TvOQN6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tensorflow.compat.v1.disable_eager_execution()\n",
        "stop_cb = EarlyStopping(monitor = 'accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "# If the error only occurs when you use smaller datasets, you're very likely using datasets small enough to not have a single sample in the validation set. \n",
        "\n",
        "bow_model_SGD = KerasClassifier(build_fn=create_bow_model_SGD, epochs=40, batch_size=32, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvHQGoNB0q9e",
        "colab_type": "code",
        "outputId": "88593b52-8012-4ed5-b62f-f0a8f8468011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = datetime.now()\n",
        "print(\"start time =\", start)\n",
        "\n",
        "# set the hyperparameter: example: np.logspace(-10, 1, num = 7, base = 2)\n",
        "learning_rate = np.logspace(-7, 0, num = 5, base = 2).tolist()\n",
        "params = dict(learning_rate = learning_rate)\n",
        "\n",
        "grid = GridSearchCV(estimator = bow_model_SGD, param_grid = params, n_jobs = 1, cv = 3 , verbose  = 1)  # cv: For integer/None inputs, if the estimator is a classifier and y is \n",
        "                                                                                                        # either binary or multiclass, StratifiedKFold is used.\n",
        "X = joint_feature_matrix.toarray()\n",
        "    \n",
        "grid_result_SGD = grid.fit(X, joint_numbers, callbacks=[stop_cb, mc_cb])\n",
        "\n",
        "end = datetime.now()\n",
        "print(\"end time =\", end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start time = 2020-05-01 10:54:05.028913\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 273us/step - loss: 4.5955 - accuracy: 0.0787\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.5714 - accuracy: 0.1328\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.5463 - accuracy: 0.1340\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 4.5197 - accuracy: 0.1380\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4928 - accuracy: 0.1385\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4656 - accuracy: 0.1355\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.4367 - accuracy: 0.1414\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4078 - accuracy: 0.1422\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.3778 - accuracy: 0.1370\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.3469 - accuracy: 0.1372\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.3147 - accuracy: 0.1375\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.2808 - accuracy: 0.1380\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 4.2448 - accuracy: 0.1367\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.2087 - accuracy: 0.1350\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.1698 - accuracy: 0.1370\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1279 - accuracy: 0.1367\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.0828 - accuracy: 0.1355\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.0411 - accuracy: 0.1370\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.9944 - accuracy: 0.1370\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.9491 - accuracy: 0.1340\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8981 - accuracy: 0.1370\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.8489 - accuracy: 0.1370\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8041 - accuracy: 0.1385\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7584 - accuracy: 0.1330\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7140 - accuracy: 0.1347\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.6775 - accuracy: 0.1328\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 3.6398 - accuracy: 0.1333\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.6073 - accuracy: 0.1290\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00028: early stopping\n",
            "2015/2015 [==============================] - 1s 362us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 280us/step - loss: 4.5940 - accuracy: 0.0794\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5681 - accuracy: 0.1295\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5404 - accuracy: 0.1313\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5117 - accuracy: 0.1278\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4821 - accuracy: 0.1303\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4521 - accuracy: 0.1305\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4210 - accuracy: 0.1238\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.3893 - accuracy: 0.1256\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 4.3556 - accuracy: 0.1223\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.3208 - accuracy: 0.1261\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.2853 - accuracy: 0.1253\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 4.2472 - accuracy: 0.1223\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.2067 - accuracy: 0.1231\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1631 - accuracy: 0.1236\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1222 - accuracy: 0.1223\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.0756 - accuracy: 0.1213\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.0311 - accuracy: 0.1231\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.9801 - accuracy: 0.1191\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.9317 - accuracy: 0.1231\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8820 - accuracy: 0.1213\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8359 - accuracy: 0.1206\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7866 - accuracy: 0.1233\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.7439 - accuracy: 0.1223\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00023: early stopping\n",
            "2015/2015 [==============================] - 1s 370us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 276us/step - loss: 4.5909 - accuracy: 0.1122\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.5637 - accuracy: 0.1330\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.5335 - accuracy: 0.1318\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.5024 - accuracy: 0.1325\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.4696 - accuracy: 0.1323\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.4358 - accuracy: 0.1323\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.4015 - accuracy: 0.1325\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.3664 - accuracy: 0.1323\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 148us/step - loss: 4.3290 - accuracy: 0.1323\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.2906 - accuracy: 0.1323\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 4.2499 - accuracy: 0.1323\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.2083 - accuracy: 0.1323\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 4.1619 - accuracy: 0.1323\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.1156 - accuracy: 0.1323\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.0661 - accuracy: 0.1323\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.0153 - accuracy: 0.1323\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.9642 - accuracy: 0.1325\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.9124 - accuracy: 0.1323\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.8603 - accuracy: 0.1323\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.8083 - accuracy: 0.1323\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7589 - accuracy: 0.1323\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7127 - accuracy: 0.1320\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00022: early stopping\n",
            "2015/2015 [==============================] - 1s 347us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 273us/step - loss: 4.5639 - accuracy: 0.1226\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.4678 - accuracy: 0.1288\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.3597 - accuracy: 0.1283\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.2368 - accuracy: 0.1283\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.0924 - accuracy: 0.1283\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.9304 - accuracy: 0.1283\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7719 - accuracy: 0.1283\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.6367 - accuracy: 0.1283\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.5401 - accuracy: 0.1303\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.4633 - accuracy: 0.1315\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.4147 - accuracy: 0.1419\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3753 - accuracy: 0.1414\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.3407 - accuracy: 0.1449\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3157 - accuracy: 0.1474\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2879 - accuracy: 0.1603\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.2673 - accuracy: 0.1533\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2559 - accuracy: 0.1593\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.2456 - accuracy: 0.1573\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.2311 - accuracy: 0.1633\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.2205 - accuracy: 0.1586\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2107 - accuracy: 0.1618\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1988 - accuracy: 0.1707\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1939 - accuracy: 0.1650\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1850 - accuracy: 0.1677\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1841 - accuracy: 0.1819\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1686 - accuracy: 0.1839\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1664 - accuracy: 0.1789\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1558 - accuracy: 0.1898\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1573 - accuracy: 0.1816\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1473 - accuracy: 0.1792\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.1459 - accuracy: 0.1931\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1362 - accuracy: 0.1903\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.1370 - accuracy: 0.1903\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1245 - accuracy: 0.2030\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1287 - accuracy: 0.2040\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.1166 - accuracy: 0.1965\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1153 - accuracy: 0.2060\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1076 - accuracy: 0.2176\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1028 - accuracy: 0.2082\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.0989 - accuracy: 0.2129\n",
            "2015/2015 [==============================] - 1s 360us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 308us/step - loss: 4.5604 - accuracy: 0.1084\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.4510 - accuracy: 0.1263\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.3284 - accuracy: 0.1194\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 4.1866 - accuracy: 0.1221\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.0256 - accuracy: 0.1226\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.8519 - accuracy: 0.1201\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.6923 - accuracy: 0.1213\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.5733 - accuracy: 0.1258\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.4928 - accuracy: 0.1263\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.4376 - accuracy: 0.1335\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.3943 - accuracy: 0.1337\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.3574 - accuracy: 0.1315\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3316 - accuracy: 0.1370\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3045 - accuracy: 0.1412\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2892 - accuracy: 0.1409\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2701 - accuracy: 0.1352\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2526 - accuracy: 0.1424\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2475 - accuracy: 0.1382\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2353 - accuracy: 0.1504\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2234 - accuracy: 0.1496\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.2227 - accuracy: 0.1514\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.2132 - accuracy: 0.1471\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2006 - accuracy: 0.1558\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1969 - accuracy: 0.1566\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1891 - accuracy: 0.1603\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1820 - accuracy: 0.1553\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.1776 - accuracy: 0.1588\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1722 - accuracy: 0.1670\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1666 - accuracy: 0.1715\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1563 - accuracy: 0.1734\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1546 - accuracy: 0.1710\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1554 - accuracy: 0.1653\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1483 - accuracy: 0.1772\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1442 - accuracy: 0.1787\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1409 - accuracy: 0.1844\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1399 - accuracy: 0.1752\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.1322 - accuracy: 0.1901\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1252 - accuracy: 0.1950\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1229 - accuracy: 0.1886\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.1191 - accuracy: 0.1841\n",
            "2015/2015 [==============================] - 1s 368us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 292us/step - loss: 4.5551 - accuracy: 0.1313\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 4.4396 - accuracy: 0.1476\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 4.3093 - accuracy: 0.1496\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 4.1581 - accuracy: 0.1489\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.9852 - accuracy: 0.1504\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.8041 - accuracy: 0.1367\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.6473 - accuracy: 0.1437\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.5358 - accuracy: 0.1432\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.4634 - accuracy: 0.1432\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.4072 - accuracy: 0.1499\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.3683 - accuracy: 0.1427\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.3325 - accuracy: 0.1459\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.3051 - accuracy: 0.1548\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2868 - accuracy: 0.1533\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2653 - accuracy: 0.1563\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2471 - accuracy: 0.1568\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.2373 - accuracy: 0.1563\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.2265 - accuracy: 0.1536\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.2120 - accuracy: 0.1603\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2049 - accuracy: 0.1620\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1996 - accuracy: 0.1667\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1886 - accuracy: 0.1685\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1835 - accuracy: 0.1727\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1742 - accuracy: 0.1737\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1747 - accuracy: 0.1754\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1642 - accuracy: 0.1754\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1561 - accuracy: 0.1829\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1492 - accuracy: 0.1794\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1410 - accuracy: 0.1908\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.1381 - accuracy: 0.1906\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1369 - accuracy: 0.1888\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1336 - accuracy: 0.1826\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1253 - accuracy: 0.1998\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1213 - accuracy: 0.1913\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1108 - accuracy: 0.2062\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1145 - accuracy: 0.2079\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1050 - accuracy: 0.2060\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0991 - accuracy: 0.2082\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0982 - accuracy: 0.2117\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0924 - accuracy: 0.2072\n",
            "2015/2015 [==============================] - 1s 367us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 300us/step - loss: 4.4479 - accuracy: 0.1251\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 4.0008 - accuracy: 0.1280\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.5616 - accuracy: 0.1290\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.3820 - accuracy: 0.1407\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2938 - accuracy: 0.1538\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2451 - accuracy: 0.1603\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2065 - accuracy: 0.1695\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 3.1815 - accuracy: 0.1697\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1643 - accuracy: 0.1801\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.1492 - accuracy: 0.1968\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.1296 - accuracy: 0.2020\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1086 - accuracy: 0.2154\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.0908 - accuracy: 0.2233\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.0811 - accuracy: 0.2342\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0627 - accuracy: 0.2434\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0432 - accuracy: 0.2462\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0202 - accuracy: 0.2581\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.9936 - accuracy: 0.2749\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.9722 - accuracy: 0.2759\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9437 - accuracy: 0.2859\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9298 - accuracy: 0.2898\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.8986 - accuracy: 0.2975\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.8764 - accuracy: 0.3035\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.8438 - accuracy: 0.3151\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.8265 - accuracy: 0.3176\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.7936 - accuracy: 0.3305\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.7654 - accuracy: 0.3370\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.7404 - accuracy: 0.3395\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.7157 - accuracy: 0.3536\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.6954 - accuracy: 0.3596\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.6755 - accuracy: 0.3556\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.6498 - accuracy: 0.3672\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.6343 - accuracy: 0.3720\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.6053 - accuracy: 0.3846\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5855 - accuracy: 0.3826\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5657 - accuracy: 0.3896\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.5489 - accuracy: 0.3960\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5315 - accuracy: 0.4000\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.5057 - accuracy: 0.4015\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 2.4955 - accuracy: 0.4030\n",
            "2015/2015 [==============================] - 1s 377us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 292us/step - loss: 4.4613 - accuracy: 0.1124\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.0571 - accuracy: 0.1206\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.5967 - accuracy: 0.1275\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.3862 - accuracy: 0.1414\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2978 - accuracy: 0.1397\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2459 - accuracy: 0.1449\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2165 - accuracy: 0.1556\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1922 - accuracy: 0.1692\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1709 - accuracy: 0.1777\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1599 - accuracy: 0.1794\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1414 - accuracy: 0.1864\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1267 - accuracy: 0.1993\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1054 - accuracy: 0.2042\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.0947 - accuracy: 0.2144\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0702 - accuracy: 0.2288\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.0579 - accuracy: 0.2261\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0347 - accuracy: 0.2409\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0195 - accuracy: 0.2548\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9923 - accuracy: 0.2655\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9740 - accuracy: 0.2640\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.9487 - accuracy: 0.2744\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9253 - accuracy: 0.2811\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9001 - accuracy: 0.2960\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.8733 - accuracy: 0.2953\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.8416 - accuracy: 0.3124\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8174 - accuracy: 0.3228\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.7881 - accuracy: 0.3385\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7618 - accuracy: 0.3352\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7404 - accuracy: 0.3407\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.7135 - accuracy: 0.3509\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6874 - accuracy: 0.3573\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.6646 - accuracy: 0.3638\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.6444 - accuracy: 0.3667\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6199 - accuracy: 0.3774\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6014 - accuracy: 0.3787\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5787 - accuracy: 0.3881\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.5562 - accuracy: 0.3933\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.5377 - accuracy: 0.3931\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.5271 - accuracy: 0.3916\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5014 - accuracy: 0.4007\n",
            "2015/2015 [==============================] - 1s 380us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 293us/step - loss: 4.4408 - accuracy: 0.1283\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.9804 - accuracy: 0.1323\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.5475 - accuracy: 0.1350\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.3675 - accuracy: 0.1380\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.2821 - accuracy: 0.1442\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2304 - accuracy: 0.1531\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2043 - accuracy: 0.1650\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.1718 - accuracy: 0.1655\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1617 - accuracy: 0.1749\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1428 - accuracy: 0.1769\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1266 - accuracy: 0.1958\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1162 - accuracy: 0.1861\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.0931 - accuracy: 0.2089\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.0755 - accuracy: 0.2179\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.0602 - accuracy: 0.2283\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0457 - accuracy: 0.2459\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0189 - accuracy: 0.2514\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0054 - accuracy: 0.2605\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9789 - accuracy: 0.2749\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9561 - accuracy: 0.2849\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9364 - accuracy: 0.2868\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9032 - accuracy: 0.3022\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8899 - accuracy: 0.3032\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8568 - accuracy: 0.3141\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8279 - accuracy: 0.3266\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.7980 - accuracy: 0.3310\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.7746 - accuracy: 0.3362\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.7536 - accuracy: 0.3489\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7240 - accuracy: 0.3531\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6980 - accuracy: 0.3615\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6778 - accuracy: 0.3690\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6527 - accuracy: 0.3715\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6297 - accuracy: 0.3801\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6043 - accuracy: 0.3878\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5872 - accuracy: 0.3854\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5666 - accuracy: 0.4010\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5426 - accuracy: 0.4035\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.5224 - accuracy: 0.4037\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.4969 - accuracy: 0.4112\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 2.4872 - accuracy: 0.4164\n",
            "2015/2015 [==============================] - 1s 379us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 305us/step - loss: 3.9474 - accuracy: 0.1238\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2762 - accuracy: 0.1452\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.1772 - accuracy: 0.1638\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1176 - accuracy: 0.1963\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.0618 - accuracy: 0.2266\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9878 - accuracy: 0.2588\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.9086 - accuracy: 0.2826\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.8199 - accuracy: 0.3055\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.7373 - accuracy: 0.3395\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.6550 - accuracy: 0.3571\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.5899 - accuracy: 0.3797\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.5296 - accuracy: 0.3851\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.4699 - accuracy: 0.4047\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.4257 - accuracy: 0.4099\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.3732 - accuracy: 0.4300\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.3298 - accuracy: 0.4372\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.2773 - accuracy: 0.4434\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.2452 - accuracy: 0.4509\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 2.2084 - accuracy: 0.4581\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.1798 - accuracy: 0.4630\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.1373 - accuracy: 0.4759\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.1014 - accuracy: 0.4811\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.0704 - accuracy: 0.4911\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.0372 - accuracy: 0.4968\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 2.0119 - accuracy: 0.5119\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 1.9791 - accuracy: 0.5139\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.9604 - accuracy: 0.5169\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.9210 - accuracy: 0.5320\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.8881 - accuracy: 0.5439\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.8580 - accuracy: 0.5546\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 1.8280 - accuracy: 0.5643\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 1.7989 - accuracy: 0.5685\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.7690 - accuracy: 0.5854\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.7335 - accuracy: 0.5928\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.7047 - accuracy: 0.6022\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.6761 - accuracy: 0.6082\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.6499 - accuracy: 0.6169\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.6250 - accuracy: 0.6241\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.5933 - accuracy: 0.6365\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 1.5701 - accuracy: 0.6417\n",
            "2015/2015 [==============================] - 1s 395us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 296us/step - loss: 3.9822 - accuracy: 0.1159\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.2829 - accuracy: 0.1370\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.1852 - accuracy: 0.1548\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1309 - accuracy: 0.1881\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0800 - accuracy: 0.2040\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0153 - accuracy: 0.2439\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.9382 - accuracy: 0.2737\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8585 - accuracy: 0.2995\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.7669 - accuracy: 0.3293\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6840 - accuracy: 0.3610\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.6083 - accuracy: 0.3702\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.5403 - accuracy: 0.3873\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.4867 - accuracy: 0.4007\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.4288 - accuracy: 0.4141\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 2.3861 - accuracy: 0.4258\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.3329 - accuracy: 0.4325\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2913 - accuracy: 0.4447\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2506 - accuracy: 0.4533\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2092 - accuracy: 0.4667\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.1777 - accuracy: 0.4692\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.1412 - accuracy: 0.4764\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.1031 - accuracy: 0.4913\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.0672 - accuracy: 0.4926\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.0394 - accuracy: 0.5042\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.0045 - accuracy: 0.5119\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.9659 - accuracy: 0.5223\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.9356 - accuracy: 0.5357\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 1.9099 - accuracy: 0.5439\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.8734 - accuracy: 0.5553\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.8371 - accuracy: 0.5717\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.8073 - accuracy: 0.5777\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.7806 - accuracy: 0.5878\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.7415 - accuracy: 0.5935\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.7223 - accuracy: 0.6089\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.6936 - accuracy: 0.6156\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 1.6646 - accuracy: 0.6288\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.6329 - accuracy: 0.6310\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5962 - accuracy: 0.6467\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5811 - accuracy: 0.6469\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5584 - accuracy: 0.6548\n",
            "2015/2015 [==============================] - 1s 384us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 296us/step - loss: 3.9500 - accuracy: 0.1333\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.2708 - accuracy: 0.1439\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1771 - accuracy: 0.1707\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 3.1172 - accuracy: 0.1918\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0631 - accuracy: 0.2248\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0014 - accuracy: 0.2459\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9247 - accuracy: 0.2809\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8372 - accuracy: 0.3057\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.7553 - accuracy: 0.3345\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6773 - accuracy: 0.3553\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5942 - accuracy: 0.3777\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5318 - accuracy: 0.3918\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.4712 - accuracy: 0.4032\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.4164 - accuracy: 0.4208\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.3725 - accuracy: 0.4283\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.3298 - accuracy: 0.4404\n",
            "Epoch 17/40\n",
            "3300/4030 [=======================>......] - ETA: 0s - loss: 2.3048 - accuracy: 0.4473"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f129a95d4879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_feature_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_result_SGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_numbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 184\u001b[0;31m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0m\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLoC5f_WW42",
        "colab_type": "code",
        "outputId": "3cbdf930-6683-47d7-8747-15854e5419f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result_SGD.best_score_, grid_result_SGD.best_params_))\n",
        "\n",
        "means = grid_result_SGD.cv_results_['mean_test_score']\n",
        "stds = grid_result_SGD.cv_results_['std_test_score']\n",
        "params = grid_result_SGD.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.553846 using {'learning_rate': 1.0}\n",
            "0.130356 (0.007660) with: {'learning_rate': 0.0078125}\n",
            "0.234243 (0.024834) with: {'learning_rate': 0.026278012976678578}\n",
            "0.406286 (0.022258) with: {'learning_rate': 0.08838834764831845}\n",
            "0.526882 (0.004518) with: {'learning_rate': 0.29730177875068026}\n",
            "0.553846 (0.003991) with: {'learning_rate': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XK-bINWst5",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.3. Predict \n",
        "\n",
        "Now we can predict directly with optimized hyperparameters by calling \"grid_result.predict\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrHjQcjWrKU",
        "colab_type": "code",
        "outputId": "1eb8799a-2a59-4aa7-eb86-85e674ddf0de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "print(test_feature_matrix.shape)\n",
        "\n",
        "predictions_SGD = grid_result_SGD.predict(test_feature_matrix)\n",
        "predicted_labels_SGD = label_encoder.inverse_transform(list(predictions_SGD))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_SGD)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 20000)\n",
            "1500/1500 [==============================] - 0s 202us/step\n",
            "Classification accuracy:  6.4 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngH0B3sRYcES",
        "colab_type": "text"
      },
      "source": [
        "### Nested CV with SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCmyydMlYf81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# How to choose number of splits: https://machinelearningmastery.com/k-fold-cross-validation/\n",
        "\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "# Inner CV: optimize number hyperparameter\n",
        "# Set up possible values of parameters to optimize over\n",
        "learning_rate = np.logspace(-7, 0, num = 5, base = 2).tolist()\n",
        "params = dict(learning_rate = learning_rate)\n",
        "\n",
        "nested_grid_SGD = GridSearchCV(estimator=bow_model_SGD, param_grid=params, n_jobs=1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "nested_grid_result_SGD = nested_grid_SGD.fit(X, joint_numbers, callbacks = [stop_cb])\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGlQVhbxbDML",
        "colab_type": "code",
        "outputId": "2a25ee71-a620-47a0-ca5b-5fe6cb435b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (nested_grid_result_SGD.best_score_, nested_grid_result_SGD.best_params_))\n",
        "\n",
        "means = nested_grid_result_SGD.cv_results_['mean_test_score']\n",
        "stds = nested_grid_result_SGD.cv_results_['std_test_score']\n",
        "params = nested_grid_result_SGD.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.557982 using {'learning_rate': 1.0}\n",
            "0.119438 (0.005350) with: {'learning_rate': 0.0078125}\n",
            "0.236559 (0.048157) with: {'learning_rate': 0.026278012976678578}\n",
            "0.409429 (0.003865) with: {'learning_rate': 0.08838834764831845}\n",
            "0.528371 (0.003851) with: {'learning_rate': 0.29730177875068026}\n",
            "0.557982 (0.005905) with: {'learning_rate': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkJ7wPURapWO",
        "colab_type": "code",
        "outputId": "15f828ff-3a42-433b-996f-38d444c7bdbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predictions_nested_SGD = nested_grid_result_SGD.predict(test_feature_matrix)\n",
        "predicted_labels_nested_SGD = label_encoder.inverse_transform(list(predictions_nested_SGD))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_nested_SGD)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 1s 489us/step\n",
            "Classification accuracy:  7.4 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5JoCfs3LiU4",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tyq5l2bLT3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = joint_feature_matrix.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY2kbDqScdLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_count, feature_count = joint_feature_matrix.shape\n",
        "class_count = len(train_labels.unique())\n",
        "\n",
        "def create_bow_model_Adam(learning_rate=0.01, nodes=200):\n",
        "  inp = Input(shape=(feature_count,))\n",
        "  hidden1 = Dense(nodes, activation=\"relu\")(inp)\n",
        "  hidden2 = Dense(nodes, activation=\"relu\")(hidden1)                                          # two hidden layers\n",
        "  dropout = Dropout(0.3)(hidden2)\n",
        "  outp = Dense(class_count, activation='softmax')(dropout)                                    # also dropoutrate could be optimized\n",
        "  model = Model(inputs=[inp], outputs=[outp])\n",
        "  optimizer = optimizers.Adam(learning_rate=learning_rate)                                    # also hyperparameter momentum could be optimized\n",
        "  model.compile(optimizer = optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy']) # for one hot encodings\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz9l0NRWkRkY",
        "colab_type": "code",
        "outputId": "b3b03a2f-a6f1-4c3d-cf2d-8aa0f420220b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "model = create_bow_model_Adam()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 200)               4000200   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               20100     \n",
            "=================================================================\n",
            "Total params: 4,060,500\n",
            "Trainable params: 4,060,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqtIMrYQftS2",
        "colab_type": "text"
      },
      "source": [
        "## Nested cross-validation\n",
        "- nested, aka external cross-validation\n",
        "- cross-validation within cross-validation\n",
        "- inner loop for model selection, outer for evaluation\n",
        "- Split the data to K non-overlapping folds\n",
        "1. designate i:th fold as the outer cross-validation test fold, set it\n",
        "aside\n",
        "2. use the remaining K-1 folds to perform model selection with\n",
        "cross-validation as usual\n",
        "3. train the model with selected parameters on the K-1 training\n",
        "folds\n",
        "4. compute predictions using the remaining ith fold as test set\n",
        "I afterwards collect the together predictions made at step 4 for\n",
        "all the K folds and compute your error rate or other\n",
        "performance measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmKrE2zoblrD",
        "colab_type": "code",
        "outputId": "7741249c-5d80-4a48-c0b0-76c6f31347ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
        "# https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/\n",
        "# https://mlfromscratch.com/nested-cross-validation-python-code/#/\n",
        "\n",
        "# Choose cross-validation techniques for the inner and outer loops\n",
        "# outer loop: X folds\n",
        "# inner loop: each fold of outer loop is folded again! To avoid too heavy computaion do not fold too many times or try running parallel \n",
        "# (n_jobs=-1): parallel, (n_jobs=1): sequential\n",
        "\n",
        "start = datetime.now()\n",
        "print(\"start time =\", start)\n",
        "\n",
        "bow_model_Adam = KerasClassifier(build_fn=create_bow_model_Adam, epochs=40, batch_size=100, verbose=0)\n",
        "\n",
        "# Call backs won't work / not needed with gridsearch... \n",
        "# stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "# Warning: 'val_accuracy' not available: Stack overflow search suggestion: \"If the error only occurs when you use smaller datasets, \n",
        "# you're very likely using datasets small enough to not have a single sample in the validation set. \"\n",
        "# mc_cb = ModelCheckpoint(filepath='models/bow_model.h5', monitor='val_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "# Warning: memory leak\n",
        "\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "learning_rate = [0.005, 0.001, 0.01, 0.1]\n",
        "epochs = [20, 40]\n",
        "params = dict(learning_rate=learning_rate, epochs = epochs )\n",
        "\n",
        "nested_grid_Adam = GridSearchCV(estimator=bow_model_Adam, param_grid=params, n_jobs=-1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "nested_grid_result_Adam = nested_grid_Adam.fit(X, joint_labels, verbose = 1, callbacks=[mc_cb])\n",
        "\n",
        "end = datetime.now()\n",
        "print(\"start time =\", end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start time = 2020-05-05 16:06:13.958064\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6045 samples\n",
            "Epoch 1/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 3.5174 - acc: 0.2223WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 2s 295us/sample - loss: 3.4626 - acc: 0.2328\n",
            "Epoch 2/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 2.1627 - acc: 0.4897WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 2.1551 - acc: 0.4907\n",
            "Epoch 3/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 1.5263 - acc: 0.6478WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 128us/sample - loss: 1.5263 - acc: 0.6467\n",
            "Epoch 4/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 1.0318 - acc: 0.7658WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 128us/sample - loss: 1.0313 - acc: 0.7658\n",
            "Epoch 5/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.6426 - acc: 0.8642WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.6438 - acc: 0.8653\n",
            "Epoch 6/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.3970 - acc: 0.9312WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 0.3959 - acc: 0.9327\n",
            "Epoch 7/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9640WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.2492 - acc: 0.9643\n",
            "Epoch 8/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.1708 - acc: 0.9739WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 131us/sample - loss: 0.1689 - acc: 0.9745\n",
            "Epoch 9/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 0.1264 - acc: 0.9800WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 0.1270 - acc: 0.9800\n",
            "Epoch 10/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9846WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0982 - acc: 0.9846\n",
            "Epoch 11/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9865WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0795 - acc: 0.9858\n",
            "Epoch 12/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0654 - acc: 0.9889WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 124us/sample - loss: 0.0649 - acc: 0.9891\n",
            "Epoch 13/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 0.0557 - acc: 0.9893WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.0554 - acc: 0.9894\n",
            "Epoch 14/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9908WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.0463 - acc: 0.9909\n",
            "Epoch 15/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9919WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0380 - acc: 0.9917\n",
            "Epoch 16/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9937WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0346 - acc: 0.9935\n",
            "Epoch 17/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0308 - acc: 0.9946WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 131us/sample - loss: 0.0306 - acc: 0.9945\n",
            "Epoch 18/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0204 - acc: 0.9975WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 132us/sample - loss: 0.0230 - acc: 0.9970\n",
            "Epoch 19/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0211 - acc: 0.9965WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 133us/sample - loss: 0.0202 - acc: 0.9967\n",
            "Epoch 20/40\n",
            "4500/6045 [=====================>........] - ETA: 0s - loss: 0.0165 - acc: 0.9978"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-bd735814dff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnested_grid_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbow_model_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnested_grid_result_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_grid_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3459\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3461\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhzTAqY7brC6",
        "colab_type": "code",
        "outputId": "ea930ab0-0384-4386-eef6-62f89f13ace7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (nested_grid_result_Adam.best_score_, nested_grid_result_Adam.best_params_))\n",
        "\n",
        "means = nested_grid_result_Adam.cv_results_['mean_test_score']\n",
        "stds = nested_grid_result_Adam.cv_results_['std_test_score']\n",
        "params = nested_grid_result_Adam.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "print()\n",
        "predictions_nested_Adam = nested_grid_result_Adam.predict(test_feature_matrix)\n",
        "predicted_labels_nested_Adam = label_encoder.inverse_transform(list(predictions_nested_Adam))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_nested_Adam)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.548883 using {'epochs': 20, 'learning_rate': 0.001}\n",
            "0.535318 (0.004333) with: {'epochs': 20, 'learning_rate': 0.005}\n",
            "0.548883 (0.007505) with: {'epochs': 20, 'learning_rate': 0.001}\n",
            "0.522581 (0.010179) with: {'epochs': 20, 'learning_rate': 0.01}\n",
            "0.303888 (0.021451) with: {'epochs': 20, 'learning_rate': 0.1}\n",
            "0.190240 (0.041566) with: {'epochs': 20, 'learning_rate': 0.2}\n",
            "0.532672 (0.014120) with: {'epochs': 40, 'learning_rate': 0.005}\n",
            "0.541935 (0.008314) with: {'epochs': 40, 'learning_rate': 0.001}\n",
            "0.489992 (0.001638) with: {'epochs': 40, 'learning_rate': 0.01}\n",
            "0.338792 (0.025922) with: {'epochs': 40, 'learning_rate': 0.1}\n",
            "0.119107 (0.009950) with: {'epochs': 40, 'learning_rate': 0.2}\n",
            "\n",
            "Classification accuracy:  13.8 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug4n17gTf9Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This took like ages and ended up with disconnected runtime!!!!!!!!!!!!!!\n",
        "\n",
        "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
        "# # https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/\n",
        "# # https://mlfromscratch.com/nested-cross-validation-python-code/#/\n",
        "\n",
        "# # Choose cross-validation techniques for the inner and outer loops\n",
        "# # outer loop: N folds\n",
        "# # inner loop: each fold of outer loop is folded again! To avoid too heavy computaion do not fold too many times or try running parallel\n",
        "\n",
        "# bow_model_Adam = KerasClassifier(build_fn=create_bow_model_Adam, epochs=40, batch_size=100, verbose=0)\n",
        "\n",
        "# stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "# mc_cb = ModelCheckpoint(filepath='models/bow_model.h5', monitor='accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "# inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "# outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "# # Set the hyperparameter grid. Also possible hyperparams (when parametrized for the functions called!):\n",
        "# # - dropout rate\n",
        "# # - if optimizer is SGD, momentum\n",
        "# # - max_feat (input shape)\n",
        "# nodes = [100, 200]\n",
        "# epochs = [40, 64]\n",
        "# batch_size = [4, 10, 32]\n",
        "# learning_rate = [0.0001, 0.001, 0.01]\n",
        "# params = dict(learning_rate = learning_rate, nodes = nodes, epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "# nested_grid_Adam = GridSearchCV(estimator=bow_model_Adam, param_grid=params, n_jobs=1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "# nested_grid_result_Adam = nested_grid_Adam.fit(X, joint_labels, verbose = 1) #, callbacks=[stop_cb])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2HwFxZzwOP",
        "colab_type": "text"
      },
      "source": [
        "## Fitting BOW-classifier OLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQnwLuHcDntV"
      },
      "source": [
        "Now we will fit the data. Here we will also need the validation data. \n",
        "Let's try with different optimizers available in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwm-SO6ciArq",
        "colab_type": "code",
        "outputId": "a8221790-2150-41f9-d989-c844c7fdfc39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "example_count, feature_count = train_feature_matrix.shape\n",
        "class_count = len(train_labels.unique()) \n",
        "\n",
        "inp = Input(shape = (feature_count, ))                   # Tuple. The size of the inputlayer is the number of the vectors\n",
        "hidden = Dense(300, activation=\"relu\")(inp)  \n",
        "hidden2 = Dense(300, activation=\"relu\")(hidden)\n",
        "dropout = Dropout(0.3)(hidden2)                          # Non-linear activation function. tanh or relu? \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden2) # As many output possibilities as we have input classes. \n",
        "                                                         # Softmax: produces probability distribution of the classes\n",
        "bow_model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "bow_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               6000300   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               30100     \n",
            "=================================================================\n",
            "Total params: 6,120,700\n",
            "Trainable params: 6,120,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SM_ApKFQ0b0",
        "colab_type": "code",
        "outputId": "c70c7131-710d-4558-99b1-6fa665e9e392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Choose best OP and LR and fit the model\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=40, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "bow_model.compile(optimizer = 'Adam', loss=\"categorical_crossentropy\", metrics=['accuracy']) # with one-hot encodings loss = categorical_crossentropy!!!\n",
        "\n",
        "bow_history = bow_model.fit(train_feature_matrix, train_hot, batch_size=32, \n",
        "                 verbose=1, epochs=100, validation_data=(dev_feature_matrix, dev_hot), callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 5295 samples, validate on 750 samples\n",
            "Epoch 1/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 2.6677 - accuracy: 0.3804 - val_loss: 1.9671 - val_accuracy: 0.5320\n",
            "Epoch 2/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 1.2771 - accuracy: 0.7005 - val_loss: 1.7759 - val_accuracy: 0.5733\n",
            "Epoch 3/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.4830 - accuracy: 0.8997 - val_loss: 1.8270 - val_accuracy: 0.5640\n",
            "Epoch 4/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.1798 - accuracy: 0.9726 - val_loss: 1.9260 - val_accuracy: 0.5627\n",
            "Epoch 5/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0993 - accuracy: 0.9817 - val_loss: 1.9714 - val_accuracy: 0.5747\n",
            "Epoch 6/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0696 - accuracy: 0.9858 - val_loss: 2.1091 - val_accuracy: 0.5547\n",
            "Epoch 7/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0519 - accuracy: 0.9883 - val_loss: 2.1371 - val_accuracy: 0.5680\n",
            "Epoch 8/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0404 - accuracy: 0.9924 - val_loss: 2.1620 - val_accuracy: 0.5733\n",
            "Epoch 9/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0256 - accuracy: 0.9962 - val_loss: 2.2527 - val_accuracy: 0.5547\n",
            "Epoch 10/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0174 - accuracy: 0.9960 - val_loss: 2.2290 - val_accuracy: 0.5747\n",
            "Epoch 11/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0137 - accuracy: 0.9968 - val_loss: 2.2201 - val_accuracy: 0.5667\n",
            "Epoch 12/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0119 - accuracy: 0.9957 - val_loss: 2.3929 - val_accuracy: 0.5333\n",
            "Epoch 13/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 2.2785 - val_accuracy: 0.5613\n",
            "Epoch 14/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 2.3964 - val_accuracy: 0.5600\n",
            "Epoch 15/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 2.3295 - val_accuracy: 0.5640\n",
            "Epoch 16/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 2.4246 - val_accuracy: 0.5467\n",
            "Epoch 17/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 2.6989 - val_accuracy: 0.5347\n",
            "Epoch 18/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 2.5332 - val_accuracy: 0.5440\n",
            "Epoch 19/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0080 - accuracy: 0.9966 - val_loss: 2.4533 - val_accuracy: 0.5587\n",
            "Epoch 20/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 2.4860 - val_accuracy: 0.5507\n",
            "Epoch 21/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 2.4765 - val_accuracy: 0.5573\n",
            "Epoch 22/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0066 - accuracy: 0.9975 - val_loss: 2.5494 - val_accuracy: 0.5627\n",
            "Epoch 23/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0071 - accuracy: 0.9958 - val_loss: 2.5375 - val_accuracy: 0.5613\n",
            "Epoch 24/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 2.5119 - val_accuracy: 0.5613\n",
            "Epoch 25/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0057 - accuracy: 0.9974 - val_loss: 2.5735 - val_accuracy: 0.5587\n",
            "Epoch 26/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0073 - accuracy: 0.9966 - val_loss: 2.5949 - val_accuracy: 0.5520\n",
            "Epoch 27/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0061 - accuracy: 0.9970 - val_loss: 2.6181 - val_accuracy: 0.5613\n",
            "Epoch 28/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 2.6174 - val_accuracy: 0.5507\n",
            "Epoch 29/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 2.6717 - val_accuracy: 0.5573\n",
            "Epoch 30/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0063 - accuracy: 0.9972 - val_loss: 2.7221 - val_accuracy: 0.5520\n",
            "Epoch 31/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0060 - accuracy: 0.9968 - val_loss: 2.7433 - val_accuracy: 0.5587\n",
            "Epoch 32/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0059 - accuracy: 0.9977 - val_loss: 2.7393 - val_accuracy: 0.5587\n",
            "Epoch 33/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0052 - accuracy: 0.9974 - val_loss: 2.7310 - val_accuracy: 0.5627\n",
            "Epoch 34/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0052 - accuracy: 0.9972 - val_loss: 2.9057 - val_accuracy: 0.5493\n",
            "Epoch 35/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0058 - accuracy: 0.9970 - val_loss: 2.7401 - val_accuracy: 0.5587\n",
            "Epoch 36/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0051 - accuracy: 0.9979 - val_loss: 2.8739 - val_accuracy: 0.5387\n",
            "Epoch 37/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0061 - accuracy: 0.9972 - val_loss: 2.8903 - val_accuracy: 0.5427\n",
            "Epoch 38/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0059 - accuracy: 0.9974 - val_loss: 2.8280 - val_accuracy: 0.5453\n",
            "Epoch 39/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0055 - accuracy: 0.9970 - val_loss: 3.0793 - val_accuracy: 0.5507\n",
            "Epoch 40/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0064 - accuracy: 0.9970 - val_loss: 2.9128 - val_accuracy: 0.5533\n",
            "Epoch 41/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0054 - accuracy: 0.9974 - val_loss: 2.8703 - val_accuracy: 0.5587\n",
            "Epoch 42/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0054 - accuracy: 0.9970 - val_loss: 2.9243 - val_accuracy: 0.5587\n",
            "Epoch 43/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0046 - accuracy: 0.9972 - val_loss: 3.0769 - val_accuracy: 0.5480\n",
            "Epoch 44/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0046 - accuracy: 0.9979 - val_loss: 2.9347 - val_accuracy: 0.5573\n",
            "Epoch 45/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 2.9236 - val_accuracy: 0.5667\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00045: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_zfbGvKjIL1",
        "colab_type": "code",
        "outputId": "eaf0ecb8-308b-427b-a0c7-2340ab33d4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# form feature matrix for test data set\n",
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "print(test_feature_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MR-tO01ge6V",
        "colab_type": "code",
        "outputId": "8fec90f7-944c-4b4a-f9c5-ea6b22ac6487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "predictions = np.argmax(bow_model.predict(test_feature_matrix), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"predicted labels: \\n\", predicted_labels)\n",
        "print()\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: \n",
            " ['NE NA ' 'NA  ' 'NA  ' ... 'NA  ' 'NA  ' 'PB NA ']\n",
            "\n",
            "Classification accuracy:  9.0 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqmyFiO0CSrd",
        "colab_type": "code",
        "outputId": "a16ba3a7-26b2-486c-9f1d-c95fc30b1cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "# Without np.argmax:\n",
        "\n",
        "# predictions = bow_model.predict(test_feature_matrix) \n",
        "# predictions = one_hot_encoder.inverse_transform(predictions) # transfer form one hot encodings to numerical labels \n",
        "# predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "# print(\"predicted labels: \\n\", predicted_labels)\n",
        "# print()\n",
        "# print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: \n",
            " ['NE NA ' 'NA  ' 'NA  ' ... 'NA  ' 'NA  ' 'PB NA ']\n",
            "\n",
            "Classification accuracy:  9.0 percent\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHF-M4emLu-h",
        "colab_type": "text"
      },
      "source": [
        "Why does the model perform so badly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrzogBR6abt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_numbers)))\n",
        "print(\"-development data: \", len(np.unique(dev_numbers)))\n",
        "print(\"-test data: \", len(np.unique(test_numbers)))\n",
        "print()\n",
        "inter = np.intersect1d(train_numbers, dev_numbers)\n",
        "inter2 = np.intersect1d(train_numbers, test_numbers)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U28dT9xjZYKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# # Confusion matrix has the true labels on rows, and predicted labels on columns in sorted order\n",
        "# print(cnf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8AaFnYV_DRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot confusion matrix\n",
        "# plt.figure()\n",
        "# plot_confusion_matrix(cnf_matrix, classes = all_labels, normalize = False)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JdOntcnMyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # np-argmaxin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "# print(predictions[0])\n",
        "# print(model.predict(test_feature_matrix)[0][25])\n",
        "# print(model.predict(test_feature_matrix)[0])\n",
        "# print(sum(model.predict(test_feature_matrix)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqG5963ZhDa",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.2: Recurrent Neural Network Classifier (multi-class)\n",
        "\n",
        "Modify your codes from milestone 1.1 to use recurrent neural networks (e.g. LSTM or biLSTM) in the classifier. Evaluate your model and report your results with different hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOfJs2YLeAtA",
        "colab_type": "text"
      },
      "source": [
        "For RNN-calssifier we use Tokenizer which turns tokens, in our case the words of training data to integers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7j5rqbNzm3O",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KY7mtEZl-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=97000, # max num of most common words\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6eKbcvnd9m4",
        "colab_type": "code",
        "outputId": "931e1f43-ad55-4aba-fb02-8cc6863e06fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from pprint import pprint    # pretty-printer\n",
        "\n",
        "def truncate_dict(d, count=10):\n",
        "    # Returns at most count items from the given dictionary.  \n",
        "    return dict(i for i, _ in zip(d.items(), range(count)))\n",
        "\n",
        "# Check if 0 is in the index, and print examples of the mapping\n",
        "# 0 is reserved for padding!\n",
        "print(tokenizer.word_index.get(0))\n",
        "pprint(truncate_dict(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "{'ei': 3,\n",
            " 'että': 4,\n",
            " 'ja': 1,\n",
            " 'kun': 9,\n",
            " 'mutta': 8,\n",
            " 'oli': 7,\n",
            " 'on': 2,\n",
            " 'ovat': 10,\n",
            " 'se': 6,\n",
            " 'tai': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDmMZZfg62w",
        "colab_type": "code",
        "outputId": "dac7f186-30cf-42ee-8f87-84545360ed40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "print(len(train_sequences)) \n",
        "\n",
        "# Print an example text, its corresponding sequence, and the tokens it represents\n",
        "print('Text:', train_text[0][0:200]) # first item of the suffled data (index not 0!)\n",
        "#print('Text:', train_text.head(1)[0:200]) # first item of the suffled data (index not 0!)\n",
        "print('Sequence:', train_sequences[0][:10])\n",
        "print('Mapped back:', [tokenizer.index_word[i] for i in train_sequences[0][:10]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5295\n",
            "Text:  Tämähän menee ihan hurjaksi muotihurjasteluksi . Jo toistamiseen tälle syksylle postauksen aiheena vaatetus . Mutta tämä liittyy tavallaan matkailuun . Ja katsokaa nyt näitä , mitkä maailman ihanimma\n",
            "Sequence: [21429, 13274, 40282, 11697, 15340, 796, 657, 388, 12826, 3002]\n",
            "Mapped back: ['logistiikka', 'jenni', 'lindholm', 'laskutus', 'ritva', 'ota', 'yhteyttä', 'nimi', 'puhelinnumero', 'sähköposti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js1rksZ6AA6f",
        "colab_type": "code",
        "outputId": "1acf0d02-6125-42e2-c3e3-f9d5bfb2b2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "lengths = [len(s) for s in train_sequences]\n",
        "print('Lengths:', lengths[:10], 'min:', min(lengths), 'max:', max(lengths), 'mean:', np.mean(lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengths: [384, 921, 158, 194, 932, 388, 123, 384, 1577, 167] min: 0 max: 79136 mean: 583.0457034938621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcr2XZ8zg1H",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Since Keras demands for all of the input items (separate documents of our training data) to have the same length, we need to \"pad\" all but the longest document by filling in the \"missing\" number of words with zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA209Lqxzd8t",
        "colab_type": "code",
        "outputId": "09347272-6e76-4e09-eab6-b391a06c77af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequence_length = np.floor(np.mean(lengths)).astype(int) # based on mean value of input length: we will cut sequences longer than this and pad with zeros sequeces shorter than this\n",
        "\n",
        "type(sequence_length)\n",
        "\n",
        "padded_X = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk9M_AtI1Jc",
        "colab_type": "code",
        "outputId": "79c30b03-d73f-430b-9fb4-d151a98a6f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Prepare model development data\n",
        "\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_text)\n",
        "padded_dev = pad_sequences(\n",
        "    dev_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_dev.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(750, 583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HXbVzAF6UH",
        "colab_type": "text"
      },
      "source": [
        "## LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vn7FWLFGeJQ",
        "colab_type": "text"
      },
      "source": [
        "## HUOMAA!!!\n",
        "Tämä teksti alkuperäisestä RNN-classification notebookista!\n",
        "\n",
        "We define a basic RNN model that takes the RNN cell class (RNN_class) as an argument:\n",
        "\n",
        "- input: sequence of sequence_length integers corresponding to words\n",
        "- embedding: randomly initialized mapping from integers to embedding_dim-dimensional vectors\n",
        "- rnn: recurrent neural network with rnn_units-dimensional state\n",
        "- output: num_classes-dimensional fully connected layer with softmax activation\n",
        "\n",
        "# KATSO NÄITÄ!\n",
        "We're intentionally leaving out a few fairly obvious things that would be expected to help here, including\n",
        "\n",
        "- Any form of regularization, e.g. dropout\n",
        "- Initializing the embeddings with pre-trained word vectors (ks. fasttext)\n",
        "- Masking to ignore padding (see Masking and padding with Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhRaStPGH7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# We'll use these model parameters for all of our examples here.\n",
        "embedding_dim = 50 # input vector\n",
        "rnn_units = 100\n",
        "\n",
        "def build_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "    input_ = Input(shape=(sequence_length,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized. Layer turns positive integers (indexes) into dense vectors of fixed size\n",
        "    rnn = RNN_class(rnn_units)(embedding) # can support different RNNs\n",
        "    output = Dense(num_classes, activation='softmax')(rnn)\n",
        "    return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "sequence_length = padded_X.shape[1]\n",
        "vocab_size = tokenizer.num_words\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5y48zddjkf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4icO32GGX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = build_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikBgdQhpVwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 100\n",
        "stop_cb = EarlyStopping(monitor = 'val_acc', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbxk_lppo_tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_history = lstm_model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])\n",
        "# , callbacks=[stop_cb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXAKNMb5PbM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model test data\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "padded_test = pad_sequences(\n",
        "    test_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrcF1iyLzBkW",
        "colab_type": "text"
      },
      "source": [
        "Predict with LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3MGvq6CzAIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(lstm_model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIycPktw_m39",
        "colab_type": "text"
      },
      "source": [
        "## Bidirectional LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8vdw-jcfAKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Input(shape=(sequence_length,)\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWl2MJjjGmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7L2tJHok6_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "hist_bi_lstm = model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvFxjZZ2ZnQ",
        "colab_type": "text"
      },
      "source": [
        "Predict with bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQIaS_tR2Tiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPskSfawiO5I",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.1: Deep contextual representations with Bert (multi-class)\n",
        "Train a Bert classifier to predict the register categories. Similar to Milestone 1, the setting is multi-class, and the evaluations should include results with different hyperparameters.\n",
        "\n",
        "Neural language models trained on large\n",
        "unannotated corpora can be used to create\n",
        "contextualized representations of\n",
        "meaning\n",
        "\n",
        "[How-to](https://github.com/HannaKi/Deep_Learning_in_LangTech_course/blob/master/bert_text_classification_extended_comments.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSfx6EwPjmR",
        "colab_type": "text"
      },
      "source": [
        "Keep only labels (and texts which correspond to those labels) which appear in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqrKmnsLI1AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 1500\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Kwvy7nKMYt",
        "colab_type": "text"
      },
      "source": [
        "To avoid out of memory we have to limit the input size to MAX_EXAMPLES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82iZbp0N0FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncate_data(data, MAX_EXAMPLES):\n",
        "  if len(data) > MAX_EXAMPLES: # truncate data if needed to avoid OOM\n",
        "    print('Note: truncating examples from {} to {}'.format(len(data), MAX_EXAMPLES)) # should take stratified subsample?\n",
        "    data = data[:MAX_EXAMPLES]\n",
        "  return(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3LbSG4Vxgp_",
        "colab_type": "code",
        "outputId": "a831beb6-f908-44df-e2a7-bda175071f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# We use dev_ and test_ data sets which only have labels that appear in train data\n",
        "# All the datas will be truncated if needed\n",
        "\n",
        "train_ = truncate_data(train, MAX_EXAMPLES)\n",
        "dev_ = truncate_data(dev, MAX_EXAMPLES)\n",
        "test_ = truncate_data(test, MAX_EXAMPLES)\n",
        "\n",
        "frames = [train_, dev_, test_]\n",
        "for d in frames:\n",
        "  print(d.shape)\n",
        "  #print(d.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: truncating examples from 5295 to 1500\n",
            "Note: truncating examples from 1513 to 1500\n",
            "(1500, 3)\n",
            "(756, 3)\n",
            "(1500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMN0rcLOK4rx",
        "colab_type": "code",
        "outputId": "cf23a68f-4776-45b4-e8ca-912b10dd4b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Miksi ei onnistu ensin ottaa päällekäiset labelit pois ja sitten leikata 1000:een?!?!?\n",
        "\n",
        "train_labels = train_['label']\n",
        "\n",
        "test_ = test_[test_['label'].isin(train_labels.tolist())]\n",
        "dev_ = dev_[dev_['label'].isin(train_labels.tolist())] \n",
        "\n",
        "frames = [train_, dev_, test_]\n",
        "for d in frames:\n",
        "  print(d.shape)\n",
        "  #print(d.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 3)\n",
            "(746, 3)\n",
            "(1476, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZp6Fz5VWzlp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "95ae8a98-a7cb-4743-bcd7-02e022c03475"
      },
      "source": [
        "!pip3 install keras-bert"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (0.81.0)\n",
            "Requirement already satisfied: keras-transformer>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (0.33.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.4)\n",
            "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.11.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.22.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.14.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.6.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.7.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert) (0.41.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fVv1axNJhVn",
        "colab_type": "text"
      },
      "source": [
        "Set an environment variable for keras-bert to use tensorflow.python.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Td8Gp40JeYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zEw1o-yKC3T",
        "colab_type": "text"
      },
      "source": [
        "## Download pretrained FinBERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRasDCPh1jUM",
        "colab_type": "text"
      },
      "source": [
        "Download pretrained TurkuNLP FinBERT from: https://github.com/TurkuNLP/FinBERT and prepare it for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J87QH8NiKFQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6f2cb6fc-3c55-4bf4-9358-efa13a229e1d"
      },
      "source": [
        "!wget -nc http://dl.turkunlp.org/finbert/bert-base-finnish-cased-v1.zip\n",
        "\n",
        "# Give -n argument so that existing files aren't overwritten \n",
        "!unzip -n bert-base-finnish-cased-v1.zip\n",
        "\n",
        "# Store paths to important files:\n",
        "\n",
        "bert_vocab_path = 'bert-base-finnish-cased-v1/vocab.txt'\n",
        "bert_config_path = 'bert-base-finnish-cased-v1/bert_config.json'\n",
        "bert_checkpoint_path = 'bert-base-finnish-cased-v1/bert_model.ckpt' # suffixes not required\n",
        "\n",
        "# \n",
        "model_is_cased = True"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘bert-base-finnish-cased-v1.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  bert-base-finnish-cased-v1.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cXqCpbSa-20",
        "colab_type": "text"
      },
      "source": [
        "## Load vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LyiI4clbCU1",
        "colab_type": "code",
        "outputId": "cce7d420-3bf4-4daf-cf33-4371851068e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "vocab = []\n",
        "with open(bert_vocab_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
        "\n",
        "\n",
        "# Print a list with every 500th vocabulary item (includes suffixes)\n",
        "print(vocab[0::500]) # 0 is for padding!\n",
        "print(vocab[103]) # tags are included"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '##hin', '##tila', 'tehtä', '##ce', 'avulla', '##uttua', 'Sil', '##iver', 'tarvits', 'kilometrin', 'sanotaan', '##ya', 'onnistuu', 'tapaus', 'rento', '##otin', '##kasvat', 'kauhe', 'puolin', 'ymmärrän', 'polttoain', 'arvot', 'ajattelu', '##impiin', 'huomasi', 'tietokoneen', 'tiedämme', 'johdossa', 'teemme', 'paikallisen', 'rekryt', 'vuosikymmeniä', 'kohdistuu', 'tyhmiä', 'baa', '##ipa', 'aero', '##ehtien', 'viimeisteli', '##llosta', 'luulevat', 'verenpain', 'tuottamaan', 'vahingot', 'opiskelijan', '##päivinä', '##uksellisesti', 'uskottava', '##elemaan', 'ilmennyt', 'määrätieto', 'leppo', 'yksityiset', 'kirjailijan', 'vastikään', 'samoista', '##misiin', '##pankkien', 'tuut', 'muutoinkin', 'säilyi', '##ivuoren', '##pauksia', 'kaupungilta', '##lalle', 'tervetulleeksi', 'lähteitä', 'isoin', 'Tuskinpa', 'hallinnut', '##kanslerin', '##jela', 'siniset', 'erikoiskokeella', 'todistavat', 'itsehallinto', 'Oran', 'vikaan', 'ISIS', 'hankinnan', '##eleitä', '##heitolla', '##upar', 'raam', 'edelläkävijä', 'tunnistettu', 'kohtaamis', 'punaisena', 'murtaa', '##ske', 'toimivuudesta', 'zo', '##ektro', '114', 'seinästä', 'Kuhmon', 'viesteissä', 'tukim', 'sellaisiksi', 'Rautio']\n",
            "[SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srU3eKQvbMbW",
        "colab_type": "text"
      },
      "source": [
        "## Load BERT configuration\n",
        "\n",
        "Just peek into the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHDZ0R-nbO6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(bert_config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Print configuration contents\n",
        "# pprint(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5KmJ0OecMDp",
        "colab_type": "text"
      },
      "source": [
        "## Create BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYckpNLBcXMJ",
        "colab_type": "text"
      },
      "source": [
        "To create the tokenizer, we'll need a mapping from vocabulary items to their corresponding integer indices. We do this conventionally using enumerate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBV0vAhwcLqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create mapping from vocabulary items to their indices in the vocabulary\n",
        "token_dict = { v: i for i, v in enumerate(vocab) }\n",
        "\n",
        "# Print some random examples of the mapping\n",
        "#pprint(dict(random.choices(list(token_dict.items()), k=10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-T7zTcU08",
        "colab_type": "code",
        "outputId": "d139ff8d-b4dc-4678-8bd4-ed734ebd6ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "# Test keras-Bert tokenizer\n",
        "from keras_bert import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(token_dict, cased=model_is_cased) \n",
        "\n",
        "# Let's test that out\n",
        "for s in ['Heippa BERT!', 'Tämä lause on suomeksi. tai ei', 'Kiinaa tai japania: 你', 'Yksi kirjain a on yksi']:\n",
        "    print('Original string:', s)\n",
        "    print('Tokenized:', tokenizer.tokenize(s))\n",
        "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
        "    print('Encoded:', indices)\n",
        "    print('Segments:', segments)\n",
        "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
        "    print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string: Heippa BERT!\n",
            "Tokenized: ['[CLS]', 'Hei', '##ppa', 'B', '##ER', '##T', '!', '[SEP]']\n",
            "Encoded: [102, 5050, 2096, 415, 9981, 50031, 380, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Hei ##ppa B ##ER ##T !\n",
            "\n",
            "Original string: Tämä lause on suomeksi. tai ei\n",
            "Tokenized: ['[CLS]', 'Tämä', 'lause', 'on', 'suomeksi', '.', 'tai', 'ei', '[SEP]']\n",
            "Encoded: [102, 1131, 17580, 145, 11695, 111, 337, 193, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Tämä lause on suomeksi . tai ei\n",
            "\n",
            "Original string: Kiinaa tai japania: 你\n",
            "Tokenized: ['[CLS]', 'Kiinaa', 'tai', 'japan', '##ia', ':', '你', '[SEP]']\n",
            "Encoded: [102, 39876, 337, 9780, 157, 236, 101, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Kiinaa tai japan ##ia : [UNK]\n",
            "\n",
            "Original string: Yksi kirjain a on yksi\n",
            "Tokenized: ['[CLS]', 'Yksi', 'kirjain', 'a', 'on', 'yksi', '[SEP]']\n",
            "Encoded: [102, 3420, 28106, 151, 145, 1034, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Yksi kirjain a on yksi\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFPCAVJd9gD",
        "colab_type": "text"
      },
      "source": [
        "## Prepare labels\n",
        "\n",
        "Y holds the labels the model will learn to predict. We will train with the labels the truncated training data contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ioyLbFPKqkS",
        "colab_type": "code",
        "outputId": "f6429ada-91b2-467d-97d2-f15524232eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "# training data\n",
        "\n",
        "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
        "train_label = train_['label'].tolist() \n",
        "\n",
        "#label_encoder.fit(all_labels)\n",
        "label_encoder.fit(train_label)\n",
        "Y = label_encoder.transform(train_label) # encoded labels (not one hot!)\n",
        "\n",
        "# Take note of how many unique labels there are in the data\n",
        "num_labels = len(set(Y))\n",
        "\n",
        "# Print out some examples\n",
        "print(\"Training data\")\n",
        "print('Number of unique labels in training data:', num_labels)\n",
        "print(type(train_label), train_label[:10])\n",
        "print(type(Y), Y[:10])\n",
        "print()\n",
        "\n",
        "# development data\n",
        "print(\"Development data\")\n",
        "dev_label = dev_['label']#.tolist()\n",
        "y = label_encoder.transform(dev_label)\n",
        "print('Number of unique labels in training data:', len(set(y)))\n",
        "print(type(dev_label), dev_label[:10])\n",
        "print(type(y), y[:10])\n",
        "print()\n",
        "# test data\n",
        "print(\"Test data\")\n",
        "test_label = test_['label'].tolist()\n",
        "true_labels = label_encoder.transform(test_label)\n",
        "print('Number of unique labels in training data:', len(set(true_labels)))\n",
        "print(type(test_label), test_label[:10])\n",
        "print(type(true_labels), true_labels[:10])\n",
        "# print(len(test_label))\n",
        "# print(len(test_['text']))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data\n",
            "Number of unique labels in training data: 64\n",
            "<class 'list'> ['DS IG ', 'RS OP ', 'NE NA ', 'SR NA ', 'MT OS ', 'NA  ', 'DP IN ', 'DS IG ', 'DF ID ', 'DT IN ']\n",
            "<class 'numpy.ndarray'> [11 58 37 61 32 36  8 11  6 12]\n",
            "\n",
            "Development data\n",
            "Number of unique labels in training data: 48\n",
            "<class 'pandas.core.series.Series'> 0     OA NA \n",
            "1     DS IG \n",
            "2     DS IG \n",
            "3     DF ID \n",
            "4     OA NA \n",
            "6     DF ID \n",
            "7     MT OS \n",
            "8     CB NA \n",
            "9       HI  \n",
            "10      NA  \n",
            "Name: label, dtype: object\n",
            "<class 'numpy.ndarray'> [42 11 11  6 42  6 32  1 22 36]\n",
            "\n",
            "Test data\n",
            "Number of unique labels in training data: 47\n",
            "<class 'list'> ['HI  ', 'NA  ', 'DT IN ', 'DP IN ', 'DT IN ', 'AV OP ', 'CB NA ', 'IN  ', 'NE NA ', 'TB NA ']\n",
            "<class 'numpy.ndarray'> [22 36 12  8 12  0  1 27 37 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7z05ec1Hfr",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize input data\n",
        "Keep token indices and segment ids in separate lists and store as numpy arrays. X here is the final vectorized form of the input we'll be providing to the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYRRn2deAt7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model_inputs(text):\n",
        "  token_indices, segment_ids = [], []\n",
        "  for text in text:\n",
        "      # tokenizer.encode() returns a sequence of token indices\n",
        "      # and a sequence of segment IDs. BERT expects both as input,\n",
        "      # even if the segments IDs are just all zeros (like here).\n",
        "      tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
        "      token_indices.append(tid)\n",
        "      segment_ids.append(sid)\n",
        "  inp = [np.array(token_indices), np.array(segment_ids)] # Format input as list of two numpy arrays\n",
        "  return inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxBP2UFcBLFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = make_model_inputs(train_['text'])\n",
        "x = make_model_inputs(dev_['text'])\n",
        "test_inp = make_model_inputs(test_['text'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4WUr_OfBU1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print some examples\n",
        "print(X[0].shape)\n",
        "print('Token indices:')\n",
        "print(test_inp[0][:2])\n",
        "print('Decoded:')\n",
        "for i in test_inp[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(test_inp[1][:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O7-xnrg3nyT",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained BERT model form checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QBf-vQiWdcY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load pretrained BERT model\n",
        "\n",
        "We'll use the keras-bert function load_trained_model_from_checkpoint to load the model from the checkpoint we downloaded earlier.\n",
        "\n",
        "Explanation for a few parameters from keras-bert documentation:\n",
        "\n",
        "- training: If training, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
        "- trainable: Whether the model is trainable. The default value is the same with training.\n",
        "\n",
        "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use training=False but trainable=True.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReplOvS3nAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False, # ignore MLM [masked language modeling] and NSP [next sentence prediction] parts of the model\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH # define the size of input layer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3jk07Kw4Lrm",
        "colab_type": "code",
        "outputId": "09529d0e-f9af-46b2-ae17-6e0cdaa4efdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# token and segment inputs, just as we made\n",
        "print(pretrained_model.inputs)\n",
        "\n",
        "print(pretrained_model.outputs)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'Input-Token:0' shape=(?, 250) dtype=float32>, <tf.Tensor 'Input-Segment:0' shape=(?, 250) dtype=float32>]\n",
            "[<tf.Tensor 'Encoder-12-FeedForward-Norm/add_1:0' shape=(?, 250, 768) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwqTgaZ55vD0",
        "colab_type": "text"
      },
      "source": [
        "Size of the output layer does not match our label count. This must be fixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQNBB7Gk5etU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained_model.summary()\n",
        "# same transformer layer repeated 12 times: self\n",
        "# attention, feedforward, drop out, normalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss678QTb63-0",
        "colab_type": "text"
      },
      "source": [
        "## Build classification model by wrapping the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNV-IQ6dX4De",
        "colab_type": "text"
      },
      "source": [
        "We will \"catch\" the model output and plug our own output layer on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8G2WqT763Cd",
        "colab_type": "code",
        "outputId": "caae24bd-3f18-43c6-9572-3019c34bdb5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLpSFbIA0jQv",
        "colab_type": "text"
      },
      "source": [
        "Wrapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-G-h5baSoWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip freeze"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8-fYS4y7ICV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "out = Dense(num_labels, activation='softmax')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
        "model = Model(\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuDLml6v83JD",
        "colab_type": "text"
      },
      "source": [
        "Now the size of the output layer matches the number of our data labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2eJUnT78uQX",
        "colab_type": "code",
        "outputId": "142067ab-fd00-45c2-8bc4-362b896430da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.output"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense/Softmax:0' shape=(?, 64) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BekdEFHw7jqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYwXxwPe9J1j",
        "colab_type": "text"
      },
      "source": [
        "## Create optimizer\n",
        "\n",
        "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We won't go into detail on these optimizer settings, but will instead largely copy parameters used in the original BERT work.\n",
        "\n",
        "(If you're interested in tuning the training process, trying different values of LEARNING_RATE is a good place to start!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs_9hj7ey_ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 12                             #16\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.00002                   #0.01\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 16                          #8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoqDpD3C9LJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train_['text']),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2MKRaKT9iHk",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkYA99RO9grF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy', # encoded labels!\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR2OgW1B9rsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_sparse_categorical_accuracy', patience=4, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "mc_cb = ModelCheckpoint(filepath='models/BERT_multiclass.h5', monitor='val_sparse_categorical_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x, y), \n",
        "    callbacks=[stop_cb, mc_cb]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn_hm0UDNsgK",
        "colab_type": "code",
        "outputId": "8c536341-456c-42b2-fd66-0fb80d5c6aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrP6xhZJmpV",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noTnBHs4cJcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load best model\n",
        "model.load_weights('models/BERT_multiclass.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvK_t2LXNWam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(test_inp), axis=1) # np.argmax gives the index which has the highest value e.g. class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWn3JBFpQSak",
        "colab_type": "code",
        "outputId": "628fb23f-7137-4308-93ed-7a6a01eebcd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "predicted_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['HI  ', 'PB NA ', 'DT IN ', ..., 'OA NA ', 'PB NA ', 'MT OS '],\n",
              "      dtype='<U12')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uamUmR_ZQMFq",
        "colab_type": "code",
        "outputId": "faf6e0f1-8197-47b3-ec95-069d07648f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification accuracy: \", round(accuracy_score(test_label, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  64.8 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ2lrJaZQ42Z",
        "colab_type": "text"
      },
      "source": [
        "Accuracy pretty good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSi-7hwqqTs",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.2: Error analysis\n",
        "Compare the errors made by the classifiers you have trained from milestones 1 and 2.1. Are there any patterns? How do the errors one model makes differ from those made by another.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5QbHGcldxep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy\n",
        "# confusion matrix\n",
        "# classification_report\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HcrqoxqxiI",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 3.1: Bert (multi-LABEL)\n",
        "\n",
        "For milestone 3 we will train two multi-label classifiers, one using non-deep contextual representations, the other using Bert. In this setting, each label is assigned independently. Do hyperparameter optimization on these classifiers.\n",
        "\n",
        "We selected Support Vector Machine (SVM) to be the non-deep classifier for this milestone. According to [scikit documentation](https://https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) \"SVM -- is widely regarded as one of the best text classification algorithms (although it’s also a bit slower than naïve Bayes). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8PqFpHSXrPR",
        "colab_type": "text"
      },
      "source": [
        "According to the task \"each label is assigned independently\". We interpreted this to mean, that highlevel and sublevel labels have no connection and did not explore possible (and presumable) connections. \n",
        "\n",
        "How ever we skimmed how to improve multilabel classification. According to \n",
        "[scikit-multilearn](http://scikit.ml/labelrelations.html): \"Multi-label classification tends to have problems with overfitting and underfitting classifiers when the label space is large, especially in problem transformation approaches. A well known approach to remedy this is to split the problem into subproblems with smaller label subsets to improve the generalization quality.\"\n",
        "In our case we could improve the classification by training the separate models for high leve and sublevel registers, since the labels are not independent.\n",
        "\n",
        "Besides of this a priori knowledge relations between classes could be ecplored by scikit-multilearn tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrVf5bheeRNH",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation for milestone 3\n",
        "\n",
        "Since we do multi label classification we are dealing with pure highlevel and sublevel registers instead of combinations. This means we have significantly less labels (for example 'NA OP' becomes 'NA' and 'OP') and we have to prepare the data differently from the previous milestones.\n",
        "\n",
        "For convenience we manipulate the combined data set, where column 'data' holds the information, to which original dataset each row belongs to We will separate all the registers and one hot encode them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwa_GN5xdzy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data manipulation: high level registers\n",
        "# hiregs = ['NA', 'OP', 'IN', 'ID', 'HI', 'IP', 'IG', 'LY', 'SP', 'OS']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXVJU0CQPIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean up the label(register) column:\n",
        "\n",
        "def from_registers(col):\n",
        "  lst = col.tolist() # column to list\n",
        "  registers = [[]] # list of lists, as long as the original column\n",
        "  idx = 0\n",
        "  for element in lst:    # NA OP\n",
        "      parts = element.split(' ') # NA #OP\n",
        "      for i in parts: \n",
        "        if i == '': # omit empty strings\n",
        "          continue\n",
        "        registers[idx].append(i) # append to list in list\n",
        "      if len(lst)-1 > idx : # if we have more labels to separate\n",
        "        idx = idx+1 \n",
        "        registers.insert(idx, []) # add new empty list to list\n",
        "  return registers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5laiU_0WPw",
        "colab_type": "code",
        "outputId": "a606fc08-cd5c-4f6d-ca51-05eb98c8510b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# use combined data formed at the beinning of the notebook\n",
        "\n",
        "print(df.shape)\n",
        "all_regs=from_registers(df['label']) # use custom made function to list the registers\n",
        "df['regs'] = all_regs\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7564, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "      <th>regs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "      <td>[DS, IG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[RS, OP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "      <td>[NE, NA]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[SR, NA]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "      <td>[MT, OS]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data      regs\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train  [DS, IG]\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train  [RS, OP]\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train  [NE, NA]\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train  [SR, NA]\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train  [MT, OS]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwsgYM9Q0mVx",
        "colab_type": "text"
      },
      "source": [
        "Next we will turn those listed registers (column 'regs') to one hot encodings with MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbMk-Mz9Uj4",
        "colab_type": "code",
        "outputId": "e912ecdc-4bf8-4fb7-c7bb-50ea4fcc97b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "dummies = pd.DataFrame(mlb.fit_transform(df['regs']), columns=mlb.classes_, index=df.index)\n",
        "# join the dummies to original data frame\n",
        "df = pd.concat([df, dummies], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "      <th>regs</th>\n",
              "      <th>AV</th>\n",
              "      <th>CB</th>\n",
              "      <th>CM</th>\n",
              "      <th>DF</th>\n",
              "      <th>DP</th>\n",
              "      <th>DS</th>\n",
              "      <th>DT</th>\n",
              "      <th>EB</th>\n",
              "      <th>EN</th>\n",
              "      <th>FA</th>\n",
              "      <th>FC</th>\n",
              "      <th>FS</th>\n",
              "      <th>HA</th>\n",
              "      <th>HI</th>\n",
              "      <th>IB</th>\n",
              "      <th>ID</th>\n",
              "      <th>IG</th>\n",
              "      <th>IN</th>\n",
              "      <th>IP</th>\n",
              "      <th>IT</th>\n",
              "      <th>JD</th>\n",
              "      <th>LT</th>\n",
              "      <th>LY</th>\n",
              "      <th>MT</th>\n",
              "      <th>NA</th>\n",
              "      <th>NE</th>\n",
              "      <th>OA</th>\n",
              "      <th>OB</th>\n",
              "      <th>OP</th>\n",
              "      <th>OS</th>\n",
              "      <th>PB</th>\n",
              "      <th>PO</th>\n",
              "      <th>QA</th>\n",
              "      <th>RA</th>\n",
              "      <th>RE</th>\n",
              "      <th>RP</th>\n",
              "      <th>RS</th>\n",
              "      <th>RV</th>\n",
              "      <th>SL</th>\n",
              "      <th>SP</th>\n",
              "      <th>SR</th>\n",
              "      <th>TB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "      <td>[DS, IG]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[RS, OP]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "      <td>[NE, NA]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[SR, NA]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "      <td>[MT, OS]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data  ... SP  SR  TB\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train  ...  0   0   0\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train  ...  0   0   0\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train  ...  0   0   0\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train  ...  0   1   0\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train  ...  0   0   0\n",
              "\n",
              "[5 rows x 46 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJz7I5ZbmlOo",
        "colab_type": "code",
        "outputId": "6d353a68-0fc2-4428-d8fa-91adbae7d0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# subregs = np.setdiff1d(mlb.classes_, hiregs)\n",
        "# subregs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AV', 'CB', 'CM', 'DF', 'DP', 'DS', 'DT', 'EB', 'EN', 'FA', 'FC',\n",
              "       'FS', 'HA', 'IB', 'IT', 'JD', 'LT', 'MT', 'NE', 'OA', 'OB', 'PB',\n",
              "       'PO', 'QA', 'RA', 'RE', 'RP', 'RS', 'RV', 'SL', 'SR', 'TB'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inhn9BbbcJTl",
        "colab_type": "code",
        "outputId": "a918a7c2-6316-4e07-b4f1-cc0a765c2808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# separate train, test and dev datas from one hot encoded combined dataset\n",
        "\n",
        "train = df[df['data'] == 'train'] \n",
        "test = df[df['data'] == 'test'] \n",
        "dev = df[df['data'] == 'dev'] \n",
        "\n",
        "# for grid search (maybe not needed/used)\n",
        "# joint = df[df['data'] != 'test'] \n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(dev.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 46)\n",
            "(1513, 46)\n",
            "(756, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FCQ2W_8f5vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate X (features) and Y (labels) for training data and x and y for development data\n",
        "\n",
        "# train\n",
        "X = train['text'] # features\n",
        "Y = train.drop(['regs', 'label','text', 'data'], axis=1) # labels\n",
        "\n",
        "# dev\n",
        "x = dev['text']\n",
        "y = dev.drop(['regs', 'label','text', 'data'], axis=1)\n",
        "\n",
        "# test\n",
        "test_feats = test['text']\n",
        "true_labels = test.drop(['regs', 'label','text', 'data'], axis=1) # true labels (one-hot encoded) for testing the model predictions\n",
        "true_regs = test['regs']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtm800nFKgNE",
        "colab_type": "text"
      },
      "source": [
        "## Linear support vector machine (SVM)\n",
        "\n",
        "To do multilabel classification we wrap the SVM with [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier).\n",
        "\n",
        "\n",
        "For this milestone we followed [scikit tutorial](https://https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) and this [tutorial](https://www.datatechnotes.com/2020/03/multi-output-classification-with-multioutputclassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAIrQZ5JlyXx",
        "colab_type": "code",
        "outputId": "02366e9c-baf5-4e84-cb3d-78afbbe9f81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# form feature matrixes\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features = 20000, ngram_range = (1,1)) \n",
        "\n",
        "train_feature_matrix = vectorizer.fit_transform(X)\n",
        "dev_feature_matrix = vectorizer.transform(x)\n",
        "test_feature_matrix = vectorizer.transform(test_feats)\n",
        "\n",
        "# for grid search:\n",
        "# XG = vectorizer.fit_transform(XG)\n",
        "\n",
        "# Scale input data for SVM\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "train_fm = scaler.fit_transform(train_feature_matrix)\n",
        "dev_fm = scaler.transform(dev_feature_matrix)\n",
        "test_fm = scaler.transform(test_feature_matrix)\n",
        "\n",
        "print(\"shape of the training data: \", train_fm.shape)\n",
        "print(\"shape of the development data: \", dev_fm.shape)\n",
        "# print(\"shape of the  data: \", XG.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5295, 20000)\n",
            "shape of the development data:  (756, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UawS5wlqCTzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# guide for multiouput moldel for one hot encoded data: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5 \n",
        "# and https://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "cat = Y.columns.values.tolist() # labels/registers\n",
        "\n",
        "SVM_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(SGDClassifier(loss='hinge', random_state=42, alpha=0.0001, max_iter=40, early_stopping=True))),\n",
        "            ])\n",
        "for category in cat:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    SVM_pipeline.fit(train_feature_matrix, Y[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = SVM_pipeline.predict(test_fm)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(true_labels[category], prediction)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPURaKKaw-zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.datatechnotes.com/2020/03/multi-output-classification-with-multioutputclassifier.html\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "SGDC = SGDClassifier(loss='hinge', random_state=42, alpha=0.0001, max_iter=40, early_stopping=True)\n",
        "model = MultiOutputClassifier(estimator=SGDC)\n",
        "\n",
        "\n",
        "#We can check the parameters of the model by the print command.\n",
        "\n",
        "print(model)\n",
        "\n",
        "model.fit(train_feature_matrix, Y)\n",
        "# training accuracy\n",
        "print(model.score(train_feature_matrix, Y))\n",
        "\n",
        "predictions = model.predict(test_fm)\n",
        "multilabel_confusion_matrix(true_labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DJReTy7wW7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F_UWKlUf9p6",
        "colab_type": "text"
      },
      "source": [
        "## Bert (multi-LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsTCKCcthHT5",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9OdoQjFF4ZU",
        "colab_type": "text"
      },
      "source": [
        "#### Feature tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDISZvoOf8wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 1500\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWZ2Ni5dVq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To avoid OOM, truncate one hot encoded data\n",
        "\n",
        "train = truncate_data(train, MAX_EXAMPLES)\n",
        "dev = truncate_data(dev, MAX_EXAMPLES)\n",
        "test = truncate_data(test, MAX_EXAMPLES)\n",
        "\n",
        "frames = [train, dev, test]\n",
        "for d in frames:\n",
        "  print(d.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIR1xUNihUZw",
        "colab_type": "text"
      },
      "source": [
        "We need to convert our data into a format that BERT understands. Some utility functions are provided to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wWJ3csi8k0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a8d76dc2-68ea-4459-83a8-4401b0913c2b"
      },
      "source": [
        "# remember, INPUT_LENGTH is a parameter for make_model_inputs -function. \n",
        "# We also use BERT tokenizer here.\n",
        "\n",
        "X = make_model_inputs(train['text'])\n",
        "x = make_model_inputs(dev['text'])\n",
        "test_inp = make_model_inputs(test['text'])\n",
        "\n",
        "print(X[0].shape)\n",
        "print(x[0].shape)\n",
        "print(test_inp[0].shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 250)\n",
            "(756, 250)\n",
            "(1500, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9SmUCcoPt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print some examples (looks the same as in milestone 2)\n",
        "print('Token indices:')\n",
        "print(X[0][:2])\n",
        "print('Decoded:')\n",
        "for i in X[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(X[1][:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpqqKGfeQdl",
        "colab_type": "text"
      },
      "source": [
        "Extract one hot label encodings from the truncated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5SgLu9xePxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "Y = train.drop(['regs', 'label','text', 'data'], axis=1)\n",
        "\n",
        "# dev\n",
        "y = dev.drop(['regs','label','text', 'data'], axis=1)\n",
        "\n",
        "# test\n",
        "true_labels = test.drop(['regs', 'label','text', 'data'], axis=1).reset_index(drop=True) # these are used for evaluating model predictions\n",
        "true_regs = test['regs'].reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ze-mp5bpO5",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyz0YK_kbtBU",
        "colab_type": "text"
      },
      "source": [
        "This is the same as in milestone 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwQmd2F_boLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 30\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.00001\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 16 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sypxw1aJbyix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the optimizer again if hyperparameters above have been changed from milestone 2\n",
        "\n",
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train['text']),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaQNy6Uptzw",
        "colab_type": "text"
      },
      "source": [
        "### Wrap the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eZhMgp9N7KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model again if model hyperparameter INPUT_LENGTH has changed\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False, # ignore MLM [masked language modeling] and NSP [next sentence prediction] parts of the model\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH # define the size of input layer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAyXXiqCpeG5",
        "colab_type": "code",
        "outputId": "3d7d97d2-edb5-4486-de5a-bded8135589f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(pretrained_model.inputs)\n",
        "\n",
        "print(pretrained_model.output)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'Input-Token:0' shape=(?, 250) dtype=float32>, <tf.Tensor 'Input-Segment:0' shape=(?, 250) dtype=float32>]\n",
            "Tensor(\"Encoder-12-FeedForward-Norm/add_1:0\", shape=(?, 250, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS7bRChHubSD",
        "colab_type": "code",
        "outputId": "2aa6aadb-a338-4dbc-f3a4-de3b32f4c496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# model.outputs is a list, here with a single item. Here\n",
        "# pretrained_model.outputs[0] just grabs that item (the output tensor).\n",
        "# Indxing that tensor with [:,0] gives the first position in the sequence\n",
        "# for all elements in the batch (the `:`).\n",
        "\n",
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvbwRaMgE35m",
        "colab_type": "text"
      },
      "source": [
        "Once again we wrap the pretrained model. In multi-label classification instead of softmax(), we use sigmoid() to get the probabilities for each label separatedly. Instructions for multilabel classification were read from [Javaid Nabi's blogpost](https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d) and "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P3irmGsE2wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labels = len(mlb.classes_) # size of the output layer\n",
        "\n",
        "out = Dense(num_labels, activation='sigmoid')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
        "model = Model(                                          # sigmoid gives us independent probabilities for each class \n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")\n",
        "\n",
        "# Now size of the last layer matches the number of the layers and activation our multilabel purposes\n",
        "model.output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OouJ_0jzYw1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "637a5140-3926-4ccb-8dc2-40cc7c61e646"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy', # one hot encoded labels! Is hamming_loss better?\n",
        "    metrics=['categorical_accuracy']#, 'binary_accuracy'] #metrics=['binary_accuracy', 'categorical_accuracy'] \n",
        ")\n",
        "\n",
        "# loss functions tried: categorical_crossentropy < binary_crossentropy \n",
        "# ordered by test accuracy (treshold set to 0.9) from worst to best\n",
        "\n",
        "# categorical_crossentropy: Classification accuracy:  17.7 percent\n",
        "\n",
        "# An important choice to make is the loss function. We use the binary_crossentropy loss and not the usual in multi-class classification \n",
        "# used categorical_crossentropy loss. This might seem unreasonable, but we want to penalize each output node independently. So we pick \n",
        "# a binary loss and model the output of the network as a independent Bernoulli distributions per label.\n",
        "# source: https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIeGmqi-iXGL",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSwPhGPMZFqU",
        "colab_type": "code",
        "outputId": "61e50452-058f-4f4f-a8b9-bbb330dd9183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_categorical_accuracy', patience= 8, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "mc_cb = ModelCheckpoint(filepath='models/BERT_multilabel.h5', monitor='val_categorical_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x, y),\n",
        "    callbacks=[stop_cb, mc_cb]\n",
        ")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 756 samples\n",
            "Epoch 1/30\n",
            "1500/1500 [==============================] - 248s 165ms/sample - loss: 0.4977 - categorical_accuracy: 0.0687 - val_loss: 0.1932 - val_categorical_accuracy: 0.2593\n",
            "Epoch 2/30\n",
            "1500/1500 [==============================] - 203s 135ms/sample - loss: 0.1589 - categorical_accuracy: 0.3253 - val_loss: 0.1294 - val_categorical_accuracy: 0.3571\n",
            "Epoch 3/30\n",
            "1500/1500 [==============================] - 197s 131ms/sample - loss: 0.1212 - categorical_accuracy: 0.3747 - val_loss: 0.1030 - val_categorical_accuracy: 0.3532\n",
            "Epoch 4/30\n",
            "1500/1500 [==============================] - 197s 131ms/sample - loss: 0.0985 - categorical_accuracy: 0.3840 - val_loss: 0.0903 - val_categorical_accuracy: 0.3479\n",
            "Epoch 5/30\n",
            "1500/1500 [==============================] - 205s 137ms/sample - loss: 0.0852 - categorical_accuracy: 0.4087 - val_loss: 0.0827 - val_categorical_accuracy: 0.3651\n",
            "Epoch 6/30\n",
            "1500/1500 [==============================] - 202s 134ms/sample - loss: 0.0730 - categorical_accuracy: 0.4440 - val_loss: 0.0812 - val_categorical_accuracy: 0.3915\n",
            "Epoch 7/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0656 - categorical_accuracy: 0.4707 - val_loss: 0.0777 - val_categorical_accuracy: 0.3743\n",
            "Epoch 8/30\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 0.0565 - categorical_accuracy: 0.4893 - val_loss: 0.0763 - val_categorical_accuracy: 0.4127\n",
            "Epoch 9/30\n",
            "1500/1500 [==============================] - 195s 130ms/sample - loss: 0.0494 - categorical_accuracy: 0.4980 - val_loss: 0.0750 - val_categorical_accuracy: 0.4114\n",
            "Epoch 10/30\n",
            "1500/1500 [==============================] - 200s 133ms/sample - loss: 0.0418 - categorical_accuracy: 0.5460 - val_loss: 0.0761 - val_categorical_accuracy: 0.4471\n",
            "Epoch 11/30\n",
            "1500/1500 [==============================] - 195s 130ms/sample - loss: 0.0362 - categorical_accuracy: 0.5587 - val_loss: 0.0762 - val_categorical_accuracy: 0.4167\n",
            "Epoch 12/30\n",
            "1500/1500 [==============================] - 195s 130ms/sample - loss: 0.0317 - categorical_accuracy: 0.5487 - val_loss: 0.0758 - val_categorical_accuracy: 0.4352\n",
            "Epoch 13/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0273 - categorical_accuracy: 0.5693 - val_loss: 0.0779 - val_categorical_accuracy: 0.3968\n",
            "Epoch 14/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0242 - categorical_accuracy: 0.5527 - val_loss: 0.0768 - val_categorical_accuracy: 0.4140\n",
            "Epoch 15/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0217 - categorical_accuracy: 0.5580 - val_loss: 0.0769 - val_categorical_accuracy: 0.4048\n",
            "Epoch 16/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0199 - categorical_accuracy: 0.5613 - val_loss: 0.0781 - val_categorical_accuracy: 0.3876\n",
            "Epoch 17/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0182 - categorical_accuracy: 0.5513 - val_loss: 0.0780 - val_categorical_accuracy: 0.3929\n",
            "Epoch 18/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0168 - categorical_accuracy: 0.5547 - val_loss: 0.0789 - val_categorical_accuracy: 0.4286\n",
            "Epoch 19/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0155 - categorical_accuracy: 0.5753 - val_loss: 0.0795 - val_categorical_accuracy: 0.3902\n",
            "Epoch 20/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0144 - categorical_accuracy: 0.5473 - val_loss: 0.0800 - val_categorical_accuracy: 0.4246\n",
            "Epoch 21/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0137 - categorical_accuracy: 0.5487 - val_loss: 0.0798 - val_categorical_accuracy: 0.4180\n",
            "Epoch 22/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0128 - categorical_accuracy: 0.5533 - val_loss: 0.0796 - val_categorical_accuracy: 0.3915\n",
            "Epoch 23/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0122 - categorical_accuracy: 0.5453 - val_loss: 0.0802 - val_categorical_accuracy: 0.3955\n",
            "Epoch 24/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0117 - categorical_accuracy: 0.5307 - val_loss: 0.0809 - val_categorical_accuracy: 0.3889\n",
            "Epoch 25/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0113 - categorical_accuracy: 0.5533 - val_loss: 0.0809 - val_categorical_accuracy: 0.4206\n",
            "Epoch 26/30\n",
            "1500/1500 [==============================] - 195s 130ms/sample - loss: 0.0110 - categorical_accuracy: 0.5480 - val_loss: 0.0811 - val_categorical_accuracy: 0.4101\n",
            "Epoch 27/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0106 - categorical_accuracy: 0.5387 - val_loss: 0.0806 - val_categorical_accuracy: 0.4087\n",
            "Epoch 28/30\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0106 - categorical_accuracy: 0.5267 - val_loss: 0.0810 - val_categorical_accuracy: 0.4048\n",
            "Epoch 29/30\n",
            "1500/1500 [==============================] - 196s 130ms/sample - loss: 0.0103 - categorical_accuracy: 0.5200 - val_loss: 0.0811 - val_categorical_accuracy: 0.4048\n",
            "Epoch 30/30\n",
            "1488/1500 [============================>.] - ETA: 1s - loss: 0.0102 - categorical_accuracy: 0.5457Restoring model weights from the end of the best epoch.\n",
            "1500/1500 [==============================] - 197s 132ms/sample - loss: 0.0102 - categorical_accuracy: 0.5473 - val_loss: 0.0808 - val_categorical_accuracy: 0.4101\n",
            "Epoch 00030: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtoBHqFyiM7B",
        "colab_type": "code",
        "outputId": "b6ea7adf-1102-4ec7-d212-fb1c1cbafc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3hUVf7H8fdJJxUCBEJCCS0IhCQkgIAooAg2ULEBKogIoth21WXddXV1baur/nbXhhUVBERFWEGUJiKKoUOoCQRISINAepuZ8/vjDiFAyiSZMJnJ9/U8eZK5c+fOuZnkM2fOPUVprRFCCOEa3BxdACGEEPYjoS6EEC5EQl0IIVyIhLoQQrgQCXUhhHAhEupCCOFCbAp1pdQYpdR+pVSSUmp2NfvcppTao5RKVErNt28xhRBC2ELV1k9dKeUOHABGAalAAjBBa72n0j49gEXASK31KaVUiNY6q/GKLYQQoiq21NQHAkla60Na6zJgATDuvH3uA97SWp8CkEAXQgjH8LBhnzDgWKXbqcCg8/bpCaCU+gVwB57VWn9f00HbtGmju3TpYntJhRBCsGXLlhNa67bV3W9LqNvCA+gBDAfCgfVKqSit9enKOymlpgPTATp16sTmzZvt9PRCCNE8KKWO1HS/Lc0vaUDHSrfDrdsqSwWWaq3LtdaHMdrge5x/IK31HK11vNY6vm3bat9ohBBC1JMtoZ4A9FBKRSilvIA7gKXn7bMEo5aOUqoNRnPMITuWUwghhA1qDXWttQmYBawE9gKLtNaJSqnnlFJjrbutBE4qpfYAa4EntNYnG6vQQgghqlZrl8bGEh8fr6VNXQgh6kYptUVrHV/d/TKiVAghXIiEuhBCuBAJdSGEcCES6kI0cXvT8/h2exqy9KSwhb0GHwkh7ExrzRe/H+PZZYmUmSwcPlHIo1f1dHSxRBMnoS6cgtmiyS0uJ9jPy9FFuSiKy8z8Zckuvt6axrAebWjj782bqw7SwtOdGVd0c3TxRBMmoS6avOOni3lw/lZ2p+Uy/fKuPDSyBz6e7o4uVqM5lF3AA/O2sj8zn0eu7MHDVxqDs8vNFl5asY8WXu7cPbiLYwspmiwJddGkrT+QzSMLtlFmsjA8MoS31iazbEc6z43rw/DIEEcXz+6W70rnycU78XRXfHLPQK7oeXY6jTduj6HUZOFv3ybi4+HObQM61nAk0VzJhVLRJFksmjdXHWDyx78TEuDD0ocu4/2745l/3yA83BRTPk7gwflbycoruSjlMZktjXr8crOF5/+3hwfmbaV7iD//e3jYOYEO4Onuxn8nxnJ5z7b86eudfLv9/CmYhJARpaIJyiks49GF21l/IJubY8P4x0198fU6+6Gy1GTmvZ8O8d+1SXi7u/HEmEgmDeqMu5uq93OWlJtJPVVM6qki6/di0k6fvZ2dX0rv0EDGx4UzLqYDbfy97XGqAGTklvDg/K1sOXKKyYM785freuPlUX19q7jMzD2f/E5CyinemtifMX3b260soumrbUSphLqwG601vySd5J2fktifkc/oPu0ZHxdObMeWKGVb4G47eooH523lREEZz47tw4SBHat97OEThTy9ZDcbkk4QHR7ECzdF0TcsqMbjF5eZ2ZOeR+LxXHal5nIgq4C0U0WcKCg7Zz9Pd0WHli0Ib9WCsJYtaOPvzYakE+xMzcXDTTE8MoRb4sIY0SsEb4/6t+//knSCh7/YRnG5mZfH92NsdAebHldQauLuDzexKy2XOXfHM8IFm6JE1STURaMzWzQ/JGbwzk/J7EzNJSTAm/6dWrHuQBYl5Ra6tvVjfP9wbooNo0PLFlUeQ2vNp78e4R/f7aFdoA/vTIojKrzmgD7zuKU7jvP8//aSU1jKlCER/OHqnvh7e1BUZmLP8Tx2peWyKy2XxLQ8DmblY7H+ybf286JXaACdgn0Jb+VLmDXEw1v50jbAu8qa/4HMfL7akso329LIyi+lpa8nY6M7ML5/OP3Cg2p988ovKTc+BZwqJuFIDu+vP0TXtv68e2d/uocE1P7LriS3uJxJH/zGwcwCPp4ygCHd29Tp8cI5SaiLRlNmsrBkWxrvrk/mUHYhXVr7MuOKbtwUG4aPpzv5JeWs2JXB4i2p/J6Sg1IwtFsbxseFMbpP+4omlcJSE7O/3sWyHccZ2SuE12+LpqVv3bou5haX8+rKfczbdJS2/t4EtvAkObuAM3/ebfy9iQoLJCosiL7Wr9AgH5s/QZzPZLawIekEX21N44fEDEpNFrqH+DO+fziDu7UmK6+kohkn9VSRtSmnmNzi8nOOMy6mAy/eFIWfd/36LJwqLOOOOb9x7FQRn04dSHyX4Hod54yiMhMHMgs4kJHPvox8DmTmcyi7gO7tArjqkhBGRIbQMdi3Qc9hq3KzhZeW72PNvkwi2wfQt0MQfcODiAoLsmvzl7ORUBd2V1hq4ovfj/LBz4fJyCuhd2ggM4d349qo0GrbtY+eLOKrral8vS2VYznF+Hm5c21UKCN6hfCvH/Zz+EQhf7w6kplXdMOtAW3j246e4vUfD+Dl7lYR3lFhQbQL9K53gNcmt7ic5bvS+WpLKpuPnDrnvhae7tbav/EJILxVC8Iq/WyPcMrKL+GO934jO7+UefcNol94yxr3LzdbKCw1kZVfyv6MfOMr0/h+NKeoYj8fTzd6hATQpY0fu9NyOXyiEIDIdgGMvCSEK3uFENupVYOuZVQnt7icWfO38vPBEwzt3prjp0sqnh8gNMjHeH07BBEVHkjfsCBCAnzsXo6mSEJd2M2pwjI+2ZjC3F9TOF1UzqCIYGYO78YVPdvaHJgWiyYhJYfFW1JZviudwjIzbfy9+PeEWIZ0c/7mg8MnCtmfkU+Hlj6Et/Klla9no72ZVHb8dDG3vfcr+SUmrrqkHYWlJgrLTMb3UvM5P5ed15PH3U0R0caPyHYBRLYPoGe7AHq1D6BjsO85gX0ou4A1+7JYvTeLhJQcTBZNK19PhkeGcOUlIVzesy2BPp4NPpcjJwuZ+kkCR3OKeOGmKG6LN7pu5pWUk5iWx+60XHYfN5rUDp8orPg01i7QmxGRIYyN7sCgrq0b5c2moUrKzfzlm93MHN6N7iH+9TqGhLqwi1OFZYx6Yz0nCkq56pIQZg7vTlznVg06ZlGZiU2HcugbFkTbgOb7cdpejuUUMXPeFk4VluPr5Y6ftwf+3h74ernj7+2B35kv632t/b3oERJAtxC/Ol/szS0u5+eD2azZm8Xa/VmcKirHw01xWY82/HFUpE3XQ6qy6dBJ7v98Cxp49844Lu3ausb9C0rPXjfZdvQUa/ZlUVRmJiTAm+v6hTI2ugMxdbhQ35hKTWZmfr6VNfuyeP22aG7uH16v40ioC7t4dmkin/6awpf3Dyauc8PabYVrMVs024+dYtXeLBYmHCOnsIyx0R14YnRkndrfv9x8jKe+2UXHYF8+mjyALm386lyW4jIzq/dlsmzHcdbuy6bMbKFTsC83RIcyNjqMyPZ1uxhtL2UmCw/M28qqvZm8cFNfJg3qXO9jSaiLBjuUXcDVb6zn1viOvHRzlKOLI5qwvJJy3vspmQ83HMZs0dx1aRceGtmdVjXM2WOxaF79YT/vrEtmaPfWvD0xjiDfhjfj5JWUs3J3Bkt3HGdj8knMFk1kuwBuiA5lXEzYRb3gO2v+VlYmZvLcuD4NnuJBQl002H2fbmZj0gnWPTFCmkmETTJyS3jjxwN8ueUYft4ezBzejalDIy6Ys6eozMRjC7ezMjGTiYM68fexffB0t/9A9xMFpazYlc7SHcdJSDmFUnB173ZMG9aV+M6tGq15xmS28PCCbSzflcHfru/N1MsiGnxMCXXRIL8mn2TC+7/xxOhIHhzR3dHFEU7mQGY+r6zYx+p9WYQG+fDYqJ6M7x+Ou5siI7eEe+cmsDc9j79e15t7hna5KG3faaeL+WLTUT7fdITTReVEhwdx77CuXNu3PR52fEMxmS08tmgHy3Yc56/XXcK0YV3tclwJdVFvFotm7FsbyCkoY83jw116ZkTRuH47dJKXlu9lR2ouke0CuHtIZ/69+iCFpWb+MyGWEb0u/ojY4jIzX21N5aMNhzl0opCwli2YMqQLtw/s2OBePGaL5o+LtrNk+3FmX9OL++04XbKEuqi3r7ak8scvd/Dm7THcGBvm6OIIJ6e1ZvmuDP65ch9HThYR1rIFH06Jp1f7QIeWy2LRrNmXxQcbDvHboRz8vT24fUBHpgzpUq92d7NF88TiHXy9Na1RPuFKqIt6KS4zM+K1dbQL9OabB4Y2aECQEJWVmSys2pvJoIhgWjexkaG703L5cMNhlu04jkVrxvRtz7VRoQyKaG3T9SSLRTP7650s2pzKY1f15JGreti9jBLqol7+vfogr/94gC/vH8yABg49F8LZZOSWMPfXFOZvOloxtUO3tn4M6tqaQRHBXNq1Ne0Czx3BarFo/rJkF1/8foyHR3bnD1dHNkrZJNRFnWXllTD8tXVc0bMt79wZ5+jiCOEwJrOFxON5/HboJJsO55BwOIf8UhMAEW38KgJ+YEQwb69L4vPfjvLA8G48MTqy0S761hbqsvKRuMC/fjhAudnC7Gt6ObooQjiUh7sb0R1bEt2xJTOu6IbZotlTEfInWb4rnQUJxyr2n3F510YNdJvK7LBnFk3SnuN5LNpyjHuHRtC5dd1H9AnhytzdFFHhQUSFB3Hf5V0xWzR7042QD2zhya1x4Q6fkkBCXVTQWvPi8r0EtfDkoZH2v8AjhKtxd1MVs4E2FbJGqaiwbn82G5JO8MiVPewyTFsIcfFJqAvAuCD0wvK9RLTxa9BkQ0IIx7Ip1JVSY5RS+5VSSUqp2VXcP0Upla2U2m79mmb/oorG9EXCMZKyCvjzNb1qXPRYCNG01dqmrpRyB94CRgGpQIJSaqnWes95uy7UWs9qhDKKRpZXUs4bPx5gUEQwo3q3c3RxhBANYEuVbCCQpLU+pLUuAxYA4xq3WOJienttMqeKynj6+t4Ov3IvhGgYW0I9DDhW6Xaqddv5xiuldiqlFiulOtqldKLR7U3P46NfDnNTbFiTuoIvhKgfe3VpXAZ8obUuVUrNAOYCI8/fSSk1HZgO0KlTJzs9tagLrTVJWQWs2J3B97sz2JOeh7+3B0+MbpwhzUKIi8uWUE8DKte8w63bKmitT1a6+QHwz6oOpLWeA8wBY5qAOpVU1JvWmt1peXyfmM6K3Rkcyi5EKYjr1Iq/XncJ10aFEhrUwtHFFELYgS2hngD0UEpFYIT5HcDEyjsopUK11unWm2OBvXYtpagzi0Wz5egpvrfWyNNOF+Pupri0azD3DI1gdO92hJw3IZEQwvnVGupaa5NSahawEnAHPtJaJyqlngM2a62XAg8rpcYCJiAHmNKIZRa12Jh8gj99tZNjOcV4ubsxrEcbHrmqB6MuaVfjWpFCCOcnszS6kJJyM6+u3M+HGw4T0caPR6/qwcheIQQ0cBUXIUTTIbM0NhO703L5w6LtHMgs4K5LO/Pna3vh6yUvrxDNjfzXOzmzRfPuT8m8ueoArXy9+OSeAQyPvPjrPQohmgYJdSd29GQRf1i0nc1HTnFdVCj/uLGvtJkL0cxJqDshrTULE47x3P/24O6mePP2GMbFdJDRoEIICXVnk51fyp+/3smqvVkM6daa126NpkNL6WMuhDBIqDuRjckneGj+NvJLTTx9fW/uGdIFNzepnQshzpJQdxIbDp7g3rkJdAr25Yvpl9KzXYCjiySEaIIk1J3A+gPZ3PfpZiLa+DFv2iBa+3s7ukhCiCZKQr2J+8ka6N3b+vP5tEEES+8WIUQNJNSbsLX7s5jx2RZ6hPjz+b2DpLuiEKJWEupN1Jp9mdz/2VZ6tjcCvaWvBLoQonayGGUTtGpPJjM+20Jk+wDm3XupBLoQwmYS6k3MD4kZzJy3hd6hgXw+bRBBvjIZlxDCdhLqTcj3uzN4YN5W+nQI4rNpgwhqIYEuhKgbCfUm4vvd6cyav5Wo8CA+vXcggU1xuty9/4PcVEeXQghRAwn1JmD5rnQenL+NfuFBfDq1iQb6joWwcBJ8ch0UZDu6NEKIakioO9i2o6d4ZME2Yju25NN7BzXNBS2y98P/HoX2/SA/E+bfBmWFji6VEKIKEuoOdKqwjAfnbaVdoA8fTh6Av3cT7GFaVgiLJoOnL0xcBLd+DOnb4ct7wGxydOmEEOeRUHcQi0Xz2KLtnCgo4+1J/ZtuL5flT0D2Phj/PgSGQuQ1cN2/4OBK+O4P4KDlEIUQVZNQd5B3fkpm3f5snr7+EvqFt3R0caq2bR5snweXPwHdRp7dHj8Vhv0Rts6F9a/V//hZe2HercYFWCGEXTTBz/uu79fkk/zrh/3cEN2BOy/t7OjiVC1rL3z3R+gyDIbPvvD+kU9Dbhqs/QcEdoDYSbYf22KG396G1c+DuRSO/gah/aBlJ/uVX4hmSmrqF1lWfgkPfbGNLm38eOnmqKa5WlFpgdGO7h0A4z8EN/cL91EKxv4Hug6HZQ9D0mrbjn3qCMy9AX74K3S/CqatMZpwvrnfCPvmJDe1+Z2zaHQS6heRyWzh4S+2UVBazjuT4prmhVGtjRr6iQMw/gMIaFf9vh5ecNtn0LYXLLob0nfUfNytn8E7QyB9J9z4DtwxD8Lj4NpX4cgvsPHf9j+fpiplA7zZD76d5eiSCBcjoX4RvbHqAL8dyuEfN0YR2b6JLnKx7TPYucBocul6Re37+wTCpMXg09JoHz999MJ9CrLgiwmwdBZ0iIUHNkLMRKO2DxB9B/QeB2teqPmNwVUUZMPie8HdE3bMh8Rv7HPc8hL43x/gx2fg4I9Qkmef4wqnIqF+kazdl8Vba5O5Pb4jt8SFO7o4VcvYbfR26TrcuDhqq8BQuHMxmErg8/FQlHP2vj1L4e1LIXkNjH4J7l56Ydu5UnD9m+DXBr66D8qL7XE2TZPFDF/fByWnYer30KE/LHsU8o43/NgrnoTNH8Kv/4V5t8ArnWHOcFj5F9i/AopPN/w5LjazCYpPOboUTkVC/SJIO13MY4u2c0loIH8f18fRxalaaT58ORl8guDm96tuR69JyCVwx3w4lQILJhq1869nwKK7ICgcZqyHwQ+AWzV/cr7BcOPbcGI/rHq2oWfTdP38Lzi0Fq55xfjUcvP7YC6DJQ+AxVL/42791OiNNOyPMPso3LXE+NmjBfw+B764A17pAu9eBitmw95lTT8szSbjb+mf3YxPevtXyNgIGyjtoH7G8fHxevPmzQ557oupzGThtvd+JSmrgGUPXUZEGz9HF+lCWsNX0yDxa5i8DLpcVv9j7f4KFk8FN0/QFrj8caPW725jP/wVs2HTO3DnV8aFVFdyeD18Og763gI3zznb/LT5I/jfYzDmZbh0Zt2Pe3wbfDgaOg+GO7++8A25vATSNkPKL3BkAxxLAFOx0WR219cQFtfwc2sMy5+E39+DPjcZZS/MggBrT6vYu6BVE+051siUUlu01vHV3i+h3rj+viyRj39J4Z1J/bkmKvTiPbHZZISqhw1zsW/+2JgGYORf69bsUp3f34edC2HMK8aF0LooLzaaDIpPwwO/GjX4uijNhy2fgH97Y6CUt3/dHt9Y8jONWnKLlnDf2nPLpbVRk05eC9PXQbveth+3KAfeuwLQMP0n8Gtd+2NMZZCaAEtmGrX1O7+GjgPqeEKN7Pf3YfnjMHgWjH4BzOVw4HvYMheSVhn7dB0OcZMh8jrb/s5dhIS6Ay3flc4D87Zyz9AuPHPDRWx2yUuHD0dB7jFw9wIvP/AKML57+4OXv/XnAPBsYQwy6jIUJn1VffPIxZS+E94fCZFjjN41tnb7PLDSuFCYZ51J0qMF9BwNfcdDj1HGuTqCxQyf3WjUkO9bU3VoF2TB24MhIBTuWw0eNiwubjEbbecpG4z2+brWuHNT4ZProfCEcU2k06V1e3xjObgK5t8KPcfA7Z9f+Mnj9DFjUNzWz4zX2rcNxEyA2LuhTQ/b/14cpbwE0PX+e5RQd5Dk7ALG/fcXerTzZ+H0wXh5XKSwtFhg3ng48itc9qhR8y0rMOZwKc03vpcVGH3RywqhLN9o875riXGhsqnY8CasegbGvV37wKaCbPh+NuxebHSvvOH/jNrv7q9gzxIozDbe1HpdB1G3GDU8W5uD7GHtS/DTyzDuLYi9s/r99q8wauxDHoarn6/9uGv+AetfhRv+bdRY6yPvuBHs+Rkw6Uvjzd2RMvfAh1dDqy7GG1VNn7QsZuMC/Na5xu/OYgLlblRavK0Vl8oVmMq3fVtD627QujsEd228N/yyIsjcbfTqOr7d+J6913jN6jJgrxK7hLpSagzwf4A78IHW+uVq9hsPLAYGaK1rTGxXDvXE47lM/igBi9Yse+gywlpexBripveMXhDX/QsGTLt4z2tvFjPMHWtMHnb/BgiOuHAfrWHHF7DyKeMNatjjxhtZ5Vqu2QQpPxuBv3cZlORCi1ZGF8q+46Hz0LpfFK6L5LXw2U1Gt80b36m9FrnsEaOJYfIyiBhW/X77lsOCCUbb8rj/NqyM+RnGgLDcVGPStpqetzEVZMH7VxoXju9bbVQ2bJWfCXu+hYIMawWmwFqZKaj6dnnRuY8P6miEe+vulb66GT21bK0AlOYbnzLTd5z9OrHfaAYF4xNFhxgIjTb+/kKjbT+/Shoc6kopd+AAMApIBRKACVrrPeftFwB8B3gBs5prqG86dJJpczcT4OPBp/cOonvIRWzTzdoHc66AiMuNf86m/jG0NqePwjtDIaQ33LP83PDNOWxcBzi0DjpeCmP/DW0jaz6eqdSo2e3+ygjF8kLjn/baf0HPq+1f/vwMox3dt7XR7OJlw0XyskJ4d5hR1pm/GG3w5zuZbFx3CO4KU1eCp48dypoJn441RvxOXGB8mrmYyktg7vVGt9p7lkNY/8Z9vtJ84/eYk2x8P5l09qsk9+x+yt1owrSFqVJXXP/2ZwM81Po9sINd/iftEeqDgWe11qOtt/8MoLV+6bz93gR+BJ4AHm+Oof5DYgazvthGp2BfPp06kA4Xs4ZuKoMPRhofp2f+WvNIUGeyc5HRr3vk00ZPGrPJmDdm7Yvg5gGjnoW4qXW/FlBWZFx4W/eyUZvqe4vR+8S/rX3KbTYZPV2ObzUujIb0sv2xqVuMayJ9xxuzY55T7kL4YBTkHzcujNqzB0hBtlHmnGSje2r3K+137Jqc6X21e7FxDaX32IvzvNWVpSjnbMCfOmy8wdrCO9Aa4v0goH2jFbG2ULdlnHoYcKzS7VRg0HlP0h/oqLX+Tillh+4TzmfR5mPM/monUeEt+WTKAFr5XeSr8WtfgIxdxj+jqwQ6QNStRnvpupeMi4i/v2d8rI28Fq59DYLC6ndcL1/oe7PRzr7hDWO2yeTVMPpFiJ7Q8BrVTy8b3QdvfLdugQ5Gj6Er/gTrXjQu9EbdYmzX2mieydpjdPm0d5c+/7ZGs8+n44x+4bd/3jifYM637mUj0K98xrGBDsbr7tfa+Oo0qPb9m6AGX71TSrkBrwN/tGHf6UqpzUqpzdnZrrMk2ns/JfPk4p0M7d6G+dMGXfxAT/kFfvk/6H+3EVKuRCm4/nXwbwffPmD07Ll1rvHmVd9Ar8zD25gS4f4N0Kan0c3vsxuN5p36SlptvEnE3mn0yqiPYX+E8AHGnPVn1oX9fQ7s+hJG/qXxatF+rWHyUuONaOEk4w21NhaLMWPnoZ+Mnit1Gbm680vjDTDmTrjssfqXW1RocPOLUioISAYKrA9pD+QAY2tqgnGF5hetNS+t2Mec9Ye4vl8or98Wc/F6uZxRkmu0O7t5GMHUVPpl21vaVtj3HQyZZVzobAwWizHMftXfjZ4UI56CSx8AdxsnXjOVQcZOY7k//3YwbbXxiaC+TiYb7eth/Y2yzL0Buo8y3tAau+tp8SnjAm/Gbrj1E7jk+nObJU4mWduire3SlS88Kjdo19cYxNZ5KHQeUvV4g6ObjHb08IFw1zfNqq95Q9ijTd0D40LplUAaxoXSiVrrxGr2X0czaFM3mS3M/noXi7ekcvfgzjx7Qx/c3Orxkb34tDGasH0UDHmo7l3tvp5h1N6mrmx6A0icVW6aMVPlgRVGG+nY/1zYU6G8BLISz+2qlrXH6Lnh5W+0o7ft2fCybJlrTG3s7m1caJu+ruqLp42h+LQxl0/6dqNLYOVpBZS70e3wTLfAM99RcGSjMetmaoIxHxBASB+ju2Rn61d5odHTxScIpq2q+yCzZsxeXRqvBd7E6NL4kdb6BaXUc8BmrfXS8/Zdh4uHekm5mVnzt7JqbxaPXtWDR67sUb950YtyjNpQ+g5AG7Wbsf+2fRDJ7q9h8T1wxWwY8ee6P7+ontZGH/flT0LRSRj8oNFTpnJfY4t1HhKflkbon+nt0GmIMcmZvcqx8E6j1869P0L7vvY5rq1K8mD1c6DN1j7d1vBu1bn2Coip1PiEdWSD0UR4bNPZGr2nn/H4aauhTffGPw8XIoOP7Cy3uJz75m4m4UgOz43tw12Du9TvQEU5Rhey7P3GFX+LyRgWXZAJg2Ya7aY1dYHLTTPmJm/d3RikcTEH0zQnxafgh6eNKYnh3L7GZ7qrtezUuN1HzSZjVsemNDisPszlxpvikQ2QtsVo2uo8xNGlcjoS6naSdrqYZTuOs+D3o6SdLub122K4IbpD/Q5WeMLoYXDioNE+2sM6cVVJrtGeu/lDIyiuf6PqSa0sFuNiXmqC0Y7eulv9T0zY5mQyePjYra+xEPVljy6NzVZ2finLd6WzdMdxthwx2hNjOrbkhZuiGNq9nrWmgixjpOSpw8Ygj8oLOvsEGT09om412lE/Hw/9bjfmIa88UdOmd+HwT8ZweAn0i0N+z8JJSKifJ7e4nJWJGSzbcZxfkk5g0dCrfQBPjI7khn4d6NS6Ab0Z8jONHgynjxojPqtbWajzYJjxszH39oY3jFVsxrwM/W4zFoRe9azRT7t/Pef7EEK4LGl+wejJsmJ3Bkt3HOen/dmUmS10CvZlbHQHxsZ0oGc7Oyw9l5duBHrecZi0yPY5yzP3GLX21ATodqUx9Lwwyxg1aq/Rj0IIpwg42LcAABqkSURBVCHNLzZ45ft9vP/zYdoFenPX4M7cEN2B6PCg+vVoqUpumtEftyDLGAnYebDtj23X2+iumPAhrP67MSHRxEUS6EKIKjX7UD+WU8QnG1O4uX8Yr94SjXt9+prX5PQxI9ALTxoDLDoOrPsx3Nxh0HToda1xcbXbCPuWUQjhMpp9qL+6cj/uboonR/eyf6CfOmIEenEu3P1t3VcBOl9QeN2mIxVCNDtNYJkbx9mVmsvSHce597II2gfZYfrSyrL3wyfXGYM3Jtsh0IUQwgbNNtSNeVv2EuznxYwr7NhdzVQGP/3TmEe7vMiYHKlDrP2OL4QQNWi2zS8/HchmY/JJnrmhN4E+dhqNeSwBlj5kDCHvczNc8wr4h9jn2EIIYQPXDvWjm6A0zxg4EtSpYrY9s0Xz8op9dAr2ZdIgO8xJXZoPq583pkYN7AATFhgr2QshxEXmuqFemm9cpDSXGbfdPI11LoO7kWQKITbbgxtGXI5XUYax+EJ9uy9WrGCfBgOtK/T4BNrvPIQQog5cN9RTNhiBfs2rxpzW1nUILSeT6JK1mpc8y2DDh7ABY8a4Nt2hfT/rZE0x0K5PzSuMF2TD938y1rts2wvu/aF+3RWFEMKOXDfUk1aDpy/ETT5ndfn31iXzz6N7WDyxM3H+OWcn+8/aayzCcGY2PuVuhHXFbHzRxpznXn7nrmA//CljxRaZ4F8I0QS4bqgnr4Euw84J9FOFZby9LokRvdoT16+fsbHr8LOP0dpYOix9h7EwQPoOSFoFO+Zbd1DGgrL56bavYC+EEBeRa4b6qRRjia2B08/Z/N+1SRSWmvjTmGoWAlYKWnY0vi65/uz2vPSzQZ+1ByKugLh7Gn9JMSGEqCPXDPXkNcb3StPaHssp4tNfU7glLpzI9nWcoCsw1PiKHGO/MgohRCNwzapm8hoI6ghtelRseu0HYzqAx0bZYd1IIYRoolwv1M0mOLTemPTK2k1xV2ou324/ztShEYQG1dCjRQghnJzrhXraFijNrWh6OTMdQCtfT+4fLqvXCCFcm+uFevIaUG7GxUzOTgfw8JU97DcdgBBCNFGuGeod+oNvsP2nAxBCiCbOtUK9+BSkbYbuVwLwzbY09mXk88ToSLw8XOtUhRCiKq6VdIfXg7ZUtKf/3+oD9AsP4rqoUAcXTAghLg7XCvWk1eAdCGFxFJWZOJZTzJi+7XGz94pGQgjRRLlOqGsNyWsh4nJw9yQjtwSA9oF2XtFICCGaMNcJ9ZPJkHu0ouklI09CXQjR/LhOqCevNr5bQz3TGurt7L32qBBCNGEuFOprILirsRAGkJFbCkA7qakLIZoR1wh1Uxkc/vmcCbwy80rw9/bA39s15ywTQoiq2BTqSqkxSqn9SqkkpdTsKu6/Xym1Sym1XSm1QSnV2/5FrcGxTVBeeEGotwv0ruFBQgjhemoNdaWUO/AWcA3QG5hQRWjP11pHaa1jgH8Cr9u9pDVJXgNuHsaiGFYZeSW0l/Z0IUQzY0tNfSCQpLU+pLUuAxYA4yrvoLXOq3TTD9D2K6INktdA+MBzFnzOzC2R9nQhRLNjS6iHAccq3U61bjuHUupBpVQyRk39YfsUzwaFJ4xVibqfbXqxWDRZ+aXSnVEI0ezY7UKp1votrXU34E/AX6vaRyk1XSm1WSm1OTs72z5PfGgdoM9pTz9RWIrJoqX5RQjR7NgS6mlAx0q3w63bqrMAuLGqO7TWc7TW8Vrr+LZt29peypokrYYWrSA0pmJTVp7RnTEkQEJdCNG82BLqCUAPpVSEUsoLuANYWnkHpVSPSjevAw7ar4g10NpoT+86AtzcKzZXTBEgNXUhRDNTaydurbVJKTULWAm4Ax9prROVUs8Bm7XWS4FZSqmrgHLgFDC5MQtdIWsvFGSc0/QCMkWAEKL5smlkjtZ6ObD8vG1/q/TzI3Yul23OmxrgjMy8EtwUtPH3ckChhBDCcZx7RGnyGmjbC4LO7YyTkVtC2wBvPNyd+/SEEKKunDf1yovhyMYLaulgHXgkTS9CiGbIeUP9yEYwlVQZ6ll5pYRIqAshmiHnDfXkNeDuBZ2HXnCX1NSFEM2Vc4d6p8Hg5XvO5pJyM7nF5dKdUQjRLDlnqOelQ9Ye6H7lBXed6aMu874IIZoj5wz1Q2uN79VcJAXpoy6EaJ6cM9STVoNfCIT0ueCuM8vYtQ+SudSFEM2P84W6xWLU1LuNBLcLi38m1KX3ixCiOXK+UM/YCUUnq2x6AWNtUl8vdwJkGTshRDPkfKFeMTXAiCrvzrR2Z1RKXcRCCSFE0+B81dnoCRDcFfxDqrw7I09WPBJCNF/OV1MP7AB9bqr27oxcWZtUCNF8OV+o18BYxk5q6kKI5sulQv1UURnlZk27QOnOKIRonlwq1GXgkRCiuXOpUD/TR72dtKkLIZoplwr1jFxjwWmpqQshmivXCvW8EpSCtgHSpi6EaJ5cKtQzc0to4++NpyxjJ4Roplwq/TLzS6TnixCiWXOpUM/IlRWPhBDNm0uFeqZMESCEaOZcJtRLys2cKiqXmroQollzmVDPyjO6M0ofdSFEc+YyoS6jSYUQwoVCvWI0qYS6EKIZc7lQl5q6EKI5c5lQz8gtwcfTjcAWzrfuhxBC2IvrhLosYyeEELaFulJqjFJqv1IqSSk1u4r7/6CU2qOU2qmUWq2U6mz/otZM+qgLIYQNoa6UcgfeAq4BegMTlFK9z9ttGxCvte4HLAb+ae+C1iYjT5axE0IIW2rqA4EkrfUhrXUZsAAYV3kHrfVarXWR9eZvQLh9i1kzrTWZeaVSUxdCNHu2hHoYcKzS7VTrturcC6xoSKHq6nRROWUmi4S6EKLZs2tXEaXUnUA8cEU1908HpgN06tTJbs8rA4+EEMJgS009DehY6Xa4dds5lFJXAX8BxmqtS6s6kNZ6jtY6Xmsd37Zt2/qUt0oVoR4k0+4KIZo3W0I9AeihlIpQSnkBdwBLK++glIoF3sMI9Cz7F7NmmbkymlQIIcCGUNdam4BZwEpgL7BIa52olHpOKTXWuturgD/wpVJqu1JqaTWHaxRnauohARLqQojmzaY2da31cmD5edv+Vunnq+xcrjrJzCultZ8XXh4uM5ZKCCHqxSVSUAYeCSGEwSVCPSNXBh4JIQS4SKhLTV0IIQxOH+qlJjMnC8ukj7oQQuACoX5mGTvpoy6EEK4Q6vnW7oxSUxdCCOcP9Yxca01dQl0IIVwg1GXeFyGEqOD0oZ6ZV4KXhxstfT0dXRQhhHA4pw/1jFxZxk4IIc5w/lC3rk0qhBDCBUI9K6+EkEDpziiEEODkoa61lpq6EEJU4tShnldsoqTcIvO+CCGElVOH+pnujDLvixBCGFwi1KWmLoQQBqcO9TPL2EmbuhBCGJw71K019bYB0vtFCCHAxuXsmqqMvBJa+Xri4+nu6KKIZqq8vJzU1FRKSkocXRThYnx8fAgPD8fTs26j5Z061GVxDOFoqampBAQE0KVLFxnVLOxGa83JkydJTU0lIiKiTo916uaXjDxZxk44VklJCa1bt5ZAF3allKJ169b1+gTo3KGeWyoXSYXDSaCLxlDfvyunDfVys4WThaXS/CKatZMnTxITE0NMTAzt27cnLCys4nZZWVmNj928eTMPP/xwrc8xZMgQexW3Tl588UWHPK+zc9o29ez8UrSWgUeieWvdujXbt28H4Nlnn8Xf35/HH3+84n6TyYSHR9X/5vHx8cTHx9f6HBs3brRPYevoxRdf5KmnnnLIc59R0++vqXLamvrZgUfSnVGIyqZMmcL999/PoEGDePLJJ/n9998ZPHgwsbGxDBkyhP379wOwbt06rr/+esB4Q5g6dSrDhw+na9eu/Pvf/644nr+/f8X+w4cP55ZbbqFXr15MmjQJrTUAy5cvp1evXsTFxfHwww9XHLeyxMREBg4cSExMDP369ePgwYMAfP755xXbZ8yYgdlsZvbs2RQXFxMTE8OkSZMuONbMmTOJj4+nT58+PPPMMxXbExISGDJkCNHR0QwcOJD8/HzMZjOPP/44ffv2pV+/fvznP/8BoEuXLpw4cQIwPrUMHz684ndx1113MXToUO666y5SUlIYNmwY/fv3p3///ue8yb3yyitERUURHR3N7NmzSU5Opn///hX3Hzx48JzbF4NzvQVVcmbgkdTURVPx92WJ7DmeZ9dj9u4QyDM39Knz41JTU9m4cSPu7u7k5eXx888/4+HhwapVq3jqqaf46quvLnjMvn37WLt2Lfn5+URGRjJz5swLutNt27aNxMREOnTowNChQ/nll1+Ij49nxowZrF+/noiICCZMmFBlmd59910eeeQRJk2aRFlZGWazmb1797Jw4UJ++eUXPD09eeCBB5g3bx4vv/wy//3vfys+hZzvhRdeIDg4GLPZzJVXXsnOnTvp1asXt99+OwsXLmTAgAHk5eXRokUL5syZQ0pKCtu3b8fDw4OcnJxaf3979uxhw4YNtGjRgqKiIn788Ud8fHw4ePAgEyZMYPPmzaxYsYJvv/2WTZs24evrS05ODsHBwQQFBbF9+3ZiYmL4+OOPueeee2x4xezHaUNdlrETonq33nor7u7G+I3c3FwmT57MwYMHUUpRXl5e5WOuu+46vL298fb2JiQkhMzMTMLDw8/ZZ+DAgRXbYmJiSElJwd/fn65du1Z0vZswYQJz5sy54PiDBw/mhRdeIDU1lZtvvpkePXqwevVqtmzZwoABAwAoLi4mJCSk1vNbtGgRc+bMwWQykZ6ezp49e1BKERoaWnGswMBAAFatWsX9999f0YwSHBxc6/HHjh1LixYtAGMswqxZs9i+fTvu7u4cOHCg4rj33HMPvr6+5xx32rRpfPzxx7z++ussXLiQ33//vdbnsyenDnUvdzeC/bwcXRQhAOpVo24sfn5+FT8//fTTjBgxgm+++YaUlJSKZobzeXufbcp0d3fHZDLVa5/qTJw4kUGDBvHdd99x7bXX8t5776G1ZvLkybz00ks2H+fw4cO89tprJCQk0KpVK6ZMmVKvrn8eHh5YLBaACx5f+ff3xhtv0K5dO3bs2IHFYsHHp+aK5Pjx4/n73//OyJEjiYuLo3Xr1nUuW0M4bZt6Zq6xOIZ0JxOiZrm5uYSFhQHwySef2P34kZGRHDp0iJSUFAAWLlxY5X6HDh2ia9euPPzww4wbN46dO3dy5ZVXsnjxYrKysgDIycnhyJEjAHh6elb5qSIvLw8/Pz+CgoLIzMxkxYoVFeVIT08nISEBgPz8fEwmE6NGjeK9996reAM60/zSpUsXtmzZAlBlc9QZubm5hIaG4ubmxmeffYbZbAZg1KhRfPzxxxQVFZ1zXB8fH0aPHs3MmTMvetMLOHOo50l3RiFs8eSTT/LnP/+Z2NjYOtWsbdWiRQvefvttxowZQ1xcHAEBAQQFBV2w36JFi+jbty8xMTHs3r2bu+++m969e/OPf/yDq6++mn79+jFq1CjS09MBmD59Ov369bvgQml0dDSxsbH06tWLiRMnMnToUAC8vLxYuHAhDz30ENHR0YwaNYqSkhKmTZtGp06d6NevH9HR0cyfPx+AZ555hkceeYT4+PiKpqqqPPDAA8ydO5fo6Gj27dtXUYsfM2YMY8eOJT4+npiYGF577bWKx0yaNAk3Nzeuvvrqhv1y60GduXpd405KjQH+D3AHPtBav3ze/ZcDbwL9gDu01otrO2Z8fLzevHlzvQoNMPK1dVwSGshbky7ulWUhKtu7dy+XXHKJo4vhcAUFBfj7+6O15sEHH6RHjx489thjji6Ww7z22mvk5uby/PPPN+g4Vf19KaW2aK2r7Ytaa5u6UsodeAsYBaQCCUqppVrrPZV2OwpMAR6/8Aj2d2YZu+GRtV9QEUI0vvfff5+5c+dSVlZGbGwsM2bMcHSRHOamm24iOTmZNWvWOOT5bblQOhBI0lofAlBKLQDGARWhrrVOsd5naYQyXiC/1ERRmVn6qAvRRDz22GPNumZe2TfffOPQ57elTT0MOFbpdqp1m8NIH3UhhKjaRb1QqpSarpTarJTanJ2dXe/jSB91IYSomi2hngZ0rHQ73LqtzrTWc7TW8Vrr+LZt29bnEIDR8wWkpi6EEOezJdQTgB5KqQillBdwB7C0cYtVs0xZcFoIIapUa6hrrU3ALGAlsBdYpLVOVEo9p5QaC6CUGqCUSgVuBd5TSiU2ZqEzcksIaiHL2AkxYsQIVq5cec62N998k5kzZ1b7mOHDh3OmO/G1117L6dOnL9jn2WefPaffdVWWLFnCnj1nO8H97W9/Y9WqVXUpvl3IFL3nsqlNXWu9XGvdU2vdTWv9gnXb37TWS60/J2itw7XWflrr1lrrRh0vnZFXIu3pQmDMs7JgwYJzti1YsKDaSbXOt3z5clq2bFmv5z4/1J977jmuuuqqeh2rIZpCqDfGoK76csoRpZl5JbSTphchuOWWW/juu+8qFsRISUnh+PHjDBs2rNrpaSurPP3sCy+8QM+ePbnssssqpucFow/6gAEDiI6OZvz48RQVFbFx40aWLl3KE088QUxMDMnJyUyZMoXFi41xh6tXryY2NpaoqCimTp1KaWlpxfM988wz9O/fn6ioKPbt23dBmWSK3oZxygm9MnJL6NU+wNHFEOJcK2ZDxi77HrN9FFzzcrV3BwcHM3DgQFasWMG4ceNYsGABt912G0qpKqen7devX5XH2bJlCwsWLGD79u2YTCb69+9PXFwcADfffDP33XcfAH/961/58MMPeeihhxg7dizXX389t9xyyznHKikpYcqUKaxevZqePXty991388477/Doo48C0KZNG7Zu3crbb7/Na6+9xgcffHDO42WK3oZxupq6yWzhRIHM+yLEGZWbYCo3vSxatIj+/fsTGxtLYmLiOU0l5/v555+56aab8PX1JTAwkLFjx1bct3v3boYNG0ZUVBTz5s0jMbHmS2b79+8nIiKCnj17AjB58mTWr19fcf/NN98MQFxcXMUkYJUNHjyYF198kVdeeYUjR47QokWLc6bojYmJYfXq1Rw6dKjW301Vv4P9+/dfMEXvmbnmZ8yY0aApeu+77z6ioqK49dZbK37ftU3RazabWbhwIRMnTqz1+WzhdDX1EwVlWGQZO9EU1VCjbkzjxo3jscceY+vWrRQVFREXF2e36WnBWElpyZIlREdH88knn7Bu3boGlffM9L3VTd0rU/Q2jNPV1GXgkRDn8vf3Z8SIEUydOrWill7d9LTVufzyy1myZAnFxcXk5+ezbNmyivvy8/MJDQ2lvLycefPmVWwPCAggPz//gmNFRkaSkpJCUlISAJ999hlXXHGFzecjU/Q2jPOFeq70URfifBMmTGDHjh0VoV7d9LTV6d+/P7fffjvR0dFcc801FU0TAM8//zyDBg1i6NCh9OrVq2L7HXfcwauvvkpsbCzJyckV2318fPj444+59dZbiYqKws3Njfvvv9/mc5EpehvGpql3G0N9p96duzGFZ5YmkvCXq2gbIBN6CceSqXdFQ9Q2RW+jTL3b1LQP8mFU73a0lmXshBBOrLGm6HW6UB/dpz2j+7R3dDGEEKJBGmuKXqdrUxdCCFE9CXUhGshR16WEa6vv35WEuhAN4OPjw8mTJyXYhV1prTl58mStfd2r4nRt6kI0JeHh4aSmptKQRV+EqIqPjw/h4eF1fpyEuhAN4OnpSUREhKOLIUQFaX4RQggXIqEuhBAuREJdCCFciMOmCVBKZQNH6vnwNsAJOxanKXC1c3K18wHXOydXOx9wvXOq6nw6a63bVvcAh4V6QyilNtc094EzcrVzcrXzAdc7J1c7H3C9c6rP+UjzixBCuBAJdSGEcCHOGupzHF2ARuBq5+Rq5wOud06udj7geudU5/NxyjZ1IYQQVXPWmroQQogqOF2oK6XGKKX2K6WSlFKzHV2ehlJKpSildimltiul6r4UVBOglPpIKZWllNpdaVuwUupHpdRB6/dWjixjXVRzPs8qpdKsr9N2pdS1jixjXSmlOiql1iql9iilEpVSj1i3O+XrVMP5OO3rpJTyUUr9rpTaYT2nv1u3RyilNlkzb6FSqsYVgpyq+UUp5Q4cAEYBqUACMEFrvcehBWsApVQKEK+1dtq+tUqpy4EC4FOtdV/rtn8COVrrl61vvq201n9yZDltVc35PAsUaK1fq+mxTZVSKhQI1VpvVUoFAFuAG4EpOOHrVMP53IaTvk5KKQX4aa0LlFKewAbgEeAPwNda6wVKqXeBHVrrd6o7jrPV1AcCSVrrQ1rrMmABMM7BZWr2tNbrgZzzNo8D5lp/novxD+cUqjkfp6a1Ttdab7X+nA/sBcJw0tephvNxWtpQYL3paf3SwEhgsXV7ra+Rs4V6GHCs0u1UnPyFxHjRflBKbVFKTXd0YeyondY63fpzBtDOkYWxk1lKqZ3W5hmnaKaoilKqCxALbMIFXqfzzgec+HVSSrkrpbYDWcCPQDJwWmttsu5Sa+Y5W6i7osu01v2Ba4AHrR/9XYo22vicp52vau8A3YAYIB34l2OLUz9KKX/gK+BRrXVe5fuc8XWq4nyc+nXSWpu11jFAOEbLRK+6HsPZQj0N6Fjpdrh1m9PSWqdZv2cB32C8kK4g09rueab9M8vB5WkQrXWm9R/OAryPE75O1nbar4B5WuuvrZud9nWq6nxc4XUC0FqfBtYCg4GWSqkza1/UmnnOFuoJQA/r1WAv4A5gqYPLVG9KKT/rRR6UUn7A1cDumh/lNJYCk60/Twa+dWBZGuxM8FndhJO9TtaLcB8Ce7XWr1e6yylfp+rOx5lfJ6VUW6VUS+vPLTA6hOzFCPdbrLvV+ho5Ve8XAGsXpTcBd+AjrfULDi5SvSmlumLUzsFYhWq+M56PUuoLYDjGjHKZwDPAEmAR0AljNs7btNZOcfGxmvMZjvGRXgMpwIxKbdFNnlLqMuBnYBdgsW5+CqMd2ulepxrOZwJO+joppfphXAh1x6hwL9JaP2fNiQVAMLANuFNrXVrtcZwt1IUQQlTP2ZpfhBBC1EBCXQghXIiEuhBCuBAJdSGEcCES6kII4UIk1IUQwoVIqAshhAuRUBdCCBfy/3otphM7roGxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY26_bOplkiW",
        "colab_type": "text"
      },
      "source": [
        "Since training accuracy is only 60% it seems the model can't fully utilize training data. Changing BATCH_SIZE to 8 does not improve the results while EPOCHs stay the same.\n",
        "\n",
        "Activation function could also be optimized. By creating a custom made activation function we could shift and optimize the classification treshold of 50% (default value of sigmoid activation). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb8tvqruaIJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_inp, batch_size = BATCH_SIZE) # default batch size for predictiong is 32. See Keras documentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNLiSNOrj3f_",
        "colab_type": "code",
        "outputId": "c8249955-b108-4193-abf3-3335f115fb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.02643323 0.06258249 0.00959128 ... 0.00829169 0.00296825 0.00265291]\n",
            " [0.00284943 0.08777699 0.00139424 ... 0.00658411 0.02069277 0.04387188]\n",
            " [0.03087386 0.05152556 0.01001698 ... 0.0149112  0.00850144 0.00210509]\n",
            " ...\n",
            " [0.00288862 0.02813721 0.00924307 ... 0.00511223 0.00443214 0.00202966]\n",
            " [0.00386298 0.00596645 0.00403318 ... 0.00461656 0.00237945 0.00555652]\n",
            " [0.00699678 0.0178611  0.00847417 ... 0.00392848 0.00551081 0.00883442]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X7dkit5l-ar",
        "colab_type": "code",
        "outputId": "82dcbf00-d025-4024-ab78-57d260534083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(sum(predictions[0])) # since we do multiclass, sum of probabilities is over 1"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0998137295246124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q6hvoMzkRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8cdc49c9-c103-470d-890b-89469ede01e2"
      },
      "source": [
        "# https://datascience.stackexchange.com/questions/25752/how-does-keras-calculate-accuracy-for-multi-label-classification\n",
        "# TRY THIS OUT!\n",
        "\n",
        "# score, acc = model.evaluate(test_inp, true_labels,\n",
        "#                             batch_size=BATCH_SIZE)\n",
        "# score"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 48s 32ms/sample - loss: 0.0783 - categorical_accuracy: 0.4553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yauRbUmrRwQ",
        "colab_type": "code",
        "outputId": "c7b9eb37-dcf1-471d-c944-293e6a8a6583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "preds = (predictions > 0.50).astype(np.uint8) # set the limit for reasonable/acceptable probability for belonging to a class\n",
        "print(\"Predictions are binary sequences:\", preds[1]) \n",
        "\n",
        "predicted_labels = mlb.inverse_transform(preds) # inverse transform gives the registers\n",
        "print(\"Inverse transform gives the predicted register name(s):\" ,predicted_labels[0])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions are binary sequences: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0]\n",
            "Inverse transform gives the predicted register name(s): ('HI', 'IN')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l49oGfGjlys",
        "colab_type": "code",
        "outputId": "8263333f-988a-4116-9b38-ce7db3e2b36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification accuracy: \", round(accuracy_score(true_labels, preds)*100,1), \"percent\") # for accuracy score we use here whole binary sequence"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  49.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJzHfiBuKTee",
        "colab_type": "text"
      },
      "source": [
        "What are the measure for accuracy of multilabel data? https://stats.stackexchange.com/questions/12702/what-are-the-measure-for-accuracy-of-multilabel-data\n",
        "\n",
        "\"When considering the multi label use case, you should decide how to extend accuracy to this case. The method choose in hamming loss was to give each label equal weight. One could use other methods (e.g., taking the maximum).\n",
        "\n",
        "Since hamming loss is designed for multi class while Precision, Recall, F1-Measure are designed for the binary class, it is better to compare the last one to Accuracy. In general, there is no magical metric that is the best for every problem. In every problem you have different needs, and you should optimize for them.\" https://stats.stackexchange.com/questions/336820/what-is-a-hamming-loss-will-we-consider-it-for-an-imbalanced-binary-classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdM5H0ac-bz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true_labels.iloc[0] == preds[0] # binäärit vastaavat toisiaan!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKoLjCkSwcPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is NOT CORRECT!\n",
        "# for register in mlb.classes_:\n",
        "#   idx = 0\n",
        "#   # extract predictions for register from result matrix\n",
        "#   column = []\n",
        "#   for row in preds:\n",
        "#     column.append(row[idx])\n",
        "#   success = sum(true_labels[register] == column) # for how many input the prediction is correct? NONONOOOOO!\n",
        "#   acc = round(success/len(true_labels)*100, 1)\n",
        "#   print('For register {}'.format(register), 'test accuracy is', acc)\n",
        "#   idx = idx+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1bZmgfNnWlm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 3.2: Model comparison\n",
        "Compare the results of these two classifiers. Do the two models predict in the same way? Analyze the predictions in terms of label-specific differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rcigcDRKKCe",
        "colab_type": "text"
      },
      "source": [
        "as mentioned here, the accuracy is ambiguous in the multiple-label case.\n",
        "\n",
        "The HL thus presents one clear single-performance-value for multiple-label case in contrast to the precision/recall/f1 that can be evaluated only for independent binary classifiers for each label."
      ]
    }
  ]
}