{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Text_Classification_task_Hyperp_opt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PfqG5963ZhDa",
        "-7j5rqbNzm3O",
        "DTcr2XZ8zg1H",
        "V3HXbVzAF6UH",
        "9vn7FWLFGeJQ",
        "zIycPktw_m39",
        "tPskSfawiO5I",
        "NYSi-7hwqqTs",
        "i-HcrqoxqxiI",
        "Y1bZmgfNnWlm"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/DL_HLT_2020_groupwork/blob/master/Text_Classification_task_Hanna_4_MS_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VGkCcicEVPSF"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "This project is about text classification. You will develop a text classification system that identifies different kinds of online texts, such as news, blogs and opinionated texts. We will refer to these text categories as registers. If you want to learn more about online registers and their automatic identification, you can read, e.g., our paper [Toward Multilingual Identification of Online Registers] (https://www.aclweb.org/anthology/W19-6130/).\n",
        "\n",
        "# Data and register labels\n",
        "The data for this project consist of ~7500 documents with manual annotations on their register. You can download it from http://dl.turkunlp.org/TKO_8965-projects/classification/ . The documents are based on a (almost) random sample of the Finnish Internet. The registers are identified using a relatively detailed, hierarchical taxonomy. The taxonomy consists of 8 main categories that are divided into a large number of subregisters. The taxonomy is described at the end of this page. The table includes also the abbreviations that are used in the data.\n",
        "\n",
        "The challenge with online documents is that it is not always easy to identify the specific registers categories of the documents. Furthermore, another issue is that a document may display characteristics of several registers. For instance, a blog post may simultaneously seem like a product review. To deal with these challenges, we have followed the following guidelines:\n",
        "* For each document, the annotators have aimed at marking the specific subregister category. When this is possible, the document has two register labels: the subregister label and the main register label to which the subregister belongs. For instance, a document annotated as a news article would have the label NE for News and the corresponding higher level register label NA for Narrative. \n",
        "* In some cases, the document does not seem to fit any of the subregisters. In this case, the document can be given only one label: the main register label, such as NA for Narrative. \n",
        "* Some documents may display characteristics of several register categories. In this case, the annotator can mark several register labels for one single document. Consequently, the document may have up to four labels. This would be the case case if a document is annotated both as a Personal blog (subregister label PB + corresponding higher level register label NA) and Review (subregister label RV + corresponding higher level register label OP).\n",
        "\n",
        "\n",
        "# Register classes and abbreviations\n",
        "\n",
        "NA Narrative\n",
        "\n",
        "* NE NA    New reports / news blogs\n",
        "* SR NA    Sports reports\n",
        "* PB NA    Personal blog\n",
        "* HA NA    Historical article\n",
        "* FC NA    Fiction\n",
        "* TB NA    Travel blog\n",
        "* CB NA    Community blogs\n",
        "* OA NA    Online article\n",
        "\n",
        "OP  Opinion\n",
        "* OB OP  Personal opinion blogs\n",
        "* RV OP  Reviews\n",
        "* RS OP  Religious blogs/sermons\n",
        "* AV OP  Advice\n",
        "\n",
        "IN Informational description\n",
        "* JD IN  Job description\n",
        "* FA IN  FAQs\n",
        "* DT IN  Description of a thing\n",
        "* IB IN  Information blogs\n",
        "* DP IN  Description of a person\n",
        "* RA IN  Research articles\n",
        "* LT IN  Legal terms / conditions\n",
        "* CM IN  Course materials\n",
        "* EN IN  Encyclopedia articles\n",
        "* RP IN  Report\n",
        "\n",
        "ID Interactive discussion\n",
        "* DF ID  Discussion forums\n",
        "* QA ID  Question-answer forums\n",
        "\n",
        "HI  How-to/instructions\n",
        "* RE HI  Recipes\n",
        "\n",
        "IP IG  Informational persuasion\n",
        "* DS IG  Description with intent to sell\n",
        "* EB IG  News-opinion blogs / editorials\n",
        "\n",
        "Lyrical LY\n",
        "* PO LY  Poems\n",
        "* SL LY  Songs\n",
        "\n",
        "Spoken SP\n",
        "* IT SP Interviews\n",
        "* FS SP Formal speeches\n",
        "\n",
        "Others OS\n",
        "* MT OS Machine-translated / generated texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1_c9eZnWlG",
        "colab_type": "text"
      },
      "source": [
        "# Preparations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1s5CYypylU1",
        "colab_type": "text"
      },
      "source": [
        "Import packages, set up TF version 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsWUzLj5NOG",
        "colab_type": "code",
        "outputId": "82f90d88-3d83-4480-c55c-f734bb007494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get rid of old tf at some point!\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# to run with old tf with which the code was made\n",
        "# The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.\n",
        "# https://colab.research.google.com/notebooks/tensorflow_version.ipynb\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svG58YMGXsNr",
        "colab_type": "code",
        "outputId": "237e4066-2f88-41b0-c406-99ab351a7133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "# import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7bPGqEaPC7",
        "colab_type": "text"
      },
      "source": [
        "For saving the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYdjqCtdaNJI",
        "colab_type": "code",
        "outputId": "80ae360e-627c-49ea-ce18-cd729e1b437a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERs_vfbfabkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%script bash\n",
        "mkdir -p models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIHMK6DxXy6r",
        "colab_type": "text"
      },
      "source": [
        "Download and open data, explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHGSBx5nWlM",
        "colab_type": "code",
        "outputId": "26a1b058-4076-49f3-9f6d-9548e25ce1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "# Download development data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
        "# Download test data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
        "# Download train data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 16:46:12--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4035578 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘fincore-dev.tsv’\n",
            "\n",
            "fincore-dev.tsv     100%[===================>]   3.85M  3.68MB/s    in 1.0s    \n",
            "\n",
            "2020-05-07 16:46:13 (3.68 MB/s) - ‘fincore-dev.tsv’ saved [4035578/4035578]\n",
            "\n",
            "--2020-05-07 16:46:15--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8512687 (8.1M) [application/octet-stream]\n",
            "Saving to: ‘fincore-test.tsv’\n",
            "\n",
            "fincore-test.tsv    100%[===================>]   8.12M  6.67MB/s    in 1.2s    \n",
            "\n",
            "2020-05-07 16:46:16 (6.67 MB/s) - ‘fincore-test.tsv’ saved [8512687/8512687]\n",
            "\n",
            "--2020-05-07 16:46:18--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29379580 (28M) [application/octet-stream]\n",
            "Saving to: ‘fincore-train.tsv’\n",
            "\n",
            "fincore-train.tsv   100%[===================>]  28.02M  13.2MB/s    in 2.1s    \n",
            "\n",
            "2020-05-07 16:46:20 (13.2 MB/s) - ‘fincore-train.tsv’ saved [29379580/29379580]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8TezCY6pR1",
        "colab_type": "text"
      },
      "source": [
        "Data split\n",
        "\n",
        "- Training set: used for training the models\n",
        "- Validation (or model assessment) set: when comparing\n",
        "di\u000berent models trained on training set, select one with lowest\n",
        "error on validation set\n",
        "- Test set: test the \fnal hypothesis on test set to get unbiased\n",
        "error estimate for it\n",
        "\n",
        "After error estimation the \fnal model is often trained on\n",
        "combined training, validation and test set, using best\n",
        "hyperparameters found during model selection\n",
        "- Complication: randomized optimization approaches where\n",
        "di\u000berent runs with same hyperparameters can lead to very\n",
        "di\u000berent quality solutions (e.g. neural network)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KciwyumAEz_b",
        "colab_type": "code",
        "outputId": "8b91e52e-d44a-4a9e-9ec6-9fae499c3b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "train = pd.read_csv('fincore-train.tsv', sep='\\t', header=None)\n",
        "\n",
        "train = train.sample(frac=1, random_state = 4) # suffle the data\n",
        "train.columns = ['label','text']\n",
        "print(\"train data\")\n",
        "print(train.head())\n",
        "print(train.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(train['label'].value_counts())\n",
        "\n",
        "dev = pd.read_csv('fincore-dev.tsv', sep='\\t', header=None)\n",
        "dev.columns = ['label','text']\n",
        "print(\"dev data\")\n",
        "print(dev.head())\n",
        "print(dev.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(dev['label'].value_counts())\n",
        "\n",
        "test = pd.read_csv('fincore-test.tsv', sep='\\t', header=None)\n",
        "test.columns = ['label','text']\n",
        "print(\"test data\")\n",
        "print(test.head())\n",
        "print(test.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(test['label'].value_counts())\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data\n",
            "       label                                               text\n",
            "3982  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "2640  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...\n",
            "119   NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...\n",
            "4916  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...\n",
            "775   MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...\n",
            "(5295, 2)\n",
            "dev data\n",
            "    label                                               text\n",
            "0  OA NA    Luonnonhoito Maaperän siemenpankkia avattiin ...\n",
            "1  DS IG    • Jokainen ripsi on erittäin kevyt ja muodolt...\n",
            "2  DS IG    Mukavuudet Hotel Dila Vain muutaman metrin pä...\n",
            "3  DF ID    Vastaa viestiin Otsikko Viesti ensin omaishoi...\n",
            "4  OA NA    Dinosaur Jr 30.5.2010 Tavastia , Helsinki 198...\n",
            "(756, 2)\n",
            "test data\n",
            "    label                                               text\n",
            "0    HI     Tehkää nollaleimaus . Jos rekisteröinti onnis...\n",
            "1    NA     1 kommenttia : Syyslomallelähtijät kirjoitti ...\n",
            "2  DT IN    Ammattikoulutuksen perustana on ajatus siitä ...\n",
            "3  DP IN    Ulkonäkö : Silveriä voisi kuvata tietyllä tap...\n",
            "4  DT IN    Laulupelimannien puheenjohtajina ovat toimine...\n",
            "(1513, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwxqapN9J7PH",
        "colab_type": "code",
        "outputId": "aa806810-15b9-4938-a2a5-70caeb72fea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Prepare stratified data sets for training, development and testing:\n",
        "# Stratification aims to ensure that all the data sets (train, development and test) have the same distribution of labels. \n",
        "# This minimizes chances that a model has to try to predict labes it has not seen during training.\n",
        "\n",
        "# test-train-split causes error: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
        "# Best solution: More data\n",
        "# Second best solution: If you cannot have another dataset, you will have to play with what you have. Suggestion: remove the sample that has the lonely target. \n",
        "\n",
        "# Join all the data and re-divide it with stratification and to illustrate how skewed the data is!\n",
        "train_e = train\n",
        "train_e['data'] = 'train'\n",
        "\n",
        "dev_e = dev\n",
        "dev_e['data'] = 'dev'\n",
        "\n",
        "test_e = test\n",
        "test_e['data'] = 'test'\n",
        "\n",
        "frames = [train_e, dev_e, test_e]\n",
        "df = pd.concat(frames).reset_index(drop=True) # get rid of the old indexes # this data used for milestone 3\n",
        "\n",
        "# Separating out the target\n",
        "y = df['label'] # pd df\n",
        "# # Separating out the features\n",
        "X = df['text'] # pd df\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "nums = dict(zip(unique, counts))\n",
        "\n",
        "df.head()\n",
        "\n",
        "#pprint(sorted(nums.items(), key = lambda kv:(kv[1], kv[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWSDvMthCZFl",
        "colab_type": "code",
        "outputId": "3c51d45b-6167-483d-c83a-fd2c80c09cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "plt.figure(figsize=(26, 6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.countplot(x=\"label\", hue=\"data\", data=df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f286879b470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1872x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuGhgHP5DJkZ",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the distribution of labels is highly skewed. This might be fixed with data augmentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzVzQOvux5lJ",
        "colab_type": "text"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "Since we now know that 'MT OS ' is the most common label in the data with 957 occurences, we can set a naive baseline prediction: We will predict that an unlabeled new text belongs to this biggest class. In this case our prediction accuracy is the pure share of the biggest class in the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vU1dkUOyhQH",
        "colab_type": "code",
        "outputId": "53ba4644-eec0-4780-f1b7-fb852f2c5656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification baseline: \", round((957/df.shape[0])*100,1), \"percent\") # here the original data without modifications for training of classificators "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification baseline:  12.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_AQXZLHkXz",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.1: Bag-of-words classifier (multi-class)\n",
        "\n",
        "Train a bag-of-words classifier to predict the register categories. In this milestone, the setting is multi-class, so the register label combinations form the classes, e.g. NA_NE and NA_NE_OP_OB. \n",
        "\n",
        "- Evaluate your model and report your results with different hyperparameters\n",
        "- Ideas to try:\n",
        "  - Different activation functions\n",
        "  - Altering the learning rate\n",
        "  - Use different optimizers\n",
        "  - Adjusting the vocabulary size of the embeddings\n",
        "\n",
        "- Activation functions and optimizers supported by Keras can be found here: https://keras.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FGFhSxaVye",
        "colab_type": "text"
      },
      "source": [
        "Bow classifier is only interested in the multiplicity or appearance of words (or to be precise n-garms). Hence we loose the textual context and order of the words (n-grams). This inevitably leads to some information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLA0Alu_y6c2",
        "colab_type": "text"
      },
      "source": [
        "## Data filtering  for milestones 1 and 2\n",
        "\n",
        "Since we can not train the model to predict labels it has not seen during training phase, we will keep only rows in dev and test data which have the labels that appear also in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j80A7RBn0KOY",
        "colab_type": "code",
        "outputId": "408f92d9-3a6b-4198-9c30-de0d2490bc37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "# Optional way to treat data if stratification is bad idea...\n",
        "\n",
        "# Gather features and labels of the data\n",
        "\n",
        "# Separate text and the associated label\n",
        "train_text = train['text']\n",
        "train_labels = train['label']\n",
        "\n",
        "# print(train_text.head())\n",
        "# print(train_labels.head())\n",
        "# print()\n",
        "\n",
        "dev_text = dev['text']\n",
        "dev_labels = dev['label']\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "all_labels = pd.concat(labels)\n",
        "\n",
        "print(all_labels.head(10))\n",
        "print()\n",
        "print(\"Number of unique labels in data: \", len(all_labels.unique()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3982    DS IG \n",
            "2640    RS OP \n",
            "119     NE NA \n",
            "4916    SR NA \n",
            "775     MT OS \n",
            "5264      NA  \n",
            "5040    DP IN \n",
            "3446    DS IG \n",
            "216     DF ID \n",
            "863     DT IN \n",
            "Name: label, dtype: object\n",
            "\n",
            "Number of unique labels in data:  119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8zX0o3bxaA8",
        "colab_type": "code",
        "outputId": "b024ebfb-fb36-437e-8718-dc96e7816cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_labels)))\n",
        "print(\"-development data: \", len(np.unique(dev_labels)))\n",
        "print(\"-test data: \", len(np.unique(test_labels)))\n",
        "print()\n",
        "inter = np.intersect1d(train_labels, dev_labels)\n",
        "inter2 = np.intersect1d(train_labels, test_labels)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  58\n",
            "-test data:  70\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RtQipNUc-7S",
        "colab_type": "code",
        "outputId": "200cb2b6-abdf-44fa-b867-485ed8224e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Filter out labels not found in training data\n",
        "\n",
        "test_ = test[test['label'].isin(train_labels.tolist())]\n",
        "dev_ = dev[dev['label'].isin(train_labels.tolist())] \n",
        "\n",
        "print(test_.shape)\n",
        "print(dev_.shape)\n",
        "\n",
        "dev_text = dev_['text'] # development data for milestones 1 and 2\n",
        "dev_labels = dev_['label']\n",
        "\n",
        "test_text = test_['text'] # test data for milestones 1 and 2\n",
        "test_labels = test_['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "labels = pd.concat(labels)\n",
        "\n",
        "print(\"Number of unique labels in data: \", len(labels.unique()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 3)\n",
            "(750, 3)\n",
            "Number of unique labels in data:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7VZBC6QCWVi",
        "colab_type": "code",
        "outputId": "4e8677d2-fea4-478e-c77f-0fe04a77043c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_labels)))\n",
        "print(\"-development data: \", len(np.unique(dev_labels)))\n",
        "print(\"-test data: \", len(np.unique(test_labels)))\n",
        "print()\n",
        "inter = np.intersect1d(train_labels, dev_labels)\n",
        "inter2 = np.intersect1d(train_labels, test_labels)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  52\n",
            "-test data:  57\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ3qKWoKbeEU",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation, step  1: form feature matrix\n",
        "\n",
        "We will use CountVectorizer / TfidfVectorizer from sklearn package to transform out text data to numerical format with which our classifier is able to deal with. CountVectorizer converts the collection of text documents (our training data) to a matrix of token counts. Since we are only interested in whether a particular word of the vocabulary is in a single document or not, our vectorizer is set on \"binary\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt1fNJ7ovGge",
        "colab_type": "code",
        "outputId": "03cd8a9a-96f6-4aac-d53f-91c6444a19f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 20000, ngram_range = (1,1)) # Grid search fails: OOM when allocating tensor of shape [85000,200]\n",
        "\n",
        "# form feature matrix\n",
        "train_feature_matrix = vectorizer.fit_transform(train_text)\n",
        "dev_feature_matrix = vectorizer.transform(dev_text)\n",
        "test_feature_matrix = vectorizer.transform(test_text)\n",
        "\n",
        "print(\"shape of the training data: \", train_feature_matrix.shape)\n",
        "print(\"shape of the development data: \", dev_feature_matrix.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5295, 20000)\n",
            "shape of the development data:  (750, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymmdQP4aq2E",
        "colab_type": "text"
      },
      "source": [
        "The shape of the feature matrix tells us that we have XXX items (documents) in our training data. The number of unique n-grams exceeds XXXX but we are including only the first XXXX most common of them. Since our CountVectorizer has parameter setting \"ngram_range = 1, 1\" this means we are forming the vector with unigrams, separate words or charachters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTY0uwjd25w",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation, step 2: Label encoding and one hot encoding\n",
        "\n",
        "Next we will encode the labels. This means transforming the textual labels to numeric values, which our model is able to deal with. This step is made with LabelEncoder class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_yBCz62umP",
        "colab_type": "code",
        "outputId": "b8cc60d2-a574-414f-9c0a-d52881612a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "# label encoding \n",
        "\n",
        "label_encoder = LabelEncoder() # Create the instance of LabelEncoder we use to turn class labels into integers\n",
        "\n",
        "train_numbers = label_encoder.fit_transform(train_labels) # encode labels to integers\n",
        "dev_numbers = label_encoder.transform(dev_labels) \n",
        "test_numbers = label_encoder.transform(test_labels) \n",
        "\n",
        "print(\"Inverse transform gives unique labels in each data set: \", label_encoder.inverse_transform(train_numbers))\n",
        "print(\"Sanity checks, do we have as many labels and texts in our data sets?\")\n",
        "print(\"Train data: \", len(train_numbers), len(train_text))\n",
        "print(\"Dev data: \", len(dev_numbers), len(dev_text))\n",
        "print(\"Test data: \", len(test_numbers), len(test_text))\n",
        "\n",
        "# one hot encoding\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_hot=one_hot_encoder.fit_transform(train_numbers.reshape(-1,1))\n",
        "dev_hot=one_hot_encoder.transform(dev_numbers.reshape(-1,1))\n",
        "test_hot=one_hot_encoder.transform(test_numbers.reshape(-1,1))\n",
        "print()\n",
        "print(train_hot.shape)\n",
        "print(dev_hot.shape)\n",
        "print(test_hot.shape)\n",
        "train_hot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inverse transform gives unique labels in each data set:  ['DS IG ' 'RS OP ' 'NE NA ' ... 'DS IG ' 'MT OS ' 'MT OS ']\n",
            "Sanity checks, do we have as many labels and texts in our data sets?\n",
            "Train data:  5295 5295\n",
            "Dev data:  750 750\n",
            "Test data:  1500 1500\n",
            "\n",
            "(5295, 100)\n",
            "(750, 100)\n",
            "(1500, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3RDtU7J2di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LabelEncoderin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode tee koodaus\n",
        "# print(\"Koodaukset: \", list(le.classes_))\n",
        "# print()\n",
        "# le.transform([\"tokyo\", \"tokyo\", \"paris\"]) # käytä koodausta arvojen transformointiin --> numeeriset arvot\n",
        "# num = le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
        "# print(\"Sovitetulla encoderilla tuotetut numeeriset arvot muuttujalistasta: \", num)\n",
        "# print()\n",
        "# print(\"Inverse transform: \", list(le.inverse_transform([2, 2, 1])))\n",
        "# print()\n",
        "# test = le.fit_transform([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit kolmella\", test)\n",
        "\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"helsinki\"])\n",
        "# #print(list(le.classes_))\n",
        "\n",
        "# test = le.fit_transform([\"paris\", \"helsinki\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit neljällä\", test)\n",
        "\n",
        "# print(list(le.inverse_transform([2, 2, 1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9_7r6FeRS5",
        "colab_type": "text"
      },
      "source": [
        "Now the data is prepared for milestone 1.\n",
        "\n",
        "## Cross validation and hyperparameter optimization with Gridsearch\n",
        "\n",
        "Join train and dev data for CV grid search.\n",
        "\n",
        "Approach: Select prominent optimizers and optimize rest of the hyperparams for it. Since other hyperparams affect the optimizer performance it is not a \"fair competition\" to optimize optimizers with all the rest hyperparams set to some values. We have to deal with memory constraints also. This significantly limits the set of hyperparametrs we can optimize at once.\n",
        "\n",
        "Here Adam and SGD are explored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAoJ6ZArns0j",
        "colab_type": "code",
        "outputId": "41058cb0-fcc8-48c0-ec40-966da64018cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "# features \n",
        "frames = [train, dev_]\n",
        "joint = pd.concat(frames)\n",
        "joint_feature_matrix = vectorizer.fit_transform(joint['text'])\n",
        "\n",
        "# labels, one hot encoded\n",
        "print(type(train_hot))\n",
        "print(train_hot.shape)\n",
        "print(dev_hot.shape)\n",
        "joint_labels= np.concatenate((train_hot, dev_hot))\n",
        "print(joint_labels.shape)\n",
        "\n",
        "print(joint_feature_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(5295, 100)\n",
            "(750, 100)\n",
            "(6045, 100)\n",
            "(6045, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWUSL5YoSARM",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer: SGD\n",
        "\n",
        "### Step 1.1 Build and compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cdWVH9iQpVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_count, feature_count = joint_feature_matrix.shape\n",
        "class_count = len(labels.unique())\n",
        "\n",
        "def create_bow_model_SGD(learning_rate=0.01, nodes=200):\n",
        "  inp = Input(shape=(feature_count,))\n",
        "  hidden1 = Dense(nodes, activation=\"relu\")(inp)\n",
        " # hidden2 = Dense(nodes, activation=\"relu\")(hidden1) # multiple hidden layers possible but slow down hyperparameter optimization\n",
        "  dropout = Dropout(0.3)(hidden1)\n",
        "  outp = Dense(class_count, activation='softmax')(dropout)\n",
        "  model = Model(inputs=[inp], outputs=[outp])\n",
        "  optimizer = optimizers.SGD(learning_rate=learning_rate) # also hyperparameter momentum could be optimized\n",
        "  model.compile(optimizer = optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkzqF6nkT98F",
        "colab_type": "code",
        "outputId": "0dc59dca-742d-4d3c-e07b-d4ff3d5ccff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "model = create_bow_model_SGD()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               4000200   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "=================================================================\n",
            "Total params: 4,020,300\n",
            "Trainable params: 4,020,300\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjQl3cVXXota",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.2: GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNXj8TvOQN6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tensorflow.compat.v1.disable_eager_execution()\n",
        "stop_cb = EarlyStopping(monitor = 'accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "# If the error only occurs when you use smaller datasets, you're very likely using datasets small enough to not have a single sample in the validation set. \n",
        "\n",
        "bow_model_SGD = KerasClassifier(build_fn=create_bow_model_SGD, epochs=40, batch_size=32, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvHQGoNB0q9e",
        "colab_type": "code",
        "outputId": "88593b52-8012-4ed5-b62f-f0a8f8468011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = datetime.now()\n",
        "print(\"start time =\", start)\n",
        "\n",
        "# set the hyperparameter: example: np.logspace(-10, 1, num = 7, base = 2)\n",
        "learning_rate = np.logspace(-7, 0, num = 5, base = 2).tolist()\n",
        "params = dict(learning_rate = learning_rate)\n",
        "\n",
        "grid = GridSearchCV(estimator = bow_model_SGD, param_grid = params, n_jobs = 1, cv = 3 , verbose  = 1)  # cv: For integer/None inputs, if the estimator is a classifier and y is \n",
        "                                                                                                        # either binary or multiclass, StratifiedKFold is used.\n",
        "X = joint_feature_matrix.toarray()\n",
        "    \n",
        "grid_result_SGD = grid.fit(X, joint_numbers, callbacks=[stop_cb, mc_cb])\n",
        "\n",
        "end = datetime.now()\n",
        "print(\"end time =\", end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start time = 2020-05-01 10:54:05.028913\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 273us/step - loss: 4.5955 - accuracy: 0.0787\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.5714 - accuracy: 0.1328\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.5463 - accuracy: 0.1340\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 4.5197 - accuracy: 0.1380\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4928 - accuracy: 0.1385\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4656 - accuracy: 0.1355\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.4367 - accuracy: 0.1414\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4078 - accuracy: 0.1422\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.3778 - accuracy: 0.1370\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.3469 - accuracy: 0.1372\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.3147 - accuracy: 0.1375\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.2808 - accuracy: 0.1380\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 4.2448 - accuracy: 0.1367\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.2087 - accuracy: 0.1350\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.1698 - accuracy: 0.1370\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1279 - accuracy: 0.1367\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.0828 - accuracy: 0.1355\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.0411 - accuracy: 0.1370\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.9944 - accuracy: 0.1370\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.9491 - accuracy: 0.1340\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8981 - accuracy: 0.1370\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.8489 - accuracy: 0.1370\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8041 - accuracy: 0.1385\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7584 - accuracy: 0.1330\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7140 - accuracy: 0.1347\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.6775 - accuracy: 0.1328\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 3.6398 - accuracy: 0.1333\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.6073 - accuracy: 0.1290\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00028: early stopping\n",
            "2015/2015 [==============================] - 1s 362us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 280us/step - loss: 4.5940 - accuracy: 0.0794\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5681 - accuracy: 0.1295\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5404 - accuracy: 0.1313\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5117 - accuracy: 0.1278\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4821 - accuracy: 0.1303\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4521 - accuracy: 0.1305\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4210 - accuracy: 0.1238\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.3893 - accuracy: 0.1256\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 4.3556 - accuracy: 0.1223\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.3208 - accuracy: 0.1261\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.2853 - accuracy: 0.1253\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 4.2472 - accuracy: 0.1223\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.2067 - accuracy: 0.1231\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1631 - accuracy: 0.1236\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1222 - accuracy: 0.1223\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.0756 - accuracy: 0.1213\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.0311 - accuracy: 0.1231\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.9801 - accuracy: 0.1191\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.9317 - accuracy: 0.1231\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8820 - accuracy: 0.1213\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8359 - accuracy: 0.1206\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7866 - accuracy: 0.1233\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.7439 - accuracy: 0.1223\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00023: early stopping\n",
            "2015/2015 [==============================] - 1s 370us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 276us/step - loss: 4.5909 - accuracy: 0.1122\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.5637 - accuracy: 0.1330\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.5335 - accuracy: 0.1318\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.5024 - accuracy: 0.1325\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.4696 - accuracy: 0.1323\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.4358 - accuracy: 0.1323\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.4015 - accuracy: 0.1325\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.3664 - accuracy: 0.1323\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 148us/step - loss: 4.3290 - accuracy: 0.1323\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.2906 - accuracy: 0.1323\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 4.2499 - accuracy: 0.1323\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.2083 - accuracy: 0.1323\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 4.1619 - accuracy: 0.1323\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.1156 - accuracy: 0.1323\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.0661 - accuracy: 0.1323\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.0153 - accuracy: 0.1323\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.9642 - accuracy: 0.1325\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.9124 - accuracy: 0.1323\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.8603 - accuracy: 0.1323\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.8083 - accuracy: 0.1323\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7589 - accuracy: 0.1323\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7127 - accuracy: 0.1320\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00022: early stopping\n",
            "2015/2015 [==============================] - 1s 347us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 273us/step - loss: 4.5639 - accuracy: 0.1226\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.4678 - accuracy: 0.1288\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.3597 - accuracy: 0.1283\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.2368 - accuracy: 0.1283\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.0924 - accuracy: 0.1283\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.9304 - accuracy: 0.1283\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7719 - accuracy: 0.1283\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.6367 - accuracy: 0.1283\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.5401 - accuracy: 0.1303\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.4633 - accuracy: 0.1315\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.4147 - accuracy: 0.1419\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3753 - accuracy: 0.1414\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.3407 - accuracy: 0.1449\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3157 - accuracy: 0.1474\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2879 - accuracy: 0.1603\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.2673 - accuracy: 0.1533\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2559 - accuracy: 0.1593\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.2456 - accuracy: 0.1573\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.2311 - accuracy: 0.1633\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.2205 - accuracy: 0.1586\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2107 - accuracy: 0.1618\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1988 - accuracy: 0.1707\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1939 - accuracy: 0.1650\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1850 - accuracy: 0.1677\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1841 - accuracy: 0.1819\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1686 - accuracy: 0.1839\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1664 - accuracy: 0.1789\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1558 - accuracy: 0.1898\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1573 - accuracy: 0.1816\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1473 - accuracy: 0.1792\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.1459 - accuracy: 0.1931\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1362 - accuracy: 0.1903\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.1370 - accuracy: 0.1903\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1245 - accuracy: 0.2030\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1287 - accuracy: 0.2040\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.1166 - accuracy: 0.1965\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1153 - accuracy: 0.2060\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1076 - accuracy: 0.2176\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1028 - accuracy: 0.2082\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.0989 - accuracy: 0.2129\n",
            "2015/2015 [==============================] - 1s 360us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 308us/step - loss: 4.5604 - accuracy: 0.1084\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.4510 - accuracy: 0.1263\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.3284 - accuracy: 0.1194\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 4.1866 - accuracy: 0.1221\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.0256 - accuracy: 0.1226\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.8519 - accuracy: 0.1201\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.6923 - accuracy: 0.1213\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.5733 - accuracy: 0.1258\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.4928 - accuracy: 0.1263\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.4376 - accuracy: 0.1335\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.3943 - accuracy: 0.1337\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.3574 - accuracy: 0.1315\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3316 - accuracy: 0.1370\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3045 - accuracy: 0.1412\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2892 - accuracy: 0.1409\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2701 - accuracy: 0.1352\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2526 - accuracy: 0.1424\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2475 - accuracy: 0.1382\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2353 - accuracy: 0.1504\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2234 - accuracy: 0.1496\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.2227 - accuracy: 0.1514\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.2132 - accuracy: 0.1471\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2006 - accuracy: 0.1558\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1969 - accuracy: 0.1566\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1891 - accuracy: 0.1603\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1820 - accuracy: 0.1553\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.1776 - accuracy: 0.1588\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1722 - accuracy: 0.1670\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1666 - accuracy: 0.1715\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1563 - accuracy: 0.1734\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1546 - accuracy: 0.1710\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1554 - accuracy: 0.1653\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1483 - accuracy: 0.1772\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1442 - accuracy: 0.1787\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1409 - accuracy: 0.1844\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1399 - accuracy: 0.1752\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.1322 - accuracy: 0.1901\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1252 - accuracy: 0.1950\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1229 - accuracy: 0.1886\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.1191 - accuracy: 0.1841\n",
            "2015/2015 [==============================] - 1s 368us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 292us/step - loss: 4.5551 - accuracy: 0.1313\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 4.4396 - accuracy: 0.1476\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 4.3093 - accuracy: 0.1496\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 4.1581 - accuracy: 0.1489\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.9852 - accuracy: 0.1504\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.8041 - accuracy: 0.1367\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.6473 - accuracy: 0.1437\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.5358 - accuracy: 0.1432\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.4634 - accuracy: 0.1432\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.4072 - accuracy: 0.1499\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.3683 - accuracy: 0.1427\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.3325 - accuracy: 0.1459\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.3051 - accuracy: 0.1548\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2868 - accuracy: 0.1533\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2653 - accuracy: 0.1563\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2471 - accuracy: 0.1568\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.2373 - accuracy: 0.1563\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.2265 - accuracy: 0.1536\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.2120 - accuracy: 0.1603\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2049 - accuracy: 0.1620\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1996 - accuracy: 0.1667\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1886 - accuracy: 0.1685\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1835 - accuracy: 0.1727\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1742 - accuracy: 0.1737\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1747 - accuracy: 0.1754\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1642 - accuracy: 0.1754\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1561 - accuracy: 0.1829\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1492 - accuracy: 0.1794\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1410 - accuracy: 0.1908\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.1381 - accuracy: 0.1906\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1369 - accuracy: 0.1888\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1336 - accuracy: 0.1826\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1253 - accuracy: 0.1998\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1213 - accuracy: 0.1913\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1108 - accuracy: 0.2062\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1145 - accuracy: 0.2079\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1050 - accuracy: 0.2060\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0991 - accuracy: 0.2082\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0982 - accuracy: 0.2117\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0924 - accuracy: 0.2072\n",
            "2015/2015 [==============================] - 1s 367us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 300us/step - loss: 4.4479 - accuracy: 0.1251\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 4.0008 - accuracy: 0.1280\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.5616 - accuracy: 0.1290\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.3820 - accuracy: 0.1407\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2938 - accuracy: 0.1538\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2451 - accuracy: 0.1603\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2065 - accuracy: 0.1695\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 3.1815 - accuracy: 0.1697\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1643 - accuracy: 0.1801\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.1492 - accuracy: 0.1968\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.1296 - accuracy: 0.2020\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1086 - accuracy: 0.2154\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.0908 - accuracy: 0.2233\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.0811 - accuracy: 0.2342\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0627 - accuracy: 0.2434\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0432 - accuracy: 0.2462\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0202 - accuracy: 0.2581\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.9936 - accuracy: 0.2749\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.9722 - accuracy: 0.2759\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9437 - accuracy: 0.2859\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9298 - accuracy: 0.2898\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.8986 - accuracy: 0.2975\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.8764 - accuracy: 0.3035\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.8438 - accuracy: 0.3151\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.8265 - accuracy: 0.3176\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.7936 - accuracy: 0.3305\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.7654 - accuracy: 0.3370\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.7404 - accuracy: 0.3395\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.7157 - accuracy: 0.3536\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.6954 - accuracy: 0.3596\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.6755 - accuracy: 0.3556\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.6498 - accuracy: 0.3672\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.6343 - accuracy: 0.3720\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.6053 - accuracy: 0.3846\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5855 - accuracy: 0.3826\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5657 - accuracy: 0.3896\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.5489 - accuracy: 0.3960\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5315 - accuracy: 0.4000\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.5057 - accuracy: 0.4015\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 2.4955 - accuracy: 0.4030\n",
            "2015/2015 [==============================] - 1s 377us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 292us/step - loss: 4.4613 - accuracy: 0.1124\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.0571 - accuracy: 0.1206\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.5967 - accuracy: 0.1275\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.3862 - accuracy: 0.1414\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2978 - accuracy: 0.1397\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2459 - accuracy: 0.1449\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2165 - accuracy: 0.1556\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1922 - accuracy: 0.1692\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1709 - accuracy: 0.1777\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1599 - accuracy: 0.1794\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1414 - accuracy: 0.1864\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1267 - accuracy: 0.1993\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1054 - accuracy: 0.2042\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.0947 - accuracy: 0.2144\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0702 - accuracy: 0.2288\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.0579 - accuracy: 0.2261\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0347 - accuracy: 0.2409\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0195 - accuracy: 0.2548\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9923 - accuracy: 0.2655\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9740 - accuracy: 0.2640\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.9487 - accuracy: 0.2744\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9253 - accuracy: 0.2811\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9001 - accuracy: 0.2960\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.8733 - accuracy: 0.2953\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.8416 - accuracy: 0.3124\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8174 - accuracy: 0.3228\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.7881 - accuracy: 0.3385\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7618 - accuracy: 0.3352\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7404 - accuracy: 0.3407\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.7135 - accuracy: 0.3509\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6874 - accuracy: 0.3573\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.6646 - accuracy: 0.3638\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.6444 - accuracy: 0.3667\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6199 - accuracy: 0.3774\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6014 - accuracy: 0.3787\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5787 - accuracy: 0.3881\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.5562 - accuracy: 0.3933\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.5377 - accuracy: 0.3931\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.5271 - accuracy: 0.3916\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5014 - accuracy: 0.4007\n",
            "2015/2015 [==============================] - 1s 380us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 293us/step - loss: 4.4408 - accuracy: 0.1283\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.9804 - accuracy: 0.1323\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.5475 - accuracy: 0.1350\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.3675 - accuracy: 0.1380\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.2821 - accuracy: 0.1442\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2304 - accuracy: 0.1531\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2043 - accuracy: 0.1650\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.1718 - accuracy: 0.1655\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1617 - accuracy: 0.1749\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1428 - accuracy: 0.1769\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1266 - accuracy: 0.1958\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1162 - accuracy: 0.1861\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.0931 - accuracy: 0.2089\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.0755 - accuracy: 0.2179\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.0602 - accuracy: 0.2283\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0457 - accuracy: 0.2459\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0189 - accuracy: 0.2514\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0054 - accuracy: 0.2605\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9789 - accuracy: 0.2749\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9561 - accuracy: 0.2849\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9364 - accuracy: 0.2868\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9032 - accuracy: 0.3022\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8899 - accuracy: 0.3032\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8568 - accuracy: 0.3141\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8279 - accuracy: 0.3266\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.7980 - accuracy: 0.3310\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.7746 - accuracy: 0.3362\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.7536 - accuracy: 0.3489\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7240 - accuracy: 0.3531\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6980 - accuracy: 0.3615\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6778 - accuracy: 0.3690\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6527 - accuracy: 0.3715\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6297 - accuracy: 0.3801\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6043 - accuracy: 0.3878\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5872 - accuracy: 0.3854\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5666 - accuracy: 0.4010\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5426 - accuracy: 0.4035\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.5224 - accuracy: 0.4037\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.4969 - accuracy: 0.4112\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 2.4872 - accuracy: 0.4164\n",
            "2015/2015 [==============================] - 1s 379us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 305us/step - loss: 3.9474 - accuracy: 0.1238\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2762 - accuracy: 0.1452\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.1772 - accuracy: 0.1638\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1176 - accuracy: 0.1963\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.0618 - accuracy: 0.2266\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9878 - accuracy: 0.2588\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.9086 - accuracy: 0.2826\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.8199 - accuracy: 0.3055\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.7373 - accuracy: 0.3395\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.6550 - accuracy: 0.3571\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.5899 - accuracy: 0.3797\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.5296 - accuracy: 0.3851\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.4699 - accuracy: 0.4047\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.4257 - accuracy: 0.4099\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.3732 - accuracy: 0.4300\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.3298 - accuracy: 0.4372\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.2773 - accuracy: 0.4434\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.2452 - accuracy: 0.4509\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 2.2084 - accuracy: 0.4581\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.1798 - accuracy: 0.4630\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.1373 - accuracy: 0.4759\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.1014 - accuracy: 0.4811\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.0704 - accuracy: 0.4911\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.0372 - accuracy: 0.4968\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 2.0119 - accuracy: 0.5119\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 1.9791 - accuracy: 0.5139\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.9604 - accuracy: 0.5169\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.9210 - accuracy: 0.5320\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.8881 - accuracy: 0.5439\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.8580 - accuracy: 0.5546\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 1.8280 - accuracy: 0.5643\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 1.7989 - accuracy: 0.5685\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.7690 - accuracy: 0.5854\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.7335 - accuracy: 0.5928\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.7047 - accuracy: 0.6022\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.6761 - accuracy: 0.6082\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.6499 - accuracy: 0.6169\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.6250 - accuracy: 0.6241\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.5933 - accuracy: 0.6365\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 1.5701 - accuracy: 0.6417\n",
            "2015/2015 [==============================] - 1s 395us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 296us/step - loss: 3.9822 - accuracy: 0.1159\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.2829 - accuracy: 0.1370\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.1852 - accuracy: 0.1548\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1309 - accuracy: 0.1881\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0800 - accuracy: 0.2040\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0153 - accuracy: 0.2439\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.9382 - accuracy: 0.2737\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8585 - accuracy: 0.2995\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.7669 - accuracy: 0.3293\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6840 - accuracy: 0.3610\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.6083 - accuracy: 0.3702\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.5403 - accuracy: 0.3873\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.4867 - accuracy: 0.4007\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.4288 - accuracy: 0.4141\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 2.3861 - accuracy: 0.4258\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.3329 - accuracy: 0.4325\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2913 - accuracy: 0.4447\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2506 - accuracy: 0.4533\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2092 - accuracy: 0.4667\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.1777 - accuracy: 0.4692\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.1412 - accuracy: 0.4764\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.1031 - accuracy: 0.4913\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.0672 - accuracy: 0.4926\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.0394 - accuracy: 0.5042\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.0045 - accuracy: 0.5119\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.9659 - accuracy: 0.5223\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.9356 - accuracy: 0.5357\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 1.9099 - accuracy: 0.5439\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.8734 - accuracy: 0.5553\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.8371 - accuracy: 0.5717\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.8073 - accuracy: 0.5777\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.7806 - accuracy: 0.5878\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.7415 - accuracy: 0.5935\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.7223 - accuracy: 0.6089\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.6936 - accuracy: 0.6156\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 1.6646 - accuracy: 0.6288\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.6329 - accuracy: 0.6310\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5962 - accuracy: 0.6467\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5811 - accuracy: 0.6469\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5584 - accuracy: 0.6548\n",
            "2015/2015 [==============================] - 1s 384us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 296us/step - loss: 3.9500 - accuracy: 0.1333\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.2708 - accuracy: 0.1439\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1771 - accuracy: 0.1707\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 3.1172 - accuracy: 0.1918\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0631 - accuracy: 0.2248\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0014 - accuracy: 0.2459\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9247 - accuracy: 0.2809\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8372 - accuracy: 0.3057\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.7553 - accuracy: 0.3345\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6773 - accuracy: 0.3553\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5942 - accuracy: 0.3777\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5318 - accuracy: 0.3918\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.4712 - accuracy: 0.4032\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.4164 - accuracy: 0.4208\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.3725 - accuracy: 0.4283\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.3298 - accuracy: 0.4404\n",
            "Epoch 17/40\n",
            "3300/4030 [=======================>......] - ETA: 0s - loss: 2.3048 - accuracy: 0.4473"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f129a95d4879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_feature_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_result_SGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_numbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 184\u001b[0;31m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0m\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLoC5f_WW42",
        "colab_type": "code",
        "outputId": "3cbdf930-6683-47d7-8747-15854e5419f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result_SGD.best_score_, grid_result_SGD.best_params_))\n",
        "\n",
        "means = grid_result_SGD.cv_results_['mean_test_score']\n",
        "stds = grid_result_SGD.cv_results_['std_test_score']\n",
        "params = grid_result_SGD.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.553846 using {'learning_rate': 1.0}\n",
            "0.130356 (0.007660) with: {'learning_rate': 0.0078125}\n",
            "0.234243 (0.024834) with: {'learning_rate': 0.026278012976678578}\n",
            "0.406286 (0.022258) with: {'learning_rate': 0.08838834764831845}\n",
            "0.526882 (0.004518) with: {'learning_rate': 0.29730177875068026}\n",
            "0.553846 (0.003991) with: {'learning_rate': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XK-bINWst5",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.3. Predict \n",
        "\n",
        "Now we can predict directly with optimized hyperparameters by calling \"grid_result.predict\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrHjQcjWrKU",
        "colab_type": "code",
        "outputId": "1eb8799a-2a59-4aa7-eb86-85e674ddf0de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "print(test_feature_matrix.shape)\n",
        "\n",
        "predictions_SGD = grid_result_SGD.predict(test_feature_matrix)\n",
        "predicted_labels_SGD = label_encoder.inverse_transform(list(predictions_SGD))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_SGD)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 20000)\n",
            "1500/1500 [==============================] - 0s 202us/step\n",
            "Classification accuracy:  6.4 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngH0B3sRYcES",
        "colab_type": "text"
      },
      "source": [
        "### Nested CV with SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCmyydMlYf81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# How to choose number of splits: https://machinelearningmastery.com/k-fold-cross-validation/\n",
        "\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "# Inner CV: optimize number hyperparameter\n",
        "# Set up possible values of parameters to optimize over\n",
        "learning_rate = np.logspace(-7, 0, num = 5, base = 2).tolist()\n",
        "params = dict(learning_rate = learning_rate)\n",
        "\n",
        "nested_grid_SGD = GridSearchCV(estimator=bow_model_SGD, param_grid=params, n_jobs=1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "nested_grid_result_SGD = nested_grid_SGD.fit(X, joint_numbers, callbacks = [stop_cb])\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGlQVhbxbDML",
        "colab_type": "code",
        "outputId": "2a25ee71-a620-47a0-ca5b-5fe6cb435b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (nested_grid_result_SGD.best_score_, nested_grid_result_SGD.best_params_))\n",
        "\n",
        "means = nested_grid_result_SGD.cv_results_['mean_test_score']\n",
        "stds = nested_grid_result_SGD.cv_results_['std_test_score']\n",
        "params = nested_grid_result_SGD.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.557982 using {'learning_rate': 1.0}\n",
            "0.119438 (0.005350) with: {'learning_rate': 0.0078125}\n",
            "0.236559 (0.048157) with: {'learning_rate': 0.026278012976678578}\n",
            "0.409429 (0.003865) with: {'learning_rate': 0.08838834764831845}\n",
            "0.528371 (0.003851) with: {'learning_rate': 0.29730177875068026}\n",
            "0.557982 (0.005905) with: {'learning_rate': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkJ7wPURapWO",
        "colab_type": "code",
        "outputId": "15f828ff-3a42-433b-996f-38d444c7bdbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predictions_nested_SGD = nested_grid_result_SGD.predict(test_feature_matrix)\n",
        "predicted_labels_nested_SGD = label_encoder.inverse_transform(list(predictions_nested_SGD))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_nested_SGD)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 1s 489us/step\n",
            "Classification accuracy:  7.4 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5JoCfs3LiU4",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tyq5l2bLT3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = joint_feature_matrix.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY2kbDqScdLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_count, feature_count = joint_feature_matrix.shape\n",
        "class_count = len(train_labels.unique())\n",
        "\n",
        "def create_bow_model_Adam(learning_rate=0.01, nodes=200):\n",
        "  inp = Input(shape=(feature_count,))\n",
        "  hidden1 = Dense(nodes, activation=\"relu\")(inp)\n",
        "  hidden2 = Dense(nodes, activation=\"relu\")(hidden1)                                          # two hidden layers\n",
        "  dropout = Dropout(0.3)(hidden2)\n",
        "  outp = Dense(class_count, activation='softmax')(dropout)                                    # also dropoutrate could be optimized\n",
        "  model = Model(inputs=[inp], outputs=[outp])\n",
        "  optimizer = optimizers.Adam(learning_rate=learning_rate)                                    # also hyperparameter momentum could be optimized\n",
        "  model.compile(optimizer = optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy']) # for one hot encodings\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz9l0NRWkRkY",
        "colab_type": "code",
        "outputId": "b3b03a2f-a6f1-4c3d-cf2d-8aa0f420220b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "model = create_bow_model_Adam()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 200)               4000200   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               20100     \n",
            "=================================================================\n",
            "Total params: 4,060,500\n",
            "Trainable params: 4,060,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqtIMrYQftS2",
        "colab_type": "text"
      },
      "source": [
        "## Nested cross-validation\n",
        "- nested, aka external cross-validation\n",
        "- cross-validation within cross-validation\n",
        "- inner loop for model selection, outer for evaluation\n",
        "- Split the data to K non-overlapping folds\n",
        "1. designate i:th fold as the outer cross-validation test fold, set it\n",
        "aside\n",
        "2. use the remaining K-1 folds to perform model selection with\n",
        "cross-validation as usual\n",
        "3. train the model with selected parameters on the K-1 training\n",
        "folds\n",
        "4. compute predictions using the remaining ith fold as test set\n",
        "I afterwards collect the together predictions made at step 4 for\n",
        "all the K folds and compute your error rate or other\n",
        "performance measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmKrE2zoblrD",
        "colab_type": "code",
        "outputId": "7741249c-5d80-4a48-c0b0-76c6f31347ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
        "# https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/\n",
        "# https://mlfromscratch.com/nested-cross-validation-python-code/#/\n",
        "\n",
        "# Choose cross-validation techniques for the inner and outer loops\n",
        "# outer loop: X folds\n",
        "# inner loop: each fold of outer loop is folded again! To avoid too heavy computaion do not fold too many times or try running parallel \n",
        "# (n_jobs=-1): parallel, (n_jobs=1): sequential\n",
        "\n",
        "start = datetime.now()\n",
        "print(\"start time =\", start)\n",
        "\n",
        "bow_model_Adam = KerasClassifier(build_fn=create_bow_model_Adam, epochs=40, batch_size=100, verbose=0)\n",
        "\n",
        "# Call backs won't work / not needed with gridsearch... \n",
        "# stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "# Warning: 'val_accuracy' not available: Stack overflow search suggestion: \"If the error only occurs when you use smaller datasets, \n",
        "# you're very likely using datasets small enough to not have a single sample in the validation set. \"\n",
        "# mc_cb = ModelCheckpoint(filepath='models/bow_model.h5', monitor='val_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "# Warning: memory leak\n",
        "\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "learning_rate = [0.005, 0.001, 0.01, 0.1]\n",
        "epochs = [20, 40]\n",
        "params = dict(learning_rate=learning_rate, epochs = epochs )\n",
        "\n",
        "nested_grid_Adam = GridSearchCV(estimator=bow_model_Adam, param_grid=params, n_jobs=-1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "nested_grid_result_Adam = nested_grid_Adam.fit(X, joint_labels, verbose = 1, callbacks=[mc_cb])\n",
        "\n",
        "end = datetime.now()\n",
        "print(\"start time =\", end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start time = 2020-05-05 16:06:13.958064\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6045 samples\n",
            "Epoch 1/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 3.5174 - acc: 0.2223WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 2s 295us/sample - loss: 3.4626 - acc: 0.2328\n",
            "Epoch 2/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 2.1627 - acc: 0.4897WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 2.1551 - acc: 0.4907\n",
            "Epoch 3/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 1.5263 - acc: 0.6478WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 128us/sample - loss: 1.5263 - acc: 0.6467\n",
            "Epoch 4/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 1.0318 - acc: 0.7658WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 128us/sample - loss: 1.0313 - acc: 0.7658\n",
            "Epoch 5/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.6426 - acc: 0.8642WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.6438 - acc: 0.8653\n",
            "Epoch 6/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.3970 - acc: 0.9312WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 0.3959 - acc: 0.9327\n",
            "Epoch 7/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9640WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.2492 - acc: 0.9643\n",
            "Epoch 8/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.1708 - acc: 0.9739WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 131us/sample - loss: 0.1689 - acc: 0.9745\n",
            "Epoch 9/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 0.1264 - acc: 0.9800WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 0.1270 - acc: 0.9800\n",
            "Epoch 10/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9846WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0982 - acc: 0.9846\n",
            "Epoch 11/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9865WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0795 - acc: 0.9858\n",
            "Epoch 12/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0654 - acc: 0.9889WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 124us/sample - loss: 0.0649 - acc: 0.9891\n",
            "Epoch 13/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 0.0557 - acc: 0.9893WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.0554 - acc: 0.9894\n",
            "Epoch 14/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9908WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.0463 - acc: 0.9909\n",
            "Epoch 15/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9919WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0380 - acc: 0.9917\n",
            "Epoch 16/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9937WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0346 - acc: 0.9935\n",
            "Epoch 17/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0308 - acc: 0.9946WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 131us/sample - loss: 0.0306 - acc: 0.9945\n",
            "Epoch 18/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0204 - acc: 0.9975WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 132us/sample - loss: 0.0230 - acc: 0.9970\n",
            "Epoch 19/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0211 - acc: 0.9965WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 133us/sample - loss: 0.0202 - acc: 0.9967\n",
            "Epoch 20/40\n",
            "4500/6045 [=====================>........] - ETA: 0s - loss: 0.0165 - acc: 0.9978"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-bd735814dff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnested_grid_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbow_model_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnested_grid_result_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_grid_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3459\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3461\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhzTAqY7brC6",
        "colab_type": "code",
        "outputId": "ea930ab0-0384-4386-eef6-62f89f13ace7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (nested_grid_result_Adam.best_score_, nested_grid_result_Adam.best_params_))\n",
        "\n",
        "means = nested_grid_result_Adam.cv_results_['mean_test_score']\n",
        "stds = nested_grid_result_Adam.cv_results_['std_test_score']\n",
        "params = nested_grid_result_Adam.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "print()\n",
        "predictions_nested_Adam = nested_grid_result_Adam.predict(test_feature_matrix)\n",
        "predicted_labels_nested_Adam = label_encoder.inverse_transform(list(predictions_nested_Adam))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_nested_Adam)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.548883 using {'epochs': 20, 'learning_rate': 0.001}\n",
            "0.535318 (0.004333) with: {'epochs': 20, 'learning_rate': 0.005}\n",
            "0.548883 (0.007505) with: {'epochs': 20, 'learning_rate': 0.001}\n",
            "0.522581 (0.010179) with: {'epochs': 20, 'learning_rate': 0.01}\n",
            "0.303888 (0.021451) with: {'epochs': 20, 'learning_rate': 0.1}\n",
            "0.190240 (0.041566) with: {'epochs': 20, 'learning_rate': 0.2}\n",
            "0.532672 (0.014120) with: {'epochs': 40, 'learning_rate': 0.005}\n",
            "0.541935 (0.008314) with: {'epochs': 40, 'learning_rate': 0.001}\n",
            "0.489992 (0.001638) with: {'epochs': 40, 'learning_rate': 0.01}\n",
            "0.338792 (0.025922) with: {'epochs': 40, 'learning_rate': 0.1}\n",
            "0.119107 (0.009950) with: {'epochs': 40, 'learning_rate': 0.2}\n",
            "\n",
            "Classification accuracy:  13.8 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug4n17gTf9Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This took like ages and ended up with disconnected runtime!!!!!!!!!!!!!!\n",
        "\n",
        "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
        "# # https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/\n",
        "# # https://mlfromscratch.com/nested-cross-validation-python-code/#/\n",
        "\n",
        "# # Choose cross-validation techniques for the inner and outer loops\n",
        "# # outer loop: N folds\n",
        "# # inner loop: each fold of outer loop is folded again! To avoid too heavy computaion do not fold too many times or try running parallel\n",
        "\n",
        "# bow_model_Adam = KerasClassifier(build_fn=create_bow_model_Adam, epochs=40, batch_size=100, verbose=0)\n",
        "\n",
        "# stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "# mc_cb = ModelCheckpoint(filepath='models/bow_model.h5', monitor='accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "# inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "# outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "# # Set the hyperparameter grid. Also possible hyperparams (when parametrized for the functions called!):\n",
        "# # - dropout rate\n",
        "# # - if optimizer is SGD, momentum\n",
        "# # - max_feat (input shape)\n",
        "# nodes = [100, 200]\n",
        "# epochs = [40, 64]\n",
        "# batch_size = [4, 10, 32]\n",
        "# learning_rate = [0.0001, 0.001, 0.01]\n",
        "# params = dict(learning_rate = learning_rate, nodes = nodes, epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "# nested_grid_Adam = GridSearchCV(estimator=bow_model_Adam, param_grid=params, n_jobs=1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "# nested_grid_result_Adam = nested_grid_Adam.fit(X, joint_labels, verbose = 1) #, callbacks=[stop_cb])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2HwFxZzwOP",
        "colab_type": "text"
      },
      "source": [
        "## Fitting BOW-classifier OLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQnwLuHcDntV"
      },
      "source": [
        "Now we will fit the data. Here we will also need the validation data. \n",
        "Let's try with different optimizers available in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwm-SO6ciArq",
        "colab_type": "code",
        "outputId": "a8221790-2150-41f9-d989-c844c7fdfc39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "example_count, feature_count = train_feature_matrix.shape\n",
        "class_count = len(train_labels.unique()) \n",
        "\n",
        "inp = Input(shape = (feature_count, ))                   # Tuple. The size of the inputlayer is the number of the vectors\n",
        "hidden = Dense(300, activation=\"relu\")(inp)  \n",
        "hidden2 = Dense(300, activation=\"relu\")(hidden)\n",
        "dropout = Dropout(0.3)(hidden2)                          # Non-linear activation function. tanh or relu? \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden2) # As many output possibilities as we have input classes. \n",
        "                                                         # Softmax: produces probability distribution of the classes\n",
        "bow_model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "bow_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               6000300   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               30100     \n",
            "=================================================================\n",
            "Total params: 6,120,700\n",
            "Trainable params: 6,120,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SM_ApKFQ0b0",
        "colab_type": "code",
        "outputId": "c70c7131-710d-4558-99b1-6fa665e9e392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Choose best OP and LR and fit the model\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=40, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "bow_model.compile(optimizer = 'Adam', loss=\"categorical_crossentropy\", metrics=['accuracy']) # with one-hot encodings loss = categorical_crossentropy!!!\n",
        "\n",
        "bow_history = bow_model.fit(train_feature_matrix, train_hot, batch_size=32, \n",
        "                 verbose=1, epochs=100, validation_data=(dev_feature_matrix, dev_hot), callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 5295 samples, validate on 750 samples\n",
            "Epoch 1/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 2.6677 - accuracy: 0.3804 - val_loss: 1.9671 - val_accuracy: 0.5320\n",
            "Epoch 2/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 1.2771 - accuracy: 0.7005 - val_loss: 1.7759 - val_accuracy: 0.5733\n",
            "Epoch 3/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.4830 - accuracy: 0.8997 - val_loss: 1.8270 - val_accuracy: 0.5640\n",
            "Epoch 4/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.1798 - accuracy: 0.9726 - val_loss: 1.9260 - val_accuracy: 0.5627\n",
            "Epoch 5/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0993 - accuracy: 0.9817 - val_loss: 1.9714 - val_accuracy: 0.5747\n",
            "Epoch 6/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0696 - accuracy: 0.9858 - val_loss: 2.1091 - val_accuracy: 0.5547\n",
            "Epoch 7/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0519 - accuracy: 0.9883 - val_loss: 2.1371 - val_accuracy: 0.5680\n",
            "Epoch 8/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0404 - accuracy: 0.9924 - val_loss: 2.1620 - val_accuracy: 0.5733\n",
            "Epoch 9/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0256 - accuracy: 0.9962 - val_loss: 2.2527 - val_accuracy: 0.5547\n",
            "Epoch 10/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0174 - accuracy: 0.9960 - val_loss: 2.2290 - val_accuracy: 0.5747\n",
            "Epoch 11/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0137 - accuracy: 0.9968 - val_loss: 2.2201 - val_accuracy: 0.5667\n",
            "Epoch 12/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0119 - accuracy: 0.9957 - val_loss: 2.3929 - val_accuracy: 0.5333\n",
            "Epoch 13/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 2.2785 - val_accuracy: 0.5613\n",
            "Epoch 14/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 2.3964 - val_accuracy: 0.5600\n",
            "Epoch 15/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 2.3295 - val_accuracy: 0.5640\n",
            "Epoch 16/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 2.4246 - val_accuracy: 0.5467\n",
            "Epoch 17/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 2.6989 - val_accuracy: 0.5347\n",
            "Epoch 18/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 2.5332 - val_accuracy: 0.5440\n",
            "Epoch 19/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0080 - accuracy: 0.9966 - val_loss: 2.4533 - val_accuracy: 0.5587\n",
            "Epoch 20/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 2.4860 - val_accuracy: 0.5507\n",
            "Epoch 21/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 2.4765 - val_accuracy: 0.5573\n",
            "Epoch 22/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0066 - accuracy: 0.9975 - val_loss: 2.5494 - val_accuracy: 0.5627\n",
            "Epoch 23/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0071 - accuracy: 0.9958 - val_loss: 2.5375 - val_accuracy: 0.5613\n",
            "Epoch 24/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 2.5119 - val_accuracy: 0.5613\n",
            "Epoch 25/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0057 - accuracy: 0.9974 - val_loss: 2.5735 - val_accuracy: 0.5587\n",
            "Epoch 26/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0073 - accuracy: 0.9966 - val_loss: 2.5949 - val_accuracy: 0.5520\n",
            "Epoch 27/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0061 - accuracy: 0.9970 - val_loss: 2.6181 - val_accuracy: 0.5613\n",
            "Epoch 28/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 2.6174 - val_accuracy: 0.5507\n",
            "Epoch 29/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 2.6717 - val_accuracy: 0.5573\n",
            "Epoch 30/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0063 - accuracy: 0.9972 - val_loss: 2.7221 - val_accuracy: 0.5520\n",
            "Epoch 31/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0060 - accuracy: 0.9968 - val_loss: 2.7433 - val_accuracy: 0.5587\n",
            "Epoch 32/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0059 - accuracy: 0.9977 - val_loss: 2.7393 - val_accuracy: 0.5587\n",
            "Epoch 33/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0052 - accuracy: 0.9974 - val_loss: 2.7310 - val_accuracy: 0.5627\n",
            "Epoch 34/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0052 - accuracy: 0.9972 - val_loss: 2.9057 - val_accuracy: 0.5493\n",
            "Epoch 35/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0058 - accuracy: 0.9970 - val_loss: 2.7401 - val_accuracy: 0.5587\n",
            "Epoch 36/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0051 - accuracy: 0.9979 - val_loss: 2.8739 - val_accuracy: 0.5387\n",
            "Epoch 37/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0061 - accuracy: 0.9972 - val_loss: 2.8903 - val_accuracy: 0.5427\n",
            "Epoch 38/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0059 - accuracy: 0.9974 - val_loss: 2.8280 - val_accuracy: 0.5453\n",
            "Epoch 39/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0055 - accuracy: 0.9970 - val_loss: 3.0793 - val_accuracy: 0.5507\n",
            "Epoch 40/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0064 - accuracy: 0.9970 - val_loss: 2.9128 - val_accuracy: 0.5533\n",
            "Epoch 41/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0054 - accuracy: 0.9974 - val_loss: 2.8703 - val_accuracy: 0.5587\n",
            "Epoch 42/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0054 - accuracy: 0.9970 - val_loss: 2.9243 - val_accuracy: 0.5587\n",
            "Epoch 43/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0046 - accuracy: 0.9972 - val_loss: 3.0769 - val_accuracy: 0.5480\n",
            "Epoch 44/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0046 - accuracy: 0.9979 - val_loss: 2.9347 - val_accuracy: 0.5573\n",
            "Epoch 45/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 2.9236 - val_accuracy: 0.5667\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00045: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_zfbGvKjIL1",
        "colab_type": "code",
        "outputId": "eaf0ecb8-308b-427b-a0c7-2340ab33d4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# form feature matrix for test data set\n",
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "print(test_feature_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MR-tO01ge6V",
        "colab_type": "code",
        "outputId": "8fec90f7-944c-4b4a-f9c5-ea6b22ac6487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "predictions = np.argmax(bow_model.predict(test_feature_matrix), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"predicted labels: \\n\", predicted_labels)\n",
        "print()\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: \n",
            " ['NE NA ' 'NA  ' 'NA  ' ... 'NA  ' 'NA  ' 'PB NA ']\n",
            "\n",
            "Classification accuracy:  9.0 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqmyFiO0CSrd",
        "colab_type": "code",
        "outputId": "a16ba3a7-26b2-486c-9f1d-c95fc30b1cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "# Without np.argmax:\n",
        "\n",
        "# predictions = bow_model.predict(test_feature_matrix) \n",
        "# predictions = one_hot_encoder.inverse_transform(predictions) # transfer form one hot encodings to numerical labels \n",
        "# predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "# print(\"predicted labels: \\n\", predicted_labels)\n",
        "# print()\n",
        "# print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: \n",
            " ['NE NA ' 'NA  ' 'NA  ' ... 'NA  ' 'NA  ' 'PB NA ']\n",
            "\n",
            "Classification accuracy:  9.0 percent\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHF-M4emLu-h",
        "colab_type": "text"
      },
      "source": [
        "Why does the model perform so badly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrzogBR6abt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_numbers)))\n",
        "print(\"-development data: \", len(np.unique(dev_numbers)))\n",
        "print(\"-test data: \", len(np.unique(test_numbers)))\n",
        "print()\n",
        "inter = np.intersect1d(train_numbers, dev_numbers)\n",
        "inter2 = np.intersect1d(train_numbers, test_numbers)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U28dT9xjZYKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# # Confusion matrix has the true labels on rows, and predicted labels on columns in sorted order\n",
        "# print(cnf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8AaFnYV_DRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot confusion matrix\n",
        "# plt.figure()\n",
        "# plot_confusion_matrix(cnf_matrix, classes = all_labels, normalize = False)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JdOntcnMyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # np-argmaxin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "# print(predictions[0])\n",
        "# print(model.predict(test_feature_matrix)[0][25])\n",
        "# print(model.predict(test_feature_matrix)[0])\n",
        "# print(sum(model.predict(test_feature_matrix)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqG5963ZhDa",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.2: Recurrent Neural Network Classifier (multi-class)\n",
        "\n",
        "Modify your codes from milestone 1.1 to use recurrent neural networks (e.g. LSTM or biLSTM) in the classifier. Evaluate your model and report your results with different hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOfJs2YLeAtA",
        "colab_type": "text"
      },
      "source": [
        "For RNN-calssifier we use Tokenizer which turns tokens, in our case the words of training data to integers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7j5rqbNzm3O",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KY7mtEZl-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=97000, # max num of most common words\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6eKbcvnd9m4",
        "colab_type": "code",
        "outputId": "931e1f43-ad55-4aba-fb02-8cc6863e06fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from pprint import pprint    # pretty-printer\n",
        "\n",
        "def truncate_dict(d, count=10):\n",
        "    # Returns at most count items from the given dictionary.  \n",
        "    return dict(i for i, _ in zip(d.items(), range(count)))\n",
        "\n",
        "# Check if 0 is in the index, and print examples of the mapping\n",
        "# 0 is reserved for padding!\n",
        "print(tokenizer.word_index.get(0))\n",
        "pprint(truncate_dict(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "{'ei': 3,\n",
            " 'että': 4,\n",
            " 'ja': 1,\n",
            " 'kun': 9,\n",
            " 'mutta': 8,\n",
            " 'oli': 7,\n",
            " 'on': 2,\n",
            " 'ovat': 10,\n",
            " 'se': 6,\n",
            " 'tai': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDmMZZfg62w",
        "colab_type": "code",
        "outputId": "dac7f186-30cf-42ee-8f87-84545360ed40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "print(len(train_sequences)) \n",
        "\n",
        "# Print an example text, its corresponding sequence, and the tokens it represents\n",
        "print('Text:', train_text[0][0:200]) # first item of the suffled data (index not 0!)\n",
        "#print('Text:', train_text.head(1)[0:200]) # first item of the suffled data (index not 0!)\n",
        "print('Sequence:', train_sequences[0][:10])\n",
        "print('Mapped back:', [tokenizer.index_word[i] for i in train_sequences[0][:10]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5295\n",
            "Text:  Tämähän menee ihan hurjaksi muotihurjasteluksi . Jo toistamiseen tälle syksylle postauksen aiheena vaatetus . Mutta tämä liittyy tavallaan matkailuun . Ja katsokaa nyt näitä , mitkä maailman ihanimma\n",
            "Sequence: [21429, 13274, 40282, 11697, 15340, 796, 657, 388, 12826, 3002]\n",
            "Mapped back: ['logistiikka', 'jenni', 'lindholm', 'laskutus', 'ritva', 'ota', 'yhteyttä', 'nimi', 'puhelinnumero', 'sähköposti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js1rksZ6AA6f",
        "colab_type": "code",
        "outputId": "1acf0d02-6125-42e2-c3e3-f9d5bfb2b2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "lengths = [len(s) for s in train_sequences]\n",
        "print('Lengths:', lengths[:10], 'min:', min(lengths), 'max:', max(lengths), 'mean:', np.mean(lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengths: [384, 921, 158, 194, 932, 388, 123, 384, 1577, 167] min: 0 max: 79136 mean: 583.0457034938621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcr2XZ8zg1H",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Since Keras demands for all of the input items (separate documents of our training data) to have the same length, we need to \"pad\" all but the longest document by filling in the \"missing\" number of words with zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA209Lqxzd8t",
        "colab_type": "code",
        "outputId": "09347272-6e76-4e09-eab6-b391a06c77af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequence_length = np.floor(np.mean(lengths)).astype(int) # based on mean value of input length: we will cut sequences longer than this and pad with zeros sequeces shorter than this\n",
        "\n",
        "type(sequence_length)\n",
        "\n",
        "padded_X = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk9M_AtI1Jc",
        "colab_type": "code",
        "outputId": "79c30b03-d73f-430b-9fb4-d151a98a6f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Prepare model development data\n",
        "\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_text)\n",
        "padded_dev = pad_sequences(\n",
        "    dev_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_dev.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(750, 583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HXbVzAF6UH",
        "colab_type": "text"
      },
      "source": [
        "## LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vn7FWLFGeJQ",
        "colab_type": "text"
      },
      "source": [
        "## HUOMAA!!!\n",
        "Tämä teksti alkuperäisestä RNN-classification notebookista!\n",
        "\n",
        "We define a basic RNN model that takes the RNN cell class (RNN_class) as an argument:\n",
        "\n",
        "- input: sequence of sequence_length integers corresponding to words\n",
        "- embedding: randomly initialized mapping from integers to embedding_dim-dimensional vectors\n",
        "- rnn: recurrent neural network with rnn_units-dimensional state\n",
        "- output: num_classes-dimensional fully connected layer with softmax activation\n",
        "\n",
        "# KATSO NÄITÄ!\n",
        "We're intentionally leaving out a few fairly obvious things that would be expected to help here, including\n",
        "\n",
        "- Any form of regularization, e.g. dropout\n",
        "- Initializing the embeddings with pre-trained word vectors (ks. fasttext)\n",
        "- Masking to ignore padding (see Masking and padding with Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhRaStPGH7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# We'll use these model parameters for all of our examples here.\n",
        "embedding_dim = 50 # input vector\n",
        "rnn_units = 100\n",
        "\n",
        "def build_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "    input_ = Input(shape=(sequence_length,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized. Layer turns positive integers (indexes) into dense vectors of fixed size\n",
        "    rnn = RNN_class(rnn_units)(embedding) # can support different RNNs\n",
        "    output = Dense(num_classes, activation='softmax')(rnn)\n",
        "    return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "sequence_length = padded_X.shape[1]\n",
        "vocab_size = tokenizer.num_words\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5y48zddjkf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4icO32GGX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = build_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikBgdQhpVwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 100\n",
        "stop_cb = EarlyStopping(monitor = 'val_acc', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbxk_lppo_tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_history = lstm_model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])\n",
        "# , callbacks=[stop_cb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXAKNMb5PbM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model test data\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "padded_test = pad_sequences(\n",
        "    test_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrcF1iyLzBkW",
        "colab_type": "text"
      },
      "source": [
        "Predict with LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3MGvq6CzAIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(lstm_model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIycPktw_m39",
        "colab_type": "text"
      },
      "source": [
        "## Bidirectional LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8vdw-jcfAKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Input(shape=(sequence_length,)\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWl2MJjjGmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7L2tJHok6_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "hist_bi_lstm = model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvFxjZZ2ZnQ",
        "colab_type": "text"
      },
      "source": [
        "Predict with bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQIaS_tR2Tiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPskSfawiO5I",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.1: Deep contextual representations with Bert (multi-class)\n",
        "Train a Bert classifier to predict the register categories. Similar to Milestone 1, the setting is multi-class, and the evaluations should include results with different hyperparameters.\n",
        "\n",
        "Neural language models trained on large\n",
        "unannotated corpora can be used to create\n",
        "contextualized representations of\n",
        "meaning\n",
        "\n",
        "[How-to](https://github.com/HannaKi/Deep_Learning_in_LangTech_course/blob/master/bert_text_classification_extended_comments.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSfx6EwPjmR",
        "colab_type": "text"
      },
      "source": [
        "Keep only labels (and texts which correspond to those labels) which appear in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqrKmnsLI1AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 1500\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Kwvy7nKMYt",
        "colab_type": "text"
      },
      "source": [
        "To avoid out of memory we have to limit the input size to MAX_EXAMPLES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82iZbp0N0FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncate_data(data, MAX_EXAMPLES):\n",
        "  if len(data) > MAX_EXAMPLES: # truncate data if needed to avoid OOM\n",
        "    print('Note: truncating examples from {} to {}'.format(len(data), MAX_EXAMPLES)) # should take stratified subsample?\n",
        "    data = data[:MAX_EXAMPLES]\n",
        "  return(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3LbSG4Vxgp_",
        "colab_type": "code",
        "outputId": "e0236970-d957-496e-8c08-4531c0e98f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# We use dev_ and test_ data sets which only have labels that appear in train data\n",
        "# All the datas will be truncated if needed\n",
        "\n",
        "train_ = truncate_data(train, MAX_EXAMPLES)\n",
        "dev_ = truncate_data(dev, MAX_EXAMPLES)\n",
        "test_ = truncate_data(test, MAX_EXAMPLES)\n",
        "\n",
        "frames = [train_, dev_, test_]\n",
        "for d in frames:\n",
        "  print(d.shape)\n",
        "  #print(d.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: truncating examples from 5295 to 1500\n",
            "Note: truncating examples from 1513 to 1500\n",
            "(1500, 2)\n",
            "(756, 2)\n",
            "(1500, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMN0rcLOK4rx",
        "colab_type": "code",
        "outputId": "2eb0236d-7555-4949-e3ff-776b4ac4973a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Miksi ei onnistu ensin ottaa päällekäiset labelit pois ja sitten leikata 1000:een?!?!?\n",
        "\n",
        "train_labels = train_['label']\n",
        "\n",
        "test_ = test_[test_['label'].isin(train_labels.tolist())]\n",
        "dev_ = dev_[dev_['label'].isin(train_labels.tolist())] \n",
        "\n",
        "frames = [train_, dev_, test_]\n",
        "for d in frames:\n",
        "  print(d.shape)\n",
        "  #print(d.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 2)\n",
            "(746, 2)\n",
            "(1476, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95rtEhpXJVdo",
        "colab_type": "text"
      },
      "source": [
        "Install Keras Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi49M60HJRiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install keras-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fVv1axNJhVn",
        "colab_type": "text"
      },
      "source": [
        "Set an environment variable for keras-bert to use tensorflow.python.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Td8Gp40JeYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zEw1o-yKC3T",
        "colab_type": "text"
      },
      "source": [
        "## Download pretrained FinBERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRasDCPh1jUM",
        "colab_type": "text"
      },
      "source": [
        "Download pretrained TurkuNLP FinBERT from: https://github.com/TurkuNLP/FinBERT and prepare it for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J87QH8NiKFQm",
        "colab_type": "code",
        "outputId": "a3556b9d-6f6d-427d-c1a9-12be5696f261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!wget -nc http://dl.turkunlp.org/finbert/bert-base-finnish-cased-v1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 16:47:46--  http://dl.turkunlp.org/finbert/bert-base-finnish-cased-v1.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1374658696 (1.3G) [application/zip]\n",
            "Saving to: ‘bert-base-finnish-cased-v1.zip’\n",
            "\n",
            "bert-base-finnish-c 100%[===================>]   1.28G  13.1MB/s    in 87s     \n",
            "\n",
            "2020-05-07 16:49:14 (15.0 MB/s) - ‘bert-base-finnish-cased-v1.zip’ saved [1374658696/1374658696]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6sF2aEdYqhf",
        "colab_type": "code",
        "outputId": "c27604d2-850d-44cd-ac92-0b5dadf12c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Give -n argument so that existing files aren't overwritten \n",
        "!unzip -n bert-base-finnish-cased-v1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bert-base-finnish-cased-v1.zip\n",
            "  inflating: bert-base-finnish-cased-v1/bert_config.json  \n",
            "  inflating: bert-base-finnish-cased-v1/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: bert-base-finnish-cased-v1/bert_model.ckpt.index  \n",
            "  inflating: bert-base-finnish-cased-v1/bert_model.ckpt.meta  \n",
            "  inflating: bert-base-finnish-cased-v1/vocab.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fcLhH5_Z3LS",
        "colab_type": "text"
      },
      "source": [
        "Store paths to important files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yboNBfdZ2TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vocab_path = 'bert-base-finnish-cased-v1/vocab.txt'\n",
        "bert_config_path = 'bert-base-finnish-cased-v1/bert_config.json'\n",
        "bert_checkpoint_path = 'bert-base-finnish-cased-v1/bert_model.ckpt' # suffixes not required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ulKEdrFa5J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_is_cased = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cXqCpbSa-20",
        "colab_type": "text"
      },
      "source": [
        "## Load vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LyiI4clbCU1",
        "colab_type": "code",
        "outputId": "099d2f9b-1a93-40f5-fa4f-129af0224dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "vocab = []\n",
        "with open(bert_vocab_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
        "\n",
        "\n",
        "# Print a list with every 500th vocabulary item (includes suffixes)\n",
        "print(vocab[0::500]) # 0 is for padding!\n",
        "print(vocab[103]) # tags are included"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '##hin', '##tila', 'tehtä', '##ce', 'avulla', '##uttua', 'Sil', '##iver', 'tarvits', 'kilometrin', 'sanotaan', '##ya', 'onnistuu', 'tapaus', 'rento', '##otin', '##kasvat', 'kauhe', 'puolin', 'ymmärrän', 'polttoain', 'arvot', 'ajattelu', '##impiin', 'huomasi', 'tietokoneen', 'tiedämme', 'johdossa', 'teemme', 'paikallisen', 'rekryt', 'vuosikymmeniä', 'kohdistuu', 'tyhmiä', 'baa', '##ipa', 'aero', '##ehtien', 'viimeisteli', '##llosta', 'luulevat', 'verenpain', 'tuottamaan', 'vahingot', 'opiskelijan', '##päivinä', '##uksellisesti', 'uskottava', '##elemaan', 'ilmennyt', 'määrätieto', 'leppo', 'yksityiset', 'kirjailijan', 'vastikään', 'samoista', '##misiin', '##pankkien', 'tuut', 'muutoinkin', 'säilyi', '##ivuoren', '##pauksia', 'kaupungilta', '##lalle', 'tervetulleeksi', 'lähteitä', 'isoin', 'Tuskinpa', 'hallinnut', '##kanslerin', '##jela', 'siniset', 'erikoiskokeella', 'todistavat', 'itsehallinto', 'Oran', 'vikaan', 'ISIS', 'hankinnan', '##eleitä', '##heitolla', '##upar', 'raam', 'edelläkävijä', 'tunnistettu', 'kohtaamis', 'punaisena', 'murtaa', '##ske', 'toimivuudesta', 'zo', '##ektro', '114', 'seinästä', 'Kuhmon', 'viesteissä', 'tukim', 'sellaisiksi', 'Rautio']\n",
            "[SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srU3eKQvbMbW",
        "colab_type": "text"
      },
      "source": [
        "## Load BERT configuration\n",
        "\n",
        "Just peek into the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHDZ0R-nbO6H",
        "colab_type": "code",
        "outputId": "e04d5934-cc6d-411b-903c-2fa992bb56a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "with open(bert_config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Print configuration contents\n",
        "pprint(config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_probs_dropout_prob': 0.1,\n",
            " 'hidden_act': 'gelu',\n",
            " 'hidden_dropout_prob': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_size': 3072,\n",
            " 'max_position_embeddings': 512,\n",
            " 'num_attention_heads': 12,\n",
            " 'num_hidden_layers': 12,\n",
            " 'type_vocab_size': 2,\n",
            " 'vocab_size': 50105}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5KmJ0OecMDp",
        "colab_type": "text"
      },
      "source": [
        "## Create BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYckpNLBcXMJ",
        "colab_type": "text"
      },
      "source": [
        "To create the tokenizer, we'll need a mapping from vocabulary items to their corresponding integer indices. We do this conventionally using enumerate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBV0vAhwcLqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create mapping from vocabulary items to their indices in the vocabulary\n",
        "token_dict = { v: i for i, v in enumerate(vocab) }\n",
        "\n",
        "# Print some random examples of the mapping\n",
        "#pprint(dict(random.choices(list(token_dict.items()), k=10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-T7zTcU08",
        "colab_type": "code",
        "outputId": "d7bb522e-e8ac-4caa-93eb-8dbe866fbc89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "# Test keras-Bert tokenizer\n",
        "from keras_bert import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(token_dict, cased=model_is_cased) \n",
        "\n",
        "# Let's test that out\n",
        "for s in ['Heippa BERT!', 'Tämä lause on suomeksi. tai ei', 'Kiinaa tai japania: 你', 'Yksi kirjain a on yksi']:\n",
        "    print('Original string:', s)\n",
        "    print('Tokenized:', tokenizer.tokenize(s))\n",
        "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
        "    print('Encoded:', indices)\n",
        "    print('Segments:', segments)\n",
        "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string: Heippa BERT!\n",
            "Tokenized: ['[CLS]', 'Hei', '##ppa', 'B', '##ER', '##T', '!', '[SEP]']\n",
            "Encoded: [102, 5050, 2096, 415, 9981, 50031, 380, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Hei ##ppa B ##ER ##T !\n",
            "\n",
            "Original string: Tämä lause on suomeksi. tai ei\n",
            "Tokenized: ['[CLS]', 'Tämä', 'lause', 'on', 'suomeksi', '.', 'tai', 'ei', '[SEP]']\n",
            "Encoded: [102, 1131, 17580, 145, 11695, 111, 337, 193, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Tämä lause on suomeksi . tai ei\n",
            "\n",
            "Original string: Kiinaa tai japania: 你\n",
            "Tokenized: ['[CLS]', 'Kiinaa', 'tai', 'japan', '##ia', ':', '你', '[SEP]']\n",
            "Encoded: [102, 39876, 337, 9780, 157, 236, 101, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Kiinaa tai japan ##ia : [UNK]\n",
            "\n",
            "Original string: Yksi kirjain a on yksi\n",
            "Tokenized: ['[CLS]', 'Yksi', 'kirjain', 'a', 'on', 'yksi', '[SEP]']\n",
            "Encoded: [102, 3420, 28106, 151, 145, 1034, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Yksi kirjain a on yksi\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFPCAVJd9gD",
        "colab_type": "text"
      },
      "source": [
        "## Prepare labels\n",
        "\n",
        "Y holds the labels the model will learn to predict. We will train with the labels the truncated training data contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ioyLbFPKqkS",
        "colab_type": "code",
        "outputId": "29990939-3ac4-4c98-94cf-c56fd97ef037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# training data\n",
        "\n",
        "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
        "train_label = train_['label'].tolist() \n",
        "\n",
        "#label_encoder.fit(all_labels)\n",
        "label_encoder.fit(train_label)\n",
        "Y = label_encoder.transform(train_label) # encoded labels (not one hot!)\n",
        "\n",
        "# Take note of how many unique labels there are in the data\n",
        "num_labels = len(set(Y))\n",
        "\n",
        "# Print out some examples\n",
        "print(\"Training data\")\n",
        "print('Number of unique labels in training data:', num_labels)\n",
        "print(type(train_label), train_label[:10])\n",
        "print(type(Y), Y[:10])\n",
        "print()\n",
        "\n",
        "# development data\n",
        "print(\"Development data\")\n",
        "dev_label = dev_['label']#.tolist()\n",
        "y = label_encoder.transform(dev_label)\n",
        "print('Number of unique labels in training data:', len(set(y)))\n",
        "print(type(dev_label), dev_label[:10])\n",
        "print(type(y), y[:10])\n",
        "print()\n",
        "# test data\n",
        "print(\"Test data\")\n",
        "test_label = test_['label'].tolist()\n",
        "true_labels = label_encoder.transform(test_label)\n",
        "print('Number of unique labels in training data:', len(set(true_labels)))\n",
        "print(type(test_label), test_label[:10])\n",
        "print(type(true_labels), true_labels[:10])\n",
        "# print(len(test_label))\n",
        "# print(len(test_['text']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data\n",
            "Number of unique labels in training data: 64\n",
            "<class 'list'> ['DS IG ', 'RS OP ', 'NE NA ', 'SR NA ', 'MT OS ', 'NA  ', 'DP IN ', 'DS IG ', 'DF ID ', 'DT IN ']\n",
            "<class 'numpy.ndarray'> [11 58 37 61 32 36  8 11  6 12]\n",
            "\n",
            "Development data\n",
            "Number of unique labels in training data: 48\n",
            "<class 'pandas.core.series.Series'> 0     OA NA \n",
            "1     DS IG \n",
            "2     DS IG \n",
            "3     DF ID \n",
            "4     OA NA \n",
            "6     DF ID \n",
            "7     MT OS \n",
            "8     CB NA \n",
            "9       HI  \n",
            "10      NA  \n",
            "Name: label, dtype: object\n",
            "<class 'numpy.ndarray'> [42 11 11  6 42  6 32  1 22 36]\n",
            "\n",
            "Test data\n",
            "Number of unique labels in training data: 47\n",
            "<class 'list'> ['HI  ', 'NA  ', 'DT IN ', 'DP IN ', 'DT IN ', 'AV OP ', 'CB NA ', 'IN  ', 'NE NA ', 'TB NA ']\n",
            "<class 'numpy.ndarray'> [22 36 12  8 12  0  1 27 37 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7z05ec1Hfr",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize input data\n",
        "Keep token indices and segment ids in separate lists and store as numpy arrays. X here is the final vectorized form of the input we'll be providing to the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYRRn2deAt7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model_inputs(text):\n",
        "  token_indices, segment_ids = [], []\n",
        "  for text in text:\n",
        "      # tokenizer.encode() returns a sequence of token indices\n",
        "      # and a sequence of segment IDs. BERT expects both as input,\n",
        "      # even if the segments IDs are just all zeros (like here).\n",
        "      tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
        "      token_indices.append(tid)\n",
        "      segment_ids.append(sid)\n",
        "  inp = [np.array(token_indices), np.array(segment_ids)] # Format input as list of two numpy arrays\n",
        "  return inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxBP2UFcBLFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = make_model_inputs(train_['text'])\n",
        "x = make_model_inputs(dev_['text'])\n",
        "test_inp = make_model_inputs(test_['text'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4WUr_OfBU1C",
        "colab_type": "code",
        "outputId": "a9c129a4-96ff-4ee2-f9c3-841f3e02a1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Print some examples\n",
        "print(X[0].shape)\n",
        "print('Token indices:')\n",
        "print(test_inp[0][:2])\n",
        "print('Decoded:')\n",
        "for i in test_inp[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(test_inp[1][:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 250)\n",
            "Token indices:\n",
            "[[  102 41061  8448  6641 49802   111   609 36238  6575   142 30132  6684\n",
            "   4535 50051   166 30086 46629   171  8171  2181 35150   489  1620 15336\n",
            "  20129   364 10915  5034 12221  1236   111   609 15336 22463  8413  1904\n",
            "  22718   303  4767   422   119   206 24477   182 37947  6355 33845  1661\n",
            "    142   499   337  3381   422   943  8884  4535 50051   166 30896 32409\n",
            "  50009   111 10280   224  1611  2799 17213  6706  2264  8022   236  2065\n",
            "    111 18808  1642 35804 30114  9106   111 15671   145  7772  1091   236\n",
            "    998 46271   411   597  9258 10006 13111 12949  6360   166 42864 50009\n",
            "   5971  1827   193   257 14463 26420 34478 35403  2443   380  5971  1694\n",
            "    339  4960 17345   255 14281  2494  1642  4535 50051   166 30086   111\n",
            "  29174  1337  5253   145  3518 16883   170 20818   353 14798   111  5971\n",
            "   1232  8836   988 14589 34478   255  4896  2951  5962   111   609 34478\n",
            "    255  7656   193  5976 21841  8679   802  1038   145   652   138  3336\n",
            "   4884   776   303  4211   422 23868   176 23281   144  2177  4401 50069\n",
            "   2065 10254 50057  2960 42630   919 34478   145  3291   142 31383 15900\n",
            "    499 21399 34478 47485   303 34478   146   727   864   270 29940 21973\n",
            "   5187  2648   111  8605   358  8545  4100   126 28066  2055  3505 19048\n",
            "  20518 15869 34356   734 13368 22660 24249   170   111  3837  1322  5552\n",
            "  13368  8545  3524  4622 11619  1061   337 11841  1061 22277 15856   111\n",
            "  16572   936  4622 13953   145  5938   563  1002  1958   166   142 12171\n",
            "   2489 16620  8952   111 12905  4622 13953   145 29245   103]\n",
            " [  102   261 16634   236 15293 34209  3414   136  1742  6091   111   111\n",
            "    111 20675  1024 50016   119 25543  2342   380 29548 50011 15251 16666\n",
            "  12838   913   380  9181   119 19801 50011  6114   235  1354 31814  2167\n",
            "    427   559 38060   380 16026 50011   672  1385   590  8275   946 33124\n",
            "   2045 38636 17708   539   305  3323 25896  1442 22502 50006   142   672\n",
            "   2644   305 18308 50006   437   164 34276  9368  6054  1345  1527 26619\n",
            "   8247   105 38619   464   142 19278   333 22258   473 17671   119  5236\n",
            "    519 25134  1015 50006   142  8703 27427  8885 17856   142 27642   199\n",
            "    280   563  2010  5822 26129   519 15100   265 46444   725 29769   111\n",
            "    111   111  2297 15567   478  7657   106 30428   133 24915 50008   111\n",
            "    111   111  8159  9436 18317 18534   468   913 10981   368   142 34713\n",
            "  43256  5786   658  2073  8429   119   304   491  4658   250  7942  5323\n",
            "    119  1224   572  7065  2392   164   380 14973 50010   119  8784 16666\n",
            "  37516 20038  4361 37344   380  3586  6642   913  1671   672  1385   590\n",
            "   1676   774  3158 25353   403   119   606  2394  1107 15133  4655   845\n",
            "   1019  3430  3811   184  1251  1832  1429 19864   785   119  1224 18727\n",
            "  15601 12823   127  6670   142  9415 10101   572  7455 10997   113  6567\n",
            "   1736  1529   380  7407  7295  1346  1697  1236  5887 50007   341   567\n",
            "   3588 50012   639   111   111   111   142  4638 12318 50007  1706 34775\n",
            "  41574   207   427   119 29194  4829 29586  2073  8429   572  4019   142\n",
            "  19017   280   572 17893  1437 36443   142  2857  3870   103]]\n",
            "Decoded:\n",
            "['Tehkää', 'nolla', '##lei', '##maus', '.', 'Jos', 'rekisteröinti', 'onnistui', 'ja', 'käyttämä', '##nne', 'Q', '##R', '-', 'koodi', '##luki', '##ja', '##ohjelma', 'toimii', 'toivot', '##ulla', 'tavalla', 'saatte', 'kuitta', '##uksen', 'onnistun', '##eesta', 'leima', '##uksesta', '.', 'Jos', 'saatte', 'valituksen', 'puuttu', '##vasta', 'joukkueesta', 'niin', 'varmista', '##kaa', ',', 'että', 'teit', '##te', 'edellämain', '##itun', 'rekisteröin', '##nin', 'ja', '/', 'tai', 'kokeil', '##kaa', 'eri', 'ohjelmaa', 'Q', '##R', '-', 'koodin', 'tulkintaa', '##n', '.', 'Siir', '##ty', '##kää', 'lähtö', '##alueelle', 'viimeistään', 'klo', '09', ':', '50', '.', 'Alue', 'löytyy', 'hissi', '##linjan', 'edestä', '.', 'Lähtö', 'on', 'tasan', '10', ':', '00', 'Lähd', '##ön', 'jälkeen', 'julkaistaan', 'tapahtuman', 'sivulla', 'linkki', 'online', '-', 'seurantaa', '##n', 'Ras', '##teja', 'ei', 'ole', 'merkitty', 'maastoon', 'rasti', '##lip', '##uilla', '!', 'Ras', '##tit', 'ovat', 'pieniä', 'paperi', '##la', '##ppuja', 'joista', 'löytyy', 'Q', '##R', '-', 'koodi', '.', 'Joillakin', 'ras', '##teilla', 'on', 'puna', '##valko', '##ista', 'merkintä', '##na', '##uhaa', '.', 'Ras', '##tis', '##elit', '##teet', 'auttavat', 'rasti', '##la', '##pun', 'löytä', '##misessä', '.', 'Jos', 'rasti', '##la', '##ppua', 'ei', 'löydy', 'oikeasta', 'paikasta', 'eli', 'joku', 'on', 'lap', '##un', 'käynyt', 'poista', '##massa', 'niin', 'ilmoitta', '##kaa', 'tekstiviesti', '##llä', 'nro', '##on', '+', '35', '##8', '50', '58', '##3', '60', '##44', 'mikä', 'rasti', 'on', 'kyseessä', 'ja', 'ottakaa', 'valokuva', '/', 'valokuvia', 'rasti', '##alueesta', 'niin', 'rasti', '##lla', 'käy', '##misen', '##ne', 'tarkistetaan', 'maalissa', 'valoku', '##vista', '.', 'Myöh', '##äs', '##tyminen', 'sarjan', '##sa', 'takara', '##jasta', 'aiheuttaa', 'miinus', '##pisteen', 'jokaisesta', 'alkava', '##sta', 'myöhäs', '##tymis', '##minuut', '##ista', '.', 'Yli', '15', 'minuutin', 'myöhäs', '##tyminen', 'johtaa', 'joukkueen', 'hylkää', '##miseen', 'tai', 'siirtä', '##miseen', 'pitempään', 'sarjaan', '.', 'Kilpailun', 'aikana', 'joukkueen', 'jäsenten', 'on', 'oltava', 'koko', 'ajan', 'näkö', '-', 'ja', 'kuulo', '##etä', '##isyydellä', 'toisistaan', '.', 'Kaikkien', 'joukkueen', 'jäsenten', 'on', 'käytävä']\n",
            "['1', 'kommenttia', ':', 'Syys', '##lomalle', '##läh', '##ti', '##jät', 'kirjoitti', '.', '.', '.', 'Ly', '##sti', '##ä', ',', 'lys', '##tiä', '!', 'Onpa', '##s', 'kivoja', 'lumi', '##kuvia', 'taas', '!', 'Juu', ',', 'enpä', '##s', 'taida', '##kaan', 'ottaa', 'filla', '##ria', 'mukaan', 'vaan', 'sukset', '!', 'Joko', '##s', 'ensi', '##lu', '##men', '##lad', '##ulle', 'pääsis', 'kohta', '##puolin', 'hiihtä', '##mään', '?', 'Siis', 'tulevana', 'viikon', '##vaihteen', '##a', 'ja', 'ensi', 'viikolla', '?', 'Kip', '##a', '##isin', 'jo', 'ulla', '##kolla', 'kerää', '##mässä', 'por', '##ukalle', 'talvi', '##ta', '##mine', '##ita', 'ja', 'laukku', '##jen', 'pakkaa', '##minen', 'aloitettu', ',', 'huomenna', 'vielä', 'vanhempain', '##ilta', '##a', 'ja', 'öljyn', '##vaihtoa', 'meno', '##peliin', 'ja', 'tänäänkin', 'se', 'oli', 'koko', 'päivän', 'korjaa', '##molla', 'vielä', 'jous', '##ien', 'laaker', '##eiden', 'vaihdossa', '.', '.', '.', 'piti', 'säätää', 'vain', 'pyöri', '##en', 'aura', '##us', '##kulma', '##t', '.', '.', '.', 'Tie', '##hallinnon', 'keli', '##kamer', '##oita', 'taas', 'sela', '##sin', 'ja', 'Salla', '##tuntu', '##rilla', 'hyvin', 'pyr', '##yttänyt', ',', 'mutta', 'nyt', 'vien', '##ee', 'Pyhä', 'voiton', ',', 'siellä', 'ihan', 'kin', '##oksia', 'jo', '!', 'Je', '##e', ',', 'päästään', 'lumi', '##ukkojen', 'tekoon', 'omalla', 'tontilla', '!', 'Aika', 'kivaa', 'taas', 'päästä', 'ensi', '##lu', '##men', 'kokem', '##iseen', 'syys', '##lomalla', '##mme', ',', 'vaikka', 'ollaan', 'me', 'Saari', '##sel', '##ällä', 'siv', '##ak', '##oitu', '##kin', 'tällä', 'sama', '##isella', 'lomalla', 'ennen', ',', 'siellä', 'laitettiin', 'pikaisesti', 'lad', '##ut', 'kuntoon', 'ja', 'porukkaa', 'riitti', 'ihan', 'kilpa', '##hiih', '##tä', '##jiin', 'asti', 'heti', '!', 'Tieto', 'lat', '##ujen', 'ava', '##uksesta', 'kiir', '##i', 'kuin', 'kul', '##ova', '##l', '##kea', '.', '.', '.', 'ja', 'saatiin', 'sillo', '##i', 'tosi', 'upeat', 'jouluku', '##vat', 'mukaan', ',', 'Kaunis', '##pään', 'huipulla', 'pyr', '##yttänyt', 'ihan', 'kunnolla', 'ja', 'mökki', 'oli', 'ihan', 'lumen', '##pe', '##itossa', 'ja', 'lapset', 'ton']\n",
            "Segment ids:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O7-xnrg3nyT",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained BERT model form checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QBf-vQiWdcY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load pretrained BERT model\n",
        "\n",
        "We'll use the keras-bert function load_trained_model_from_checkpoint to load the model from the checkpoint we downloaded earlier.\n",
        "\n",
        "Explanation for a few parameters from keras-bert documentation:\n",
        "\n",
        "- training: If training, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
        "- trainable: Whether the model is trainable. The default value is the same with training.\n",
        "\n",
        "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use training=False but trainable=True.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReplOvS3nAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False, # ignore MLM [masked language modeling] and NSP [next sentence prediction] parts of the model\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH # define the size of input layer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3jk07Kw4Lrm",
        "colab_type": "code",
        "outputId": "a7ef4582-c02b-4fb4-c6dd-0b96d07d73c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# token and segment inputs, just as we made\n",
        "pretrained_model.inputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Input-Token:0' shape=(?, 250) dtype=float32>,\n",
              " <tf.Tensor 'Input-Segment:0' shape=(?, 250) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JPTQgDb4aUz",
        "colab_type": "code",
        "outputId": "bfd2b7d4-e085-4359-d36d-2db0070a81d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "pretrained_model.outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Encoder-12-FeedForward-Norm/add_1:0' shape=(?, 250, 768) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwqTgaZ55vD0",
        "colab_type": "text"
      },
      "source": [
        "Size of the output layer does not match our label count. This must be fixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQNBB7Gk5etU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained_model.summary()\n",
        "# same transformer layer repeated 12 times: self\n",
        "# attention, feedforward, drop out, normalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss678QTb63-0",
        "colab_type": "text"
      },
      "source": [
        "## Build classification model by wrapping the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNV-IQ6dX4De",
        "colab_type": "text"
      },
      "source": [
        "We will \"catch\" the model output and plug our own output layer on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8G2WqT763Cd",
        "colab_type": "code",
        "outputId": "f89dfce4-9dbe-4451-d6ba-2f23f980c9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLpSFbIA0jQv",
        "colab_type": "text"
      },
      "source": [
        "Wrapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8-fYS4y7ICV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "out = Dense(num_labels, activation='softmax')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
        "model = Model(\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuDLml6v83JD",
        "colab_type": "text"
      },
      "source": [
        "Now the size of the output layer matches the number of our data labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2eJUnT78uQX",
        "colab_type": "code",
        "outputId": "b35c6245-5c9a-44fd-a964-288f13a26cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense/Softmax:0' shape=(?, 64) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BekdEFHw7jqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYwXxwPe9J1j",
        "colab_type": "text"
      },
      "source": [
        "## Create optimizer\n",
        "\n",
        "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We won't go into detail on these optimizer settings, but will instead largely copy parameters used in the original BERT work.\n",
        "\n",
        "(If you're interested in tuning the training process, trying different values of LEARNING_RATE is a good place to start!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs_9hj7ey_ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 12                             #16\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.00002                   #0.01\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 16                          #8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoqDpD3C9LJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train_['text']),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2MKRaKT9iHk",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkYA99RO9grF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy', # encoded labels!\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR2OgW1B9rsY",
        "colab_type": "code",
        "outputId": "65dba33d-815f-44a8-8b76-b8a8c8301f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_sparse_categorical_accuracy', patience=4, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "mc_cb = ModelCheckpoint(filepath='models/BERT_multiclass.h5', monitor='val_sparse_categorical_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x, y), \n",
        "    callbacks=[stop_cb, mc_cb]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 1500 samples, validate on 746 samples\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 247s 165ms/sample - loss: 3.2485 - sparse_categorical_accuracy: 0.2707 - val_loss: 2.0331 - val_sparse_categorical_accuracy: 0.5241\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 1.7543 - sparse_categorical_accuracy: 0.5793 - val_loss: 1.6019 - val_sparse_categorical_accuracy: 0.6072\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 1.0381 - sparse_categorical_accuracy: 0.7367 - val_loss: 1.5044 - val_sparse_categorical_accuracy: 0.6247\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 0.5248 - sparse_categorical_accuracy: 0.8780 - val_loss: 1.4730 - val_sparse_categorical_accuracy: 0.6354\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 0.2398 - sparse_categorical_accuracy: 0.9573 - val_loss: 1.5075 - val_sparse_categorical_accuracy: 0.6381\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 197s 131ms/sample - loss: 0.1204 - sparse_categorical_accuracy: 0.9887 - val_loss: 1.5208 - val_sparse_categorical_accuracy: 0.6381\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 0.0677 - sparse_categorical_accuracy: 0.9973 - val_loss: 1.5363 - val_sparse_categorical_accuracy: 0.6394\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 0.0511 - sparse_categorical_accuracy: 0.9967 - val_loss: 1.5372 - val_sparse_categorical_accuracy: 0.6408\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 201s 134ms/sample - loss: 0.0393 - sparse_categorical_accuracy: 0.9973 - val_loss: 1.5505 - val_sparse_categorical_accuracy: 0.6434\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0343 - sparse_categorical_accuracy: 0.9987 - val_loss: 1.5546 - val_sparse_categorical_accuracy: 0.6408\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0300 - sparse_categorical_accuracy: 0.9980 - val_loss: 1.5611 - val_sparse_categorical_accuracy: 0.6434\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 196s 131ms/sample - loss: 0.0281 - sparse_categorical_accuracy: 0.9987 - val_loss: 1.5632 - val_sparse_categorical_accuracy: 0.6434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn_hm0UDNsgK",
        "colab_type": "code",
        "outputId": "8c536341-456c-42b2-fd66-0fb80d5c6aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrP6xhZJmpV",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noTnBHs4cJcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load best model\n",
        "model.load_weights('models/BERT_multiclass.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvK_t2LXNWam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(test_inp), axis=1) # np.argmax gives the index which has the highest value e.g. class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWn3JBFpQSak",
        "colab_type": "code",
        "outputId": "628fb23f-7137-4308-93ed-7a6a01eebcd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "predicted_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['HI  ', 'PB NA ', 'DT IN ', ..., 'OA NA ', 'PB NA ', 'MT OS '],\n",
              "      dtype='<U12')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uamUmR_ZQMFq",
        "colab_type": "code",
        "outputId": "faf6e0f1-8197-47b3-ec95-069d07648f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification accuracy: \", round(accuracy_score(test_label, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  64.8 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ2lrJaZQ42Z",
        "colab_type": "text"
      },
      "source": [
        "Accuracy pretty good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSi-7hwqqTs",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.2: Error analysis\n",
        "Compare the errors made by the classifiers you have trained from milestones 1 and 2.1. Are there any patterns? How do the errors one model makes differ from those made by another.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5QbHGcldxep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy\n",
        "# confusion matrix\n",
        "# classification_report\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HcrqoxqxiI",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 3.1: Bert (multi-LABEL)\n",
        "Train two multi-label classifiers, one using non-deep contextual representations, the other using Bert. In this setting, each label is assigned independently. Do hyperparameter optimization on these classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8PqFpHSXrPR",
        "colab_type": "text"
      },
      "source": [
        "How to improve multilabel classification:\n",
        "\n",
        "According to \n",
        "[scikit-multilearn](http://scikit.ml/labelrelations.html): \"Multi-label classification tends to have problems with overfitting and underfitting classifiers when the label space is large, especially in problem transformation approaches. A well known approach to remedy this is to split the problem into subproblems with smaller label subsets to improve the generalization quality.\"\n",
        "- We could improve the classification by training the separate models for high leve and sublevel registers, since the labels are no independent!\n",
        "\n",
        "\n",
        "[Zhang et al WHAT YEAR?!](https://http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.7318&rep=rep1&type=pdf): An  intuitive  approach  to  solving  multi-label  problem  is  todecompose  it  into  multiple  independent  binary  classification problems  (one  per  category).  However,  this  kind  of  methoddoes not consider the correlations between the different labelsof  each  instance  and  the  expressive  power  of  such  a  systemcan  be  weak.\n",
        "- We do just this: multiple  independent  binary  classification problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrVf5bheeRNH",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation for milestone 3\n",
        "\n",
        "Since we do multi label classification we are dealing with highlevel and sublevel registers and have significantly less labels (for example 'NA OP' becomes 'NA' and 'OP'. This means we have to prepare the data differently from the previous tasks\n",
        "\n",
        "We will separate all the registers and one hot encode them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwa_GN5xdzy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data manipulation: high level registers\n",
        "\n",
        "hiregs = ['NA', 'OP', 'IN', 'ID', 'HI', 'IP', 'IG', 'LY', 'SP', 'OS']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXVJU0CQPIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean up the label(register) column:\n",
        "\n",
        "def from_registers(col):\n",
        "  lst = col.tolist() # column to list\n",
        "  registers = [[]] # list of lists, as long as the original column\n",
        "  idx = 0\n",
        "  for element in lst:    #NA OP\n",
        "      parts = element.split(' ') #NA #OP\n",
        "      for i in parts: \n",
        "        if i == '': # omit empty strings\n",
        "          continue\n",
        "        registers[idx].append(i) # append to list in list\n",
        "      if len(lst)-1 > idx :\n",
        "        idx = idx+1\n",
        "        registers.insert(idx, []) # add new empty list to list\n",
        "  return registers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5laiU_0WPw",
        "colab_type": "code",
        "outputId": "82fb9d12-16cd-484b-fa6f-01cdbb968dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# use combined data formed at the beinning of the notebook\n",
        "\n",
        "print(df.shape)\n",
        "all_regs=from_registers(df['label']) # use custom made function to list the registers\n",
        "df['regs'] = all_regs\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7564, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "      <th>regs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "      <td>[DS, IG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[RS, OP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "      <td>[NE, NA]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[SR, NA]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "      <td>[MT, OS]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data      regs\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train  [DS, IG]\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train  [RS, OP]\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train  [NE, NA]\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train  [SR, NA]\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train  [MT, OS]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwsgYM9Q0mVx",
        "colab_type": "text"
      },
      "source": [
        "Next we will turn those registers to one hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbMk-Mz9Uj4",
        "colab_type": "code",
        "outputId": "1eb6dc9e-d917-4f9d-f554-cb312710353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "dummies = pd.DataFrame(mlb.fit_transform(df['regs']), columns=mlb.classes_, index=df.index)\n",
        "\n",
        "dummies"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AV</th>\n",
              "      <th>CB</th>\n",
              "      <th>CM</th>\n",
              "      <th>DF</th>\n",
              "      <th>DP</th>\n",
              "      <th>DS</th>\n",
              "      <th>DT</th>\n",
              "      <th>EB</th>\n",
              "      <th>EN</th>\n",
              "      <th>FA</th>\n",
              "      <th>FC</th>\n",
              "      <th>FS</th>\n",
              "      <th>HA</th>\n",
              "      <th>HI</th>\n",
              "      <th>IB</th>\n",
              "      <th>ID</th>\n",
              "      <th>IG</th>\n",
              "      <th>IN</th>\n",
              "      <th>IP</th>\n",
              "      <th>IT</th>\n",
              "      <th>JD</th>\n",
              "      <th>LT</th>\n",
              "      <th>LY</th>\n",
              "      <th>MT</th>\n",
              "      <th>NA</th>\n",
              "      <th>NE</th>\n",
              "      <th>OA</th>\n",
              "      <th>OB</th>\n",
              "      <th>OP</th>\n",
              "      <th>OS</th>\n",
              "      <th>PB</th>\n",
              "      <th>PO</th>\n",
              "      <th>QA</th>\n",
              "      <th>RA</th>\n",
              "      <th>RE</th>\n",
              "      <th>RP</th>\n",
              "      <th>RS</th>\n",
              "      <th>RV</th>\n",
              "      <th>SL</th>\n",
              "      <th>SP</th>\n",
              "      <th>SR</th>\n",
              "      <th>TB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7559</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7560</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7561</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7562</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7563</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7564 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AV  CB  CM  DF  DP  DS  DT  EB  EN  ...  RA  RE  RP  RS  RV  SL  SP  SR  TB\n",
              "0      0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "1      0   0   0   0   0   0   0   0   0  ...   0   0   0   1   0   0   0   0   0\n",
              "2      0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "3      0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   1   0\n",
              "4      0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
              "7559   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "7560   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "7561   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "7562   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "7563   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "\n",
              "[7564 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEmtJxJ-WQZ2",
        "colab_type": "code",
        "outputId": "03b2c22f-353b-48cd-91ef-2519cbe96c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "df =pd.concat([df, dummies], axis=1)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "      <th>regs</th>\n",
              "      <th>AV</th>\n",
              "      <th>CB</th>\n",
              "      <th>CM</th>\n",
              "      <th>DF</th>\n",
              "      <th>DP</th>\n",
              "      <th>DS</th>\n",
              "      <th>DT</th>\n",
              "      <th>EB</th>\n",
              "      <th>EN</th>\n",
              "      <th>FA</th>\n",
              "      <th>FC</th>\n",
              "      <th>FS</th>\n",
              "      <th>HA</th>\n",
              "      <th>HI</th>\n",
              "      <th>IB</th>\n",
              "      <th>ID</th>\n",
              "      <th>IG</th>\n",
              "      <th>IN</th>\n",
              "      <th>IP</th>\n",
              "      <th>IT</th>\n",
              "      <th>JD</th>\n",
              "      <th>LT</th>\n",
              "      <th>LY</th>\n",
              "      <th>MT</th>\n",
              "      <th>NA</th>\n",
              "      <th>NE</th>\n",
              "      <th>OA</th>\n",
              "      <th>OB</th>\n",
              "      <th>OP</th>\n",
              "      <th>OS</th>\n",
              "      <th>PB</th>\n",
              "      <th>PO</th>\n",
              "      <th>QA</th>\n",
              "      <th>RA</th>\n",
              "      <th>RE</th>\n",
              "      <th>RP</th>\n",
              "      <th>RS</th>\n",
              "      <th>RV</th>\n",
              "      <th>SL</th>\n",
              "      <th>SP</th>\n",
              "      <th>SR</th>\n",
              "      <th>TB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "      <td>[DS, IG]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[RS, OP]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "      <td>[NE, NA]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[SR, NA]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "      <td>[MT, OS]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data  ... SP  SR  TB\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train  ...  0   0   0\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train  ...  0   0   0\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train  ...  0   0   0\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train  ...  0   1   0\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train  ...  0   0   0\n",
              "\n",
              "[5 rows x 46 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJz7I5ZbmlOo",
        "colab_type": "code",
        "outputId": "6d353a68-0fc2-4428-d8fa-91adbae7d0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "subregs = np.setdiff1d(mlb.classes_, hiregs)\n",
        "subregs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AV', 'CB', 'CM', 'DF', 'DP', 'DS', 'DT', 'EB', 'EN', 'FA', 'FC',\n",
              "       'FS', 'HA', 'IB', 'IT', 'JD', 'LT', 'MT', 'NE', 'OA', 'OB', 'PB',\n",
              "       'PO', 'QA', 'RA', 'RE', 'RP', 'RS', 'RV', 'SL', 'SR', 'TB'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inhn9BbbcJTl",
        "colab_type": "code",
        "outputId": "7c63feee-92f6-41ea-9b1a-0b4319e2c7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# separate train, test and dev datas from one hot encoded combined dataset\n",
        "\n",
        "train = df[df['data'] == 'train'] \n",
        "test = df[df['data'] == 'test'] \n",
        "dev = df[df['data'] == 'dev'] \n",
        "\n",
        "# for grid search (maybe not needed/used)\n",
        "joint = df[df['data'] != 'test'] \n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(dev.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 46)\n",
            "(1513, 46)\n",
            "(756, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FCQ2W_8f5vV",
        "colab_type": "code",
        "outputId": "293a6dfe-cb27-422e-c4fd-cef4abfeeb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "# separate X (features) and Y (labels) for training data and x and y for development data\n",
        "\n",
        "# train\n",
        "X = train['text'] # features\n",
        "Y = train.drop(['regs', 'label','text', 'data'], axis=1) # labels\n",
        "\n",
        "# dev\n",
        "x = dev['text']\n",
        "y = dev.drop(['regs', 'label','text', 'data'], axis=1)\n",
        "\n",
        "# test\n",
        "test_feats = test['text']\n",
        "true_labels = test.drop(['regs', 'label','text', 'data'], axis=1) # true labels for testing the model predictuons\n",
        "true_regs = test['regs']\n",
        "\n",
        "true_regs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6051        [HI]\n",
              "6052        [NA]\n",
              "6053    [DT, IN]\n",
              "6054    [DP, IN]\n",
              "6055    [DT, IN]\n",
              "          ...   \n",
              "7559    [OB, OP]\n",
              "7560    [MT, OS]\n",
              "7561    [OA, NA]\n",
              "7562    [PB, NA]\n",
              "7563    [MT, OS]\n",
              "Name: regs, Length: 1513, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtm800nFKgNE",
        "colab_type": "text"
      },
      "source": [
        "## Linear support vector machine (SVM)\n",
        "\n",
        "SVM which is widely regarded as one of the best text classification algorithms (although it’s also a bit slower than naïve Bayes). https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "\n",
        "\n",
        "Scikit documentation https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier\n",
        "\n",
        "tutorial: https://www.datatechnotes.com/2020/03/multi-output-classification-with-multioutputclassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAIrQZ5JlyXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# form feature matrixes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features = 20000, ngram_range = (1,1)) \n",
        "\n",
        "train_feature_matrix = vectorizer.fit_transform(X)\n",
        "dev_feature_matrix = vectorizer.transform(x)\n",
        "test_feature_matrix = vectorizer.transform(test_feats)\n",
        "\n",
        "# for grid search:\n",
        "# XG = vectorizer.fit_transform(XG)\n",
        "\n",
        "# Scale input data for SVM\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "train_fm = scaler.fit_transform(train_feature_matrix)\n",
        "dev_fm = scaler.transform(dev_feature_matrix)\n",
        "test_fm = scaler.transform(test_feature_matrix)\n",
        "\n",
        "print(\"shape of the training data: \", train_fm.shape)\n",
        "print(\"shape of the development data: \", dev_fm.shape)\n",
        "# print(\"shape of the  data: \", XG.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UawS5wlqCTzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# guide for multiouput moldel for one hot encoded data: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "cat = Y.columns.values.tolist() # labels/registers\n",
        "\n",
        "SVM_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(SGDClassifier(loss='hinge', random_state=42, alpha=0.0001, max_iter=40, early_stopping=True))),\n",
        "            ])\n",
        "for category in cat:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    SVM_pipeline.fit(train_feature_matrix, Y[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = SVM_pipeline.predict(test_fm)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(true_labels[category], prediction)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F_UWKlUf9p6",
        "colab_type": "text"
      },
      "source": [
        "## Bert (multi-LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxV7Lhkwn8Tx",
        "colab_type": "text"
      },
      "source": [
        "Tutorial: https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsTCKCcthHT5",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9OdoQjFF4ZU",
        "colab_type": "text"
      },
      "source": [
        "#### Feature tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDISZvoOf8wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 1500\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWZ2Ni5dVq-",
        "colab_type": "code",
        "outputId": "365d77ab-96f8-4962-eaaf-90df7626db06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# To avoid OOM, truncate one hot encoded data\n",
        "\n",
        "train = truncate_data(train, MAX_EXAMPLES)\n",
        "dev = truncate_data(dev, MAX_EXAMPLES)\n",
        "test = truncate_data(test, MAX_EXAMPLES)\n",
        "\n",
        "frames = [train, dev, test]\n",
        "for d in frames:\n",
        "  print(d.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: truncating examples from 5295 to 1500\n",
            "Note: truncating examples from 1513 to 1500\n",
            "(1500, 46)\n",
            "(756, 46)\n",
            "(1500, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIR1xUNihUZw",
        "colab_type": "text"
      },
      "source": [
        "We need to convert our data into a format that BERT understands. Some utility functions are provided to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wWJ3csi8k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remember, INPUT_LENGTH is a parameter for make_model_inputs -function. \n",
        "# We also use BERT tokenizer here.\n",
        "\n",
        "X = make_model_inputs(train['text'])\n",
        "x = make_model_inputs(dev['text'])\n",
        "test_inp = make_model_inputs(test['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrvV2Xp1RJIn",
        "colab_type": "code",
        "outputId": "cd39bb2c-7aad-4542-b3d2-c54cc4207f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(X[0].shape)\n",
        "print(x[0].shape)\n",
        "print(test_inp[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 250)\n",
            "(756, 250)\n",
            "(1500, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9SmUCcoPt1",
        "colab_type": "code",
        "outputId": "a3d62457-fd75-4b74-ab2a-18f82e5b1521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Print some examples (looks the same as in milestone 2)\n",
        "print('Token indices:')\n",
        "print(X[0][:2])\n",
        "print('Decoded:')\n",
        "for i in X[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(X[1][:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices:\n",
            "[[  102 20410   719   328 15022 38416 45766  4192 25413 14442 10448  7814\n",
            "   4009 24085 20599 11165  1330 50013 27722 11025 21917 13129  3452  4834\n",
            "   4942  1099   255  4942  1099   255  2599   145 46150   203  1640 48326\n",
            "    142  7026 32871  9890   111 36800 13873   403  6534   119  9183   142\n",
            "   9659   912   922 19078 12655 50009   119 22670  6729   119 12082 27949\n",
            "    612 12191   111 44695  1388 44626  8905  1555  7411   142 15996 14280\n",
            "   3817  2304 12213  9598  5463 29231   142 29403  4004   111 22423  1138\n",
            "   4942  1099   255  2599   145 35083 10080   203  8905  3369  5514 13758\n",
            "   2834 11166   119   374  1354 10772   109  3679  3332  2214   111 13742\n",
            "    697  4942  1099  6916 45571  1248  4942  1099   255  2599   145  6821\n",
            "    142 46150   203 21984 10900   407 23909   111  2604   145  5016  8905\n",
            "   5828 20150   239 24975  1922  2955   142 35576  1305   106 43029   111\n",
            "  36800 24975   106   142 32334  1252 10366  2767  1445 12310  3305 13914\n",
            "  21984 10900   170   111 36192   269   704   166 35917  4942  1099   866\n",
            "  46594   120 38731   166 41465  5087  1444  6408 15878 14482   356 49878\n",
            "  33640   142   166 42082   677  9364 26547 44137 18887  1929 50062 21062\n",
            "    427   111 47330  1388   133   240  4442   462   193   257 38731   166\n",
            "  20818 33046  1883 21403  8700   111 47330  1388  4819  5741   481  4942\n",
            "   1099   255  2599   145 15149 30909 15674 20753  4876   407 28988   144\n",
            "    119   975  2500 29403   473   145  5412   142  4354  5886   111 45081\n",
            "    106   119  8355  1499   142  6697   333  7483  5962   103]\n",
            " [  102  2208  3043  3043 49155   187 21069   348 15497   308   101 43609\n",
            "  27819  3013  3013 11185 18895  3902   145 16616 19099  7751  1121 24888\n",
            "    119 13405 29882 22322   111 28277   145  2596   943  2106   119   683\n",
            "  27114 23350   236  2208  3043  3043 49155   187 21069   348 15497   308\n",
            "    101 43609 27819  3013  3013 11185 18895  3902   145 16616 19099  7751\n",
            "   1121 24888   119 13405 29882 22322   111 28277   145  2596   943  2106\n",
            "    119   683 27114 23350   236   166  3043  3043   555  3835   101   555\n",
            "    859  4711  3013  3013  1462 46032 30795 36222  9094   142  1462 18252\n",
            "   3904 43269   149 21260   483   111 31943 23015   913 15258  7986 10331\n",
            "    337 12356 42657  4337  5966 40321   119  4215  6166  5503  6378   111\n",
            "  17842  1462  4254 21390   106 30617 10233 21260 20502  1385   146   111\n",
            "   2177  3043  3043   555  3835   101   555   859  4711  3013  3013  1462\n",
            "  46032 30795 36222  9094   142  1462 18252  3904 43269   149 21260   483\n",
            "    111  3043  3043  7210 16554 21260   123   101 31943 23015  3013  3013\n",
            "    913 15258  7986 10331   337 12356 42657  4337  5966 40321   119  4215\n",
            "   6166  5503  6378   111 17842  1462  4254 21390   106 30617 10233 21260\n",
            "  20502  1385   146   111   166 36183 50050   145 11627   111   737 12665\n",
            "    142   145   534  3225  1229 19242  4299 16280   353   111   950  2464\n",
            "  34523 50017  4101   984 45002   269   119 10314   145  1660 34523 50017\n",
            "    142  1660 42429 50011   119   374  5107 29641   119  3043  3043  7528\n",
            "    348  2305   308   101  7528   492  3013  3013   142   103]]\n",
            "Decoded:\n",
            "['Log', '##isti', '##ikka', 'Jenni', 'Lindholm', 'Lasku', '##tus', 'Ritva', 'Lie', '##vonen', 'Ota', 'yhteyttä', 'Nimi', 'Puhelin', '##nu', '##mer', '##o', 'Sähköposti', 'Yritys', 'Viesti', 'Sul', '##je', 'x', 'Sip', '##ari', '##la', 'Sip', '##ari', '##la', 'Oy', 'on', 'palvele', '##va', 'puut', '##oimittaja', 'ja', 'pinta', '##käsittelyn', 'mestari', '.', 'Tarjoamme', 'asiakkaille', '##mme', 'tuotteita', ',', 'ratkaisuja', 'ja', 'palveluja', 'ulko', '##ver', '##hoi', '##luu', '##n', ',', 'sisus', '##tukseen', ',', 'sauna', '##tiloihin', 'sekä', 'pihalle', '.', 'Inno', '##st', '##umme', 'puun', 'mahdollis', '##uuksista', 'ja', 'kehit', '##ämme', 'jatkuvasti', 'uusia', 'tapoja', 'hyödyntää', 'puuta', 'rakentamisessa', 'ja', 'sisusta', '##misessa', '.', 'Vastuu', '##llisuus', 'Sip', '##ari', '##la', 'Oy', 'on', 'perinteitä', 'kunnioitta', '##va', 'puun', '##jal', '##ostus', '##teollisuuden', 'perhe', '##yritys', ',', 'joka', 'ottaa', 'toiminnassa', '##an', 'huomioon', 'vastuu', '##llisuuden', '.', 'Tö', '##ihin', 'Sip', '##ari', '##laan', 'Ammattila', '##isille', 'Sip', '##ari', '##la', 'Oy', 'on', 'kokenut', 'ja', 'palvele', '##va', 'puutu', '##otte', '##iden', 'valmistaja', '.', 'Meillä', 'on', 'kokemusta', 'puun', 'käytöstä', 'vaativ', '##issa', 'arkkitehti', '##koh', '##teissa', 'ja', 'unelmien', 'koti', '##en', 'toteuttamisesta', '.', 'Tarjoamme', 'arkkitehti', '##en', 'ja', 'projektia', '##sia', '##kkaiden', 'käyttöön', 'oman', 'asiantunte', '##mu', '##ksemme', 'puutu', '##otte', '##ista', '.', 'Suorit', '##usta', '##so', '-', 'ilmoitukset', 'Sip', '##ari', '##lan', 'paneel', '##it', 'CE', '-', 'merkitään', 'yhtenä', '##iste', '##tyn', 'massiiv', '##ipu', '##up', '##ane', '##eleita', 'ja', '-', 'verho', '##uksia', 'koskevan', 'eurooppalaisen', 'standardin', 'EN', '14', '##9', '##15', 'mukaan', '.', 'Sisu', '##st', '##us', '##li', '##sto', '##ille', 'ei', 'ole', 'CE', '-', 'merkintä', 'vaatimusta', 'Euroopan', 'Unionin', 'toimesta', '.', 'Sisu', '##st', '##usko', '##ht', '##eet', 'Sip', '##ari', '##la', 'Oy', 'on', 'luonut', 'kattavan', 'sisustus', '##pane', '##ele', '##iden', 'mallist', '##on', ',', 'jonka', 'avulla', 'sisusta', '##minen', 'on', 'helppoa', 'ja', 'täynnä', 'mahdollisuuksia', '.', 'Väri', '##en', ',', 'pin', '##tojen', 'ja', 'muoto', '##jen', 'yhdistä', '##misessä']\n",
            "['Tässä', '[', '[', 'Ortodoks', '##inen', 'seminaari', '(', 'Joensuu', ')', '[UNK]', 'ortodoksisen', 'seminaarin', ']', ']', 'kirkossa', 'Joensuussa', 'joulu', 'on', 'kuvattu', 'suureen', 'maala', '##ukseen', 'kattoon', ',', 'tuonne', 'pääni', 'yläpuolelle', '.', 'Kuvassa', 'on', 'useita', 'eri', 'asioita', ',', 'jotka', 'kaipaavat', 'selitystä', ':', 'Tässä', '[', '[', 'Ortodoks', '##inen', 'seminaari', '(', 'Joensuu', ')', '[UNK]', 'ortodoksisen', 'seminaarin', ']', ']', 'kirkossa', 'Joensuussa', 'joulu', 'on', 'kuvattu', 'suureen', 'maala', '##ukseen', 'kattoon', ',', 'tuonne', 'pääni', 'yläpuolelle', '.', 'Kuvassa', 'on', 'useita', 'eri', 'asioita', ',', 'jotka', 'kaipaavat', 'selitystä', ':', '-', '[', '[', 'En', '##keli', '[UNK]', 'En', '##kel', '##eistä', ']', ']', 'toinen', 'ylistää', 'ihmiseksi', 'tullutta', 'Jumalaa', 'ja', 'toinen', 'julistaa', 'ilo', '##sanom', '##aa', 'paimen', '##elle', '.', 'Pai', '##menet', 'taas', 'edustavat', 'tavallista', 'väkeä', 'tai', 'pikemminkin', 'eräällä', 'tavoin', 'yhteiskunnan', 'reunalla', ',', 'ellei', 'peräti', 'ulkopuolella', 'olevia', '.', 'Heistä', 'toinen', 'liittyy', 'enkeli', '##en', 'ylis', '##tykseen', 'paimen', '##hui', '##lu', '##lla', '.', '+', '[', '[', 'En', '##keli', '[UNK]', 'En', '##kel', '##eistä', ']', ']', 'toinen', 'ylistää', 'ihmiseksi', 'tullutta', 'Jumalaa', 'ja', 'toinen', 'julistaa', 'ilo', '##sanom', '##aa', 'paimen', '##elle', '.', '[', '[', 'Kristuksen', 'nähneet', 'paimen', '##et', '[UNK]', 'Pai', '##menet', ']', ']', 'taas', 'edustavat', 'tavallista', 'väkeä', 'tai', 'pikemminkin', 'eräällä', 'tavoin', 'yhteiskunnan', 'reunalla', ',', 'ellei', 'peräti', 'ulkopuolella', 'olevia', '.', 'Heistä', 'toinen', 'liittyy', 'enkeli', '##en', 'ylis', '##tykseen', 'paimen', '##hui', '##lu', '##lla', '.', '-', 'Joose', '##f', 'on', 'sivussa', '.', 'Hän', 'epäilee', 'ja', 'on', 'itse', 'asiassa', 'pah', '##olaisen', 'kiusa', '##ttava', '##na', '.', 'Kun', 'ensimmäinen', 'Aada', '##m', 'tehtiin', 'maan', 'tom', '##usta', ',', 'Kristus', 'on', 'uusi', 'Aada', '##m', 'ja', 'uusi', 'luomu', '##s', ',', 'joka', 'syntyy', 'taivaasta', ',', '[', '[', 'Isä', '(', 'Jumala', ')', '[UNK]', 'Isä', '##stä', ']', ']', 'ja']\n",
            "Segment ids:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpqqKGfeQdl",
        "colab_type": "text"
      },
      "source": [
        "Extract one hot label encodings from the truncated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5SgLu9xePxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "Y = train.drop(['regs', 'label','text', 'data'], axis=1)\n",
        "\n",
        "# dev\n",
        "y = dev.drop(['regs','label','text', 'data'], axis=1)\n",
        "\n",
        "# test\n",
        "true_labels = test.drop(['regs', 'label','text', 'data'], axis=1).reset_index(drop=True) # these are used for evaluating model predictions\n",
        "true_regs = test['regs'].reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ze-mp5bpO5",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyz0YK_kbtBU",
        "colab_type": "text"
      },
      "source": [
        "This is the same as in milestone 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwQmd2F_boLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 40\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.00001\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 16 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sypxw1aJbyix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the optimizer again if hyperparameters above have been changed from milestone 2\n",
        "\n",
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train_['text']),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaQNy6Uptzw",
        "colab_type": "text"
      },
      "source": [
        "### Wrap the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eZhMgp9N7KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False, # ignore MLM [masked language modeling] and NSP [next sentence prediction] parts of the model\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH # define the size of input layer\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAyXXiqCpeG5",
        "colab_type": "code",
        "outputId": "0a052054-c2ec-4ea9-bf76-24964d533f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pretrained_model.inputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Input-Token_2:0' shape=(?, 250) dtype=float32>,\n",
              " <tf.Tensor 'Input-Segment_2:0' shape=(?, 250) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_iquZgFqmZB",
        "colab_type": "code",
        "outputId": "2810f6ea-a992-4115-f8b3-ea9120100f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "pretrained_model.output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Encoder-12-FeedForward-Norm_2/add_1:0' shape=(?, 250, 768) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJsIwzVmukO7",
        "colab_type": "text"
      },
      "source": [
        "Those dimensions are (minibatch-size, sequence-length, hidden-dim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS7bRChHubSD",
        "colab_type": "code",
        "outputId": "83290ca5-92ae-4edd-ec9f-05906d4d3fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# model.outputs is a list, here with a single item. Here\n",
        "# pretrained_model.outputs[0] just grabs that item (the output tensor).\n",
        "# Indxing that tensor with [:,0] gives the first position in the sequence\n",
        "# for all elements in the batch (the `:`).\n",
        "\n",
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice_2:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvbwRaMgE35m",
        "colab_type": "text"
      },
      "source": [
        "Wrap the pretrained model. **In multi-label classification instead of softmax(), we use sigmoid() to get the probabilities.** (https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P3irmGsE2wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labels = len(mlb.classes_) # size of the output layer\n",
        "\n",
        "out = Dense(num_labels, activation='sigmoid')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
        "model = Model(                                          # sigmoid gives us independent probabilities for each class \n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9_cBMhQW-df",
        "colab_type": "code",
        "outputId": "b7479fea-1665-4725-b3b0-91561f75a109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense_2/Sigmoid:0' shape=(?, 42) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OouJ_0jzYw1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy', # one hot encoded labels! Is hamming_loss better?\n",
        "    metrics=['categorical_accuracy', 'weighted_binary_crossentropy', 'binary_accuracy'] #metrics=['binary_accuracy', 'categorical_accuracy'] \n",
        ")\n",
        "\n",
        "# loss functions tried: categorical_crossentropy < binary_crossentropy \n",
        "# ordered by test accuracy (treshold set to 0.9) from worst to best\n",
        "\n",
        "# categorical_crossentropy: Classification accuracy:  17.7 percent\n",
        "\n",
        "# An important choice to make is the loss function. We use the binary_crossentropy loss and not the usual in multi-class classification \n",
        "# used categorical_crossentropy loss. This might seem unreasonable, but we want to penalize each output node independently. So we pick \n",
        "# a binary loss and model the output of the network as a independent Bernoulli distributions per label.\n",
        "# source: https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIeGmqi-iXGL",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSwPhGPMZFqU",
        "colab_type": "code",
        "outputId": "6dd28e22-b570-4dc4-9d5b-a2fba0177067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_categorical_accuracy', patience= 20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "mc_cb = ModelCheckpoint(filepath='models/BERT_multilabel.h5', monitor='val_categorical_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x, y),\n",
        "    callbacks=[stop_cb, mc_cb]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 756 samples\n",
            "Epoch 1/40\n",
            "1500/1500 [==============================] - 161s 108ms/sample - loss: 0.5951 - categorical_accuracy: 0.0607 - val_loss: 0.2106 - val_categorical_accuracy: 0.1997\n",
            "Epoch 2/40\n",
            "1500/1500 [==============================] - 64s 43ms/sample - loss: 0.1706 - categorical_accuracy: 0.2860 - val_loss: 0.1424 - val_categorical_accuracy: 0.3294\n",
            "Epoch 3/40\n",
            "1500/1500 [==============================] - 64s 43ms/sample - loss: 0.1341 - categorical_accuracy: 0.3547 - val_loss: 0.1182 - val_categorical_accuracy: 0.4140\n",
            "Epoch 4/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.1103 - categorical_accuracy: 0.3907 - val_loss: 0.0977 - val_categorical_accuracy: 0.3862\n",
            "Epoch 5/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0935 - categorical_accuracy: 0.4220 - val_loss: 0.0883 - val_categorical_accuracy: 0.3505\n",
            "Epoch 6/40\n",
            "1500/1500 [==============================] - 64s 43ms/sample - loss: 0.0810 - categorical_accuracy: 0.4327 - val_loss: 0.0809 - val_categorical_accuracy: 0.4312\n",
            "Epoch 7/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0700 - categorical_accuracy: 0.4793 - val_loss: 0.0787 - val_categorical_accuracy: 0.4167\n",
            "Epoch 8/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0610 - categorical_accuracy: 0.4773 - val_loss: 0.0764 - val_categorical_accuracy: 0.3995\n",
            "Epoch 9/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0528 - categorical_accuracy: 0.5127 - val_loss: 0.0760 - val_categorical_accuracy: 0.3743\n",
            "Epoch 10/40\n",
            "1500/1500 [==============================] - 64s 43ms/sample - loss: 0.0453 - categorical_accuracy: 0.5220 - val_loss: 0.0762 - val_categorical_accuracy: 0.4550\n",
            "Epoch 11/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0385 - categorical_accuracy: 0.5607 - val_loss: 0.0779 - val_categorical_accuracy: 0.4471\n",
            "Epoch 12/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0326 - categorical_accuracy: 0.5760 - val_loss: 0.0775 - val_categorical_accuracy: 0.4272\n",
            "Epoch 13/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0280 - categorical_accuracy: 0.5727 - val_loss: 0.0774 - val_categorical_accuracy: 0.4259\n",
            "Epoch 14/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0245 - categorical_accuracy: 0.5707 - val_loss: 0.0795 - val_categorical_accuracy: 0.3929\n",
            "Epoch 15/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0214 - categorical_accuracy: 0.5627 - val_loss: 0.0789 - val_categorical_accuracy: 0.4021\n",
            "Epoch 16/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0190 - categorical_accuracy: 0.5800 - val_loss: 0.0796 - val_categorical_accuracy: 0.4021\n",
            "Epoch 17/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0170 - categorical_accuracy: 0.5673 - val_loss: 0.0804 - val_categorical_accuracy: 0.3889\n",
            "Epoch 18/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0154 - categorical_accuracy: 0.5633 - val_loss: 0.0809 - val_categorical_accuracy: 0.4246\n",
            "Epoch 19/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0141 - categorical_accuracy: 0.5720 - val_loss: 0.0817 - val_categorical_accuracy: 0.4114\n",
            "Epoch 20/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0130 - categorical_accuracy: 0.5567 - val_loss: 0.0819 - val_categorical_accuracy: 0.3915\n",
            "Epoch 21/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0122 - categorical_accuracy: 0.5560 - val_loss: 0.0818 - val_categorical_accuracy: 0.3876\n",
            "Epoch 22/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0113 - categorical_accuracy: 0.5773 - val_loss: 0.0822 - val_categorical_accuracy: 0.4246\n",
            "Epoch 23/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0104 - categorical_accuracy: 0.5653 - val_loss: 0.0827 - val_categorical_accuracy: 0.3823\n",
            "Epoch 24/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0097 - categorical_accuracy: 0.5580 - val_loss: 0.0829 - val_categorical_accuracy: 0.4008\n",
            "Epoch 25/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0091 - categorical_accuracy: 0.5560 - val_loss: 0.0829 - val_categorical_accuracy: 0.4034\n",
            "Epoch 26/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0087 - categorical_accuracy: 0.5460 - val_loss: 0.0835 - val_categorical_accuracy: 0.3995\n",
            "Epoch 27/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0083 - categorical_accuracy: 0.5533 - val_loss: 0.0843 - val_categorical_accuracy: 0.4021\n",
            "Epoch 28/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0079 - categorical_accuracy: 0.5620 - val_loss: 0.0843 - val_categorical_accuracy: 0.3862\n",
            "Epoch 29/40\n",
            "1500/1500 [==============================] - 59s 39ms/sample - loss: 0.0076 - categorical_accuracy: 0.5587 - val_loss: 0.0838 - val_categorical_accuracy: 0.3955\n",
            "Epoch 30/40\n",
            "1488/1500 [============================>.] - ETA: 0s - loss: 0.0072 - categorical_accuracy: 0.5531Restoring model weights from the end of the best epoch.\n",
            "1500/1500 [==============================] - 63s 42ms/sample - loss: 0.0072 - categorical_accuracy: 0.5540 - val_loss: 0.0850 - val_categorical_accuracy: 0.3929\n",
            "Epoch 00030: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtoBHqFyiM7B",
        "colab_type": "code",
        "outputId": "c7d3e202-f09c-4e84-a405-1d9a030214b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb8tvqruaIJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_inp, batch_size = BATCH_SIZE) # default batch size for predictiong is 32. See Keras documentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNLiSNOrj3f_",
        "colab_type": "code",
        "outputId": "fe5f3451-6f1c-4580-f41f-153f06562e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00810441 0.00992897 0.01294625 ... 0.00602433 0.01629329 0.01409259]\n",
            " [0.00340098 0.07013294 0.00326282 ... 0.00826952 0.01665112 0.05507287]\n",
            " [0.00539836 0.03464574 0.04505533 ... 0.01845092 0.00163728 0.00728306]\n",
            " ...\n",
            " [0.00862765 0.0179134  0.0072757  ... 0.00352192 0.00457296 0.00586751]\n",
            " [0.01063564 0.00367984 0.00471219 ... 0.00378877 0.00338483 0.01035708]\n",
            " [0.00480363 0.00806594 0.01716143 ... 0.00360712 0.02302313 0.01843941]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X7dkit5l-ar",
        "colab_type": "code",
        "outputId": "9b5706a2-3b45-498e-b2b5-40996f6d4d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(sum(predictions[0])) # since we do multiclass, sum of probabilities is over 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.87018683552742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q6hvoMzkRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://datascience.stackexchange.com/questions/25752/how-does-keras-calculate-accuracy-for-multi-label-classification\n",
        "# TRY THIS OUT!\n",
        "\n",
        "score, acc = model.evaluate(test_inp, true_labels,\n",
        "                            batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yauRbUmrRwQ",
        "colab_type": "code",
        "outputId": "39971805-aaa9-447f-9ec5-c2240b6a46d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Should we set some treshold to accept label?\n",
        "# How to select an appropriate threshold for the model? This is normally done by optimizing the threshold on a holdout set. \n",
        "# According to https://www.depends-on-the-definition.com/unet-keras-segmenting-images/ No instrucion provided how to do it!\n",
        "\n",
        "preds = (predictions > 0.50).astype(np.uint8) # set the limit for reasonable/acceptable probability for belonging to a class\n",
        "print(\"Predictions are binary sequences:\", preds[1]) \n",
        "\n",
        "predicted_labels = mlb.inverse_transform(preds) # inverse transform gives the registers\n",
        "print(\"Inverse transform gives the predicted register name(s):\" ,predicted_labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions are binary sequences: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0]\n",
            "Inverse transform gives the predicted register name(s): ('HI',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l49oGfGjlys",
        "colab_type": "code",
        "outputId": "9fbac91f-894b-4fb5-ba03-9ad3b3740f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification accuracy: \", round(accuracy_score(true_labels, preds)*100,1), \"percent\") # for accuracy score we use here whole binary sequence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  47.3 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJzHfiBuKTee",
        "colab_type": "text"
      },
      "source": [
        "What are the measure for accuracy of multilabel data? https://stats.stackexchange.com/questions/12702/what-are-the-measure-for-accuracy-of-multilabel-data\n",
        "\n",
        "\"When considering the multi label use case, you should decide how to extend accuracy to this case. The method choose in hamming loss was to give each label equal weight. One could use other methods (e.g., taking the maximum).\n",
        "\n",
        "Since hamming loss is designed for multi class while Precision, Recall, F1-Measure are designed for the binary class, it is better to compare the last one to Accuracy. In general, there is no magical metric that is the best for every problem. In every problem you have different needs, and you should optimize for them.\" https://stats.stackexchange.com/questions/336820/what-is-a-hamming-loss-will-we-consider-it-for-an-imbalanced-binary-classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdM5H0ac-bz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true_labels.iloc[0] == preds[0] # binäärit vastaavat toisiaan!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKoLjCkSwcPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is NOT CORRECT!\n",
        "# for register in mlb.classes_:\n",
        "#   idx = 0\n",
        "#   # extract predictions for register from result matrix\n",
        "#   column = []\n",
        "#   for row in preds:\n",
        "#     column.append(row[idx])\n",
        "#   success = sum(true_labels[register] == column) # for how many input the prediction is correct? NONONOOOOO!\n",
        "#   acc = round(success/len(true_labels)*100, 1)\n",
        "#   print('For register {}'.format(register), 'test accuracy is', acc)\n",
        "#   idx = idx+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1bZmgfNnWlm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 3.2: Model comparison\n",
        "Compare the results of these two classifiers. Do the two models predict in the same way? Analyze the predictions in terms of label-specific differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rcigcDRKKCe",
        "colab_type": "text"
      },
      "source": [
        "as mentioned here, the accuracy is ambiguous in the multiple-label case.\n",
        "\n",
        "The HL thus presents one clear single-performance-value for multiple-label case in contrast to the precision/recall/f1 that can be evaluated only for independent binary classifiers for each label."
      ]
    }
  ]
}