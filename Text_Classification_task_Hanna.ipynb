{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Text_Classification_task.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/priva_DL_HLT/blob/master/Text_Classification_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VGkCcicEVPSF"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "This project is about text classification. You will develop a text classification system that identifies different kinds of online texts, such as news, blogs and opinionated texts. We will refer to these text categories as registers. If you want to learn more about online registers and their automatic identification, you can read, e.g., our paper [Toward Multilingual Identification of Online Registers] (https://www.aclweb.org/anthology/W19-6130/).\n",
        "\n",
        "# Data and register labels\n",
        "The data for this project consist of ~7500 documents with manual annotations on their register. You can download it from http://dl.turkunlp.org/TKO_8965-projects/classification/ . The documents are based on a (almost) random sample of the Finnish Internet. The registers are identified using a relatively detailed, hierarchical taxonomy. The taxonomy consists of 8 main categories that are divided into a large number of subregisters. The taxonomy is described at the end of this page. The table includes also the abbreviations that are used in the data.\n",
        "\n",
        "The challenge with online documents is that it is not always easy to identify the specific registers categories of the documents. Furthermore, another issue is that a document may display characteristics of several registers. For instance, a blog post may simultaneously seem like a product review. To deal with these challenges, we have followed the following guidelines:\n",
        "* For each document, the annotators have aimed at marking the specific subregister category. When this is possible, the document has two register labels: the subregister label and the main register label to which the subregister belongs. For instance, a document annotated as a news article would have the label NE for News and the corresponding higher level register label NA for Narrative. \n",
        "* In some cases, the document does not seem to fit any of the subregisters. In this case, the document can be given only one label: the main register label, such as NA for Narrative. \n",
        "* Some documents may display characteristics of several register categories. In this case, the annotator can mark several register labels for one single document. Consequently, the document may have up to four labels. This would be the case case if a document is annotated both as a Personal blog (subregister label PB + corresponding higher level register label NA) and Review (subregister label RV + corresponding higher level register label OP).\n",
        "\n",
        "\n",
        "# Register classes and abbreviations\n",
        "\n",
        "NA Narrative\n",
        "\n",
        "* NE NA    New reports / news blogs\n",
        "* SR NA    Sports reports\n",
        "* PB NA    Personal blog\n",
        "* HA NA    Historical article\n",
        "* FC NA    Fiction\n",
        "* TB NA    Travel blog\n",
        "* CB NA    Community blogs\n",
        "* OA NA    Online article\n",
        "\n",
        "OP  Opinion\n",
        "* OB OP  Personal opinion blogs\n",
        "* RV OP  Reviews\n",
        "* RS OP  Religious blogs/sermons\n",
        "* AV OP  Advice\n",
        "\n",
        "IN Informational description\n",
        "* JD IN  Job description\n",
        "* FA IN  FAQs\n",
        "* DT IN  Description of a thing\n",
        "* IB IN  Information blogs\n",
        "* DP IN  Description of a person\n",
        "* RA IN  Research articles\n",
        "* LT IN  Legal terms / conditions\n",
        "* CM IN  Course materials\n",
        "* EN IN  Encyclopedia articles\n",
        "* RP IN  Report\n",
        "\n",
        "ID Interactive discussion\n",
        "* DF ID  Discussion forums\n",
        "* QA ID  Question-answer forums\n",
        "\n",
        "HI  How-to/instructions\n",
        "* RE HI  Recipes\n",
        "\n",
        "IP IG  Informational persuasion\n",
        "* DS IG  Description with intent to sell\n",
        "* EB IG  News-opinion blogs / editorials\n",
        "\n",
        "Lyrical LY\n",
        "* PO LY  Poems\n",
        "* SL LY  Songs\n",
        "\n",
        "Spoken SP\n",
        "* IT SP Interviews\n",
        "* FS SP Formal speeches\n",
        "\n",
        "Others OS\n",
        "* MT OS Machine-translated / generated texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1_c9eZnWlG",
        "colab_type": "text"
      },
      "source": [
        "# Preparations\n",
        "\n",
        "Download and open data, explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsWUzLj5NOG",
        "colab_type": "code",
        "outputId": "0333ee1a-9c07-4b99-d8b9-5b7932b6ae91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get rid of old tf at some point!\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# to run with old tf with which the code was made\n",
        "# The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.\n",
        "# https://colab.research.google.com/notebooks/tensorflow_version.ipynb\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHGSBx5nWlM",
        "colab_type": "code",
        "outputId": "399a7662-3391-42fc-e94b-1479681290cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "# Download development data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
        "# Download test data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
        "# Download train data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 15:56:02--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4035578 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘fincore-dev.tsv’\n",
            "\n",
            "fincore-dev.tsv     100%[===================>]   3.85M  4.29MB/s    in 0.9s    \n",
            "\n",
            "2020-04-20 15:56:03 (4.29 MB/s) - ‘fincore-dev.tsv’ saved [4035578/4035578]\n",
            "\n",
            "--2020-04-20 15:56:05--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8512687 (8.1M) [application/octet-stream]\n",
            "Saving to: ‘fincore-test.tsv’\n",
            "\n",
            "fincore-test.tsv    100%[===================>]   8.12M  6.40MB/s    in 1.3s    \n",
            "\n",
            "2020-04-20 15:56:07 (6.40 MB/s) - ‘fincore-test.tsv’ saved [8512687/8512687]\n",
            "\n",
            "--2020-04-20 15:56:10--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29379580 (28M) [application/octet-stream]\n",
            "Saving to: ‘fincore-train.tsv’\n",
            "\n",
            "fincore-train.tsv   100%[===================>]  28.02M  11.5MB/s    in 2.4s    \n",
            "\n",
            "2020-04-20 15:56:12 (11.5 MB/s) - ‘fincore-train.tsv’ saved [29379580/29379580]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8TezCY6pR1",
        "colab_type": "text"
      },
      "source": [
        "Data split\n",
        "  - Train data - all training based on it (this includes the vectorizer!)\n",
        "  - Development data - set the parameters (a.k.a validation data set)\n",
        "  - Test data - used for nothing during training, produce final results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KciwyumAEz_b",
        "colab_type": "code",
        "outputId": "8104f282-383c-4226-b072-7554cd090d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('fincore-train.tsv', sep='\\t', header=None)\n",
        "\n",
        "train = train.sample(frac=1, random_state = 4) # suffle the data\n",
        "train.columns = ['label','text']\n",
        "print(train.head())\n",
        "print(train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       label                                               text\n",
            "3982  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "2640  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...\n",
            "119   NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...\n",
            "4916  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...\n",
            "775   MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...\n",
            "(5295, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4KLkfAf7UB_",
        "colab_type": "code",
        "outputId": "29672a95-3800-48d8-87bd-0e7dcad0028c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "dev = pd.read_csv('fincore-dev.tsv', sep='\\t', header=None)\n",
        "dev.columns = ['label','text']\n",
        "print(dev.head())\n",
        "print(dev.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    label                                               text\n",
            "0  OA NA    Luonnonhoito Maaperän siemenpankkia avattiin ...\n",
            "1  DS IG    • Jokainen ripsi on erittäin kevyt ja muodolt...\n",
            "2  DS IG    Mukavuudet Hotel Dila Vain muutaman metrin pä...\n",
            "3  DF ID    Vastaa viestiin Otsikko Viesti ensin omaishoi...\n",
            "4  OA NA    Dinosaur Jr 30.5.2010 Tavastia , Helsinki 198...\n",
            "(756, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrEJSdgyZJ8",
        "colab_type": "code",
        "outputId": "a7b761ac-29d0-4bee-e037-5d56a7c52a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "test = pd.read_csv('fincore-test.tsv', sep='\\t', header=None)\n",
        "test.columns = ['label','text']\n",
        "print(test.head())\n",
        "print(test.shape)\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    label                                               text\n",
            "0    HI     Tehkää nollaleimaus . Jos rekisteröinti onnis...\n",
            "1    NA     1 kommenttia : Syyslomallelähtijät kirjoitti ...\n",
            "2  DT IN    Ammattikoulutuksen perustana on ajatus siitä ...\n",
            "3  DP IN    Ulkonäkö : Silveriä voisi kuvata tietyllä tap...\n",
            "4  DT IN    Laulupelimannien puheenjohtajina ovat toimine...\n",
            "(1513, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwxqapN9J7PH",
        "colab_type": "code",
        "outputId": "e6e5ed08-78c0-4953-cac2-47734926f87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Prepare stratified data sets for training, development and testing:\n",
        "# Stratification aims to ensure that all the data sets (train, development and test) have the same distribution of labels. \n",
        "# This minimizes chances that a model has to try to predict labes it has not seen during training.\n",
        "\n",
        "# Error: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
        "# Best solution: More data\n",
        "# Second best solution: If you cannot have another dataset, you will have to play with what you have. I would suggest you remove the sample that has the lonely target. \n",
        "# So you will have a model which does not cover that target. If that does not fit you requirements, you need a new dataset.\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from pprint import pprint\n",
        "\n",
        "# Join all the data and re-divide it with stratification\n",
        "\n",
        "frames = [train, dev, test]\n",
        "data = pd.concat(frames)\n",
        "\n",
        "# Separating out the target\n",
        "y = data['label'] # pd df\n",
        "# # Separating out the features\n",
        "X = data['text'] # pd df\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "nums = dict(zip(unique, counts))\n",
        "\n",
        "pprint(sorted(nums.items(), key = lambda kv:(kv[1], kv[0])))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('AV OP DS IG ', 1),\n",
            " ('CB NA EB IG ', 1),\n",
            " ('CB NA HI  ', 1),\n",
            " ('CB NA IP IG ', 1),\n",
            " ('CB NA RV OP ', 1),\n",
            " ('DF ID NE NA ', 1),\n",
            " ('DF ID PB NA ', 1),\n",
            " ('DF ID RE HI ', 1),\n",
            " ('DP IN CB NA ', 1),\n",
            " ('DP IN EN IN ', 1),\n",
            " ('DP IN IP IG ', 1),\n",
            " ('DP IN NA  ', 1),\n",
            " ('DS IG AV OP ', 1),\n",
            " ('DS IG IN  ', 1),\n",
            " ('DS IG MT OS ', 1),\n",
            " ('DS IG OB OP ', 1),\n",
            " ('DT IN AV OP ', 1),\n",
            " ('DT IN FC NA ', 1),\n",
            " ('DT IN NE NA ', 1),\n",
            " ('DT IN RV OP ', 1),\n",
            " ('EN IN IP IG ', 1),\n",
            " ('FC NA DP IN ', 1),\n",
            " ('FC NA DT IN ', 1),\n",
            " ('FC NA ID  ', 1),\n",
            " ('HA NA DP IN ', 1),\n",
            " ('HA NA PB NA ', 1),\n",
            " ('HI  EN IN ', 1),\n",
            " ('HI  IP IG ', 1),\n",
            " ('HI  LT IN ', 1),\n",
            " ('HI  NE NA ', 1),\n",
            " ('IB IN CB NA ', 1),\n",
            " ('IB IN IN  ', 1),\n",
            " ('ID  PB NA ', 1),\n",
            " ('IN  HI  ', 1),\n",
            " ('IP IG NE NA ', 1),\n",
            " ('IT SP DT IN ', 1),\n",
            " ('MT OS EN IN ', 1),\n",
            " ('NE NA DT IN ', 1),\n",
            " ('OA NA DP IN ', 1),\n",
            " ('OA NA FC NA ', 1),\n",
            " ('OB OP DT IN ', 1),\n",
            " ('OB OP RE HI ', 1),\n",
            " ('OP  DP IN ', 1),\n",
            " ('PB NA DS IG ', 1),\n",
            " ('PB NA IB IN ', 1),\n",
            " ('RE HI PB NA ', 1),\n",
            " ('RV OP DS IG ', 1),\n",
            " ('RV OP PB NA ', 1),\n",
            " ('SL LY ', 1),\n",
            " ('SR NA DS IG ', 1),\n",
            " ('TB NA CB NA ', 1),\n",
            " ('CB NA SR NA ', 2),\n",
            " ('DF ID HI  ', 2),\n",
            " ('DP IN DS IG ', 2),\n",
            " ('DS IG RV OP ', 2),\n",
            " ('EN IN HI  ', 2),\n",
            " ('IB IN AV OP ', 2),\n",
            " ('OA NA OP  ', 2),\n",
            " ('OA NA RV OP ', 2),\n",
            " ('PB NA RV OP ', 2),\n",
            " ('PO LY RV OP ', 2),\n",
            " ('HA NA DS IG ', 3),\n",
            " ('HI  DS IG ', 3),\n",
            " ('HI  DT IN ', 3),\n",
            " ('MT OS DT IN ', 3),\n",
            " ('NE NA RV OP ', 3),\n",
            " ('DP IN FC NA ', 4),\n",
            " ('EN IN FC NA ', 4),\n",
            " ('ID  FC NA ', 4),\n",
            " ('CB NA DS IG ', 5),\n",
            " ('DS IG HI  ', 5),\n",
            " ('DT IN HI  ', 5),\n",
            " ('OA NA HI  ', 5),\n",
            " ('PB NA RE HI ', 5),\n",
            " ('RV OP MT OS ', 5),\n",
            " ('MT OS DS IG ', 6),\n",
            " ('NE NA HI  ', 7),\n",
            " ('OA NA DS IG ', 7),\n",
            " ('PO LY ', 10),\n",
            " ('NE NA IP IG ', 13),\n",
            " ('DF ID FC NA ', 15),\n",
            " ('FS SP ', 15),\n",
            " ('AV OP ', 17),\n",
            " ('JD IN ', 20),\n",
            " ('RE HI ', 20),\n",
            " ('FA IN ', 22),\n",
            " ('OP  ', 22),\n",
            " ('IT SP ', 30),\n",
            " ('FC NA ', 36),\n",
            " ('HA NA ', 36),\n",
            " ('IP IG ', 37),\n",
            " ('CM IN ', 44),\n",
            " ('TB NA ', 44),\n",
            " ('RA IN ', 48),\n",
            " ('EB IG ', 50),\n",
            " ('QA ID ', 57),\n",
            " ('IB IN ', 61),\n",
            " ('RP IN ', 63),\n",
            " ('LT IN ', 68),\n",
            " ('DP IN ', 83),\n",
            " ('NA  ', 86),\n",
            " ('NE NA DS IG ', 87),\n",
            " ('ID  ', 100),\n",
            " ('IN  ', 130),\n",
            " ('MT OS RV OP ', 146),\n",
            " ('EN IN ', 170),\n",
            " ('RV OP ', 193),\n",
            " ('SR NA ', 207),\n",
            " ('CB NA ', 218),\n",
            " ('OB OP ', 233),\n",
            " ('HI  ', 270),\n",
            " ('OA NA ', 287),\n",
            " ('RS OP ', 308),\n",
            " ('DT IN ', 394),\n",
            " ('DF ID ', 437),\n",
            " ('DS IG ', 718),\n",
            " ('PB NA ', 789),\n",
            " ('NE NA ', 875),\n",
            " ('MT OS ', 957)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYl-1OgZIh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d70df4c-72c1-4bea-dde4-3a04b57dcd16"
      },
      "source": [
        "# Separate and handle labels that occur in data less than three times\n",
        "\n",
        "one_label = []\n",
        "for key, value in nums.items():\n",
        "    if value == 1:\n",
        "      one_label.append(key)\n",
        "\n",
        "two_labels = []\n",
        "for key, value in nums.items():\n",
        "    if value == 2:\n",
        "      two_labels.append(key)\n",
        "\n",
        "sufficient = []\n",
        "for key, value in nums.items():\n",
        "    if value >= 3:\n",
        "      sufficient.append(key)\n",
        "\n",
        "one_label_ = data[data['label'].isin(one_label)]\n",
        "print(one_label_.shape[0]) # number of labels that occur ONLY ONCE in the data. Super bad!\n",
        "\n",
        "two_labels_ = data[data['label'].isin(two_labels)]\n",
        "print(two_labels_['label'].nunique()) # number of labels that occur ONLY TWICE in the data. Bad!\n",
        "\n",
        "enough_labels = data[data['label'].isin(sufficient)]\n",
        "print(enough_labels.shape[0]) # amount of data ready for test-dev-train split as it is\n",
        "print(type(enough_labels))\n",
        "\n",
        "ones = pd.concat([one_label_, one_label_, one_label_])\n",
        "print(ones.shape)\n",
        "\n",
        "twos = pd.concat([two_labels_, two_labels_.drop_duplicates(subset=['label'])])\n",
        "print(twos.shape)\n",
        "\n",
        "data_ = pd.concat([enough_labels, ones, twos])\n",
        "data_.shape\n",
        "\n",
        "y = data_['label'] # pd df\n",
        "# # Separating out the features\n",
        "X = data_['text'] # pd df\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "nums = dict(zip(unique, counts))\n",
        "\n",
        "pprint(sorted(nums.items(), key = lambda kv:(kv[1], kv[0])))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51\n",
            "10\n",
            "7493\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(153, 2)\n",
            "(30, 2)\n",
            "[('AV OP DS IG ', 3),\n",
            " ('CB NA EB IG ', 3),\n",
            " ('CB NA HI  ', 3),\n",
            " ('CB NA IP IG ', 3),\n",
            " ('CB NA RV OP ', 3),\n",
            " ('CB NA SR NA ', 3),\n",
            " ('DF ID HI  ', 3),\n",
            " ('DF ID NE NA ', 3),\n",
            " ('DF ID PB NA ', 3),\n",
            " ('DF ID RE HI ', 3),\n",
            " ('DP IN CB NA ', 3),\n",
            " ('DP IN DS IG ', 3),\n",
            " ('DP IN EN IN ', 3),\n",
            " ('DP IN IP IG ', 3),\n",
            " ('DP IN NA  ', 3),\n",
            " ('DS IG AV OP ', 3),\n",
            " ('DS IG IN  ', 3),\n",
            " ('DS IG MT OS ', 3),\n",
            " ('DS IG OB OP ', 3),\n",
            " ('DS IG RV OP ', 3),\n",
            " ('DT IN AV OP ', 3),\n",
            " ('DT IN FC NA ', 3),\n",
            " ('DT IN NE NA ', 3),\n",
            " ('DT IN RV OP ', 3),\n",
            " ('EN IN HI  ', 3),\n",
            " ('EN IN IP IG ', 3),\n",
            " ('FC NA DP IN ', 3),\n",
            " ('FC NA DT IN ', 3),\n",
            " ('FC NA ID  ', 3),\n",
            " ('HA NA DP IN ', 3),\n",
            " ('HA NA DS IG ', 3),\n",
            " ('HA NA PB NA ', 3),\n",
            " ('HI  DS IG ', 3),\n",
            " ('HI  DT IN ', 3),\n",
            " ('HI  EN IN ', 3),\n",
            " ('HI  IP IG ', 3),\n",
            " ('HI  LT IN ', 3),\n",
            " ('HI  NE NA ', 3),\n",
            " ('IB IN AV OP ', 3),\n",
            " ('IB IN CB NA ', 3),\n",
            " ('IB IN IN  ', 3),\n",
            " ('ID  PB NA ', 3),\n",
            " ('IN  HI  ', 3),\n",
            " ('IP IG NE NA ', 3),\n",
            " ('IT SP DT IN ', 3),\n",
            " ('MT OS DT IN ', 3),\n",
            " ('MT OS EN IN ', 3),\n",
            " ('NE NA DT IN ', 3),\n",
            " ('NE NA RV OP ', 3),\n",
            " ('OA NA DP IN ', 3),\n",
            " ('OA NA FC NA ', 3),\n",
            " ('OA NA OP  ', 3),\n",
            " ('OA NA RV OP ', 3),\n",
            " ('OB OP DT IN ', 3),\n",
            " ('OB OP RE HI ', 3),\n",
            " ('OP  DP IN ', 3),\n",
            " ('PB NA DS IG ', 3),\n",
            " ('PB NA IB IN ', 3),\n",
            " ('PB NA RV OP ', 3),\n",
            " ('PO LY RV OP ', 3),\n",
            " ('RE HI PB NA ', 3),\n",
            " ('RV OP DS IG ', 3),\n",
            " ('RV OP PB NA ', 3),\n",
            " ('SL LY ', 3),\n",
            " ('SR NA DS IG ', 3),\n",
            " ('TB NA CB NA ', 3),\n",
            " ('DP IN FC NA ', 4),\n",
            " ('EN IN FC NA ', 4),\n",
            " ('ID  FC NA ', 4),\n",
            " ('CB NA DS IG ', 5),\n",
            " ('DS IG HI  ', 5),\n",
            " ('DT IN HI  ', 5),\n",
            " ('OA NA HI  ', 5),\n",
            " ('PB NA RE HI ', 5),\n",
            " ('RV OP MT OS ', 5),\n",
            " ('MT OS DS IG ', 6),\n",
            " ('NE NA HI  ', 7),\n",
            " ('OA NA DS IG ', 7),\n",
            " ('PO LY ', 10),\n",
            " ('NE NA IP IG ', 13),\n",
            " ('DF ID FC NA ', 15),\n",
            " ('FS SP ', 15),\n",
            " ('AV OP ', 17),\n",
            " ('JD IN ', 20),\n",
            " ('RE HI ', 20),\n",
            " ('FA IN ', 22),\n",
            " ('OP  ', 22),\n",
            " ('IT SP ', 30),\n",
            " ('FC NA ', 36),\n",
            " ('HA NA ', 36),\n",
            " ('IP IG ', 37),\n",
            " ('CM IN ', 44),\n",
            " ('TB NA ', 44),\n",
            " ('RA IN ', 48),\n",
            " ('EB IG ', 50),\n",
            " ('QA ID ', 57),\n",
            " ('IB IN ', 61),\n",
            " ('RP IN ', 63),\n",
            " ('LT IN ', 68),\n",
            " ('DP IN ', 83),\n",
            " ('NA  ', 86),\n",
            " ('NE NA DS IG ', 87),\n",
            " ('ID  ', 100),\n",
            " ('IN  ', 130),\n",
            " ('MT OS RV OP ', 146),\n",
            " ('EN IN ', 170),\n",
            " ('RV OP ', 193),\n",
            " ('SR NA ', 207),\n",
            " ('CB NA ', 218),\n",
            " ('OB OP ', 233),\n",
            " ('HI  ', 270),\n",
            " ('OA NA ', 287),\n",
            " ('RS OP ', 308),\n",
            " ('DT IN ', 394),\n",
            " ('DF ID ', 437),\n",
            " ('DS IG ', 718),\n",
            " ('PB NA ', 789),\n",
            " ('NE NA ', 875),\n",
            " ('MT OS ', 957)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH1zADNqU3C5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ca2c1cae-a6ee-40fe-8a49-2fde2e8bce28"
      },
      "source": [
        "y = data_['label']\n",
        "X = data_['text']\n",
        "\n",
        "# Split train and development data\n",
        "\n",
        "dev_size = dev.shape[0]/data.shape[0]\n",
        "train_text, dev_text, train_labels, dev_labels = train_test_split(X, y, stratify = y, test_size=dev_size, random_state=1)\n",
        "\n",
        "# Split train and test data\n",
        "\n",
        "test_size = test.shape[0] / len(train_text)\n",
        "train_text, test_text, train_labels, test_labels = train_test_split(train_text, train_labels, stratify = train_labels, test_size=test_size, random_state=1)\n",
        "\n",
        "print(train_text.shape, dev_text.shape, test_text.shape)\n",
        "# print(train_text.head())\n",
        "# print(train_labels.head())\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "all_labels = pd.concat(labels)\n",
        "class_count = len(all_labels.unique())\n",
        "print(\"Number of unique labels in data: \", class_count)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5395,) (768,) (1513,)\n",
            "Number of unique labels in data:  119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzVzQOvux5lJ",
        "colab_type": "text"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "Since we now know that 'MT OS ' is the most common label in the data with 957 occurences, we can set a naive baseline prediction: We will predict that an unlabeled new text belongs to this biggest class. In this case our prediction accuracy is the pure share of the biggest class in the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vU1dkUOyhQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5addb741-f61f-4ff6-94ad-5041dbc13107"
      },
      "source": [
        "print(\"Classification baseline: \", round((957/data.shape[0])*100,1), \"percent\") # here the original data without modifications for training of classificators "
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification baseline:  12.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBkAmTngYvUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# plt.subplots_adjust(left=None, bottom=None, right=None, top=1, wspace=0.6, hspace=0.6)\n",
        "\n",
        "# plt.subplot(2, 3, 1)\n",
        "# sns.countplot(x='label', data=train, )\n",
        "# #ax = sns.lineplot(x=\"t_Length\", y=\"Gross_tonnage\", hue=\"Ship_type\", data=df).set_title('Gross tonnage as a function of ship length to power of two')\n",
        "\n",
        "# plt.subplot(2, 3, 2)\n",
        "# sns.countplot(x='label', data=dev, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j80A7RBn0KOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Gather features and labels of the data\n",
        "\n",
        "# # Separate text and the associated label\n",
        "# train_text = train['text']\n",
        "# train_labels = train['label']\n",
        "\n",
        "# print(train_text.head())\n",
        "# print(train_labels.head())\n",
        "# print()\n",
        "\n",
        "# dev_text = dev['text']\n",
        "# dev_labels = dev['label']\n",
        "\n",
        "# test_text = test['text']\n",
        "# test_labels = test['label']\n",
        "\n",
        "# labels = [train_labels, dev_labels, test_labels]\n",
        "# all_labels = pd.concat(labels)\n",
        "\n",
        "# print(all_labels.head(10))\n",
        "# print()\n",
        "# class_count = len(all_labels.unique())\n",
        "# print(\"Number of unique labels in data: \", class_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_AQXZLHkXz",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.1: Bag-of-words classifier (multi-class)\n",
        "\n",
        "Train a bag-of-words classifier to predict the register categories. In this milestone, the setting is multi-class, so the register label combinations form the classes, e.g. NA_NE and NA_NE_OP_OB. \n",
        "\n",
        "- Evaluate your model and report your results with different hyperparameters\n",
        "- Ideas to try:\n",
        "  - Different activation functions\n",
        "  - Altering the learning rate\n",
        "  - Use different optimizers\n",
        "  - Adjusting the vocabulary size of the embeddings\n",
        "\n",
        "- Activation functions and optimizers supported by Keras can be found here: https://keras.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FGFhSxaVye",
        "colab_type": "text"
      },
      "source": [
        "Bow classifier is only interested in the multiplicity or appearance of words (or to be precise n-garms). Hence we loose the textual context and order of the words (n-grams). This inevitably leads to some information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ3qKWoKbeEU",
        "colab_type": "text"
      },
      "source": [
        "We will use CountVectorizer from sklearn package to transform out text data to numerical format with which our classifier is able to deal with. CountVectorizer converts the collection of text documents (our training data) to a matrix of token counts. Since we are only interested whether a particular word ot the vocabulary is in a single document or not, our vectorizer is set on \"binary\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDSR0PC0J0vC",
        "colab_type": "code",
        "outputId": "44a530f8-cfb9-4b90-83f6-6e9ac3990ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features = 85000, binary = True, ngram_range = (1,1))\n",
        "# form feature matrix\n",
        "train_feature_matrix = vectorizer.fit_transform(train_text)\n",
        "dev_feature_matrix = vectorizer.fit_transform(dev_text)\n",
        "\n",
        "print(\"shape of the training data: \", train_feature_matrix.shape)\n",
        "print(\"shape of the development data: \", dev_feature_matrix.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5395, 85000)\n",
            "shape of the development data:  (768, 85000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymmdQP4aq2E",
        "colab_type": "text"
      },
      "source": [
        "The shape of the feature matrix tells us that we have 5295 items (documents) in our training data. The number of unique n-grams exceeds 97 000 but we are including only the first 97 000 most common of them. Since our CountVectorizer has parameter setting \"ngram_range = 1, 1\" this means we are forming the vector with unigrams, separate words or charachters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTY0uwjd25w",
        "colab_type": "text"
      },
      "source": [
        "## Label encoding\n",
        "\n",
        "Next we will encode the labels. This means transforming the textula labels no numeric values, which our model is able to deal with. This step is made with LabelEncoder class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_yBCz62umP",
        "colab_type": "code",
        "outputId": "b55ad638-919b-4dd3-baa2-52e292a05a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() # Create the instance of LabelEncoder we use to turn class labels into integers\n",
        "\n",
        "label_encoder.fit(all_labels) # encode labels to integers\n",
        "\n",
        "train_numbers = label_encoder.transform(train_labels) \n",
        "dev_numbers = label_encoder.transform(dev_labels) \n",
        "test_numbers = label_encoder.transform(test_labels) \n",
        "\n",
        "print(\"Inverse transform gives unique labels in each data set: \", label_encoder.inverse_transform(train_numbers))\n",
        "print(\"Sanity checks, do we have as many labels and texts in our data sets\")\n",
        "print(len(train_numbers), len(train_text))\n",
        "print(len(dev_numbers), len(dev_text))\n",
        "print(len(test_numbers), len(test_text))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inverse transform gives unique labels in each data set:  ['NE NA ' 'CB NA ' 'IN  ' ... 'RS OP ' 'PB NA ' 'DF ID ']\n",
            "Sanity checks, do we have as many labels and texts in our data sets\n",
            "5395 5395\n",
            "768 768\n",
            "1513 1513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3RDtU7J2di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LabelEncoderin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode tee koodaus\n",
        "# print(\"Koodaukset: \", list(le.classes_))\n",
        "# print()\n",
        "# le.transform([\"tokyo\", \"tokyo\", \"paris\"]) # käytä koodausta arvojen transformointiin --> numeeriset arvot\n",
        "# num = le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
        "# print(\"Sovitetulla encoderilla tuotetut numeeriset arvot muuttujalistasta: \", num)\n",
        "# print()\n",
        "# print(\"Inverse transform: \", list(le.inverse_transform([2, 2, 1])))\n",
        "# print()\n",
        "# test = le.fit_transform([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit kolmella\", test)\n",
        "\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"helsinki\"])\n",
        "# #print(list(le.classes_))\n",
        "\n",
        "# test = le.fit_transform([\"paris\", \"helsinki\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit neljällä\", test)\n",
        "\n",
        "# print(list(le.inverse_transform([2, 2, 1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9_7r6FeRS5",
        "colab_type": "text"
      },
      "source": [
        "Now the data is prepared and we move on to building the classifier itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcK4sbI5ecs8",
        "colab_type": "code",
        "outputId": "0394821f-22c8-48d9-efd8-e90c736443e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "# from tensorflow.python.keras.models import Model\n",
        "# from tensorflow.python.keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = train_feature_matrix.shape\n",
        "\n",
        "inp = Input(shape = (feature_count, ))                  # Tuple. The size of the inputlayer is the number of the vectors\n",
        "hidden = Dense(200, activation=\"tanh\")(inp)             # Non-linear activation function. tanh or relu? \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden) # As many output possibilities as we have input classes. ALL THE POSSIBLE CLASSES!!?!??! \n",
        "                                                        # Softmax: produces probability distribution of the classes\n",
        "bow_model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "bow_model.summary()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 85000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               17000200  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 119)               23919     \n",
            "=================================================================\n",
            "Total params: 17,024,119\n",
            "Trainable params: 17,024,119\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGvX2ZUVB_6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_model.compile(optimizer=\"adam\", loss = \"sparse_categorical_crossentropy\", metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQnwLuHcDntV"
      },
      "source": [
        "Now we will fit the data. Here we will also need the validation data.\n",
        "\n",
        "batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "epochs kuinka monta kertaa mennaan lapi koko data\n",
        "validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "- jos näyttää sille, että mallin oppiminen paranisi vaikka malli on jo treenattu (val_acc kehittyy paremmaksi), lisaa epocheja. Käytä early stoppingia estään ylisovittaminen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2HwFxZzwOP",
        "colab_type": "text"
      },
      "source": [
        "## Fitting BOW-classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49kAxppBIzR_",
        "colab_type": "text"
      },
      "source": [
        "Let's try with different optimizers available in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTEzmMg8HKic",
        "colab_type": "code",
        "outputId": "0537d9a5-8ead-4f92-e0fd-0dbe8cc7f42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl\n",
        "# Lisää tähän loopiin eri learningratet?\n",
        "ops = ('Adadelta', 'Adagrad', 'Adam', 'SGD', 'RMSProp', 'Adamax', 'Nadam')\n",
        "\n",
        "# tee uusi muuttuja, jossa LR: liitetty loopin avulla optimizeriin!\n",
        "\n",
        "for op in ops:\n",
        "  print(op)\n",
        "  print()\n",
        "  bow_model.compile(optimizer = op, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  bow_history = bow_model.fit(train_feature_matrix, train_numbers, batch_size=100, \n",
        "                 verbose=1, epochs=25, validation_data=(dev_feature_matrix, dev_numbers), callbacks=[stop_cb])\n",
        "  print()\n",
        "\n",
        "# https://www.javacodemonk.com/difference-between-loss-accuracy-validation-loss-validation-accuracy-in-keras-ff358faa  "
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adadelta\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 6s 1ms/step - loss: 2.6899 - accuracy: 0.4499 - val_loss: 4.2852 - val_accuracy: 0.1159\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 5s 856us/step - loss: 1.2776 - accuracy: 0.7318 - val_loss: 4.2593 - val_accuracy: 0.0964\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 5s 856us/step - loss: 0.6465 - accuracy: 0.8890 - val_loss: 4.2239 - val_accuracy: 0.0911\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 5s 847us/step - loss: 0.3508 - accuracy: 0.9513 - val_loss: 4.2040 - val_accuracy: 0.0938\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Adagrad\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 4s 738us/step - loss: 1.3905 - accuracy: 0.6630 - val_loss: 3.9155 - val_accuracy: 0.1068\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 721us/step - loss: 0.1190 - accuracy: 0.9876 - val_loss: 3.9827 - val_accuracy: 0.1042\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 722us/step - loss: 0.0566 - accuracy: 0.9961 - val_loss: 4.0170 - val_accuracy: 0.1029\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 705us/step - loss: 0.0376 - accuracy: 0.9963 - val_loss: 4.0487 - val_accuracy: 0.0977\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Adam\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 4s 783us/step - loss: 0.1146 - accuracy: 0.9855 - val_loss: 4.2715 - val_accuracy: 0.0924\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 756us/step - loss: 0.0283 - accuracy: 0.9957 - val_loss: 4.3953 - val_accuracy: 0.0951\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 759us/step - loss: 0.0166 - accuracy: 0.9967 - val_loss: 4.5275 - val_accuracy: 0.0951\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 755us/step - loss: 0.0128 - accuracy: 0.9967 - val_loss: 4.5983 - val_accuracy: 0.0911\n",
            "Epoch 5/25\n",
            "5395/5395 [==============================] - 4s 756us/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 4.6440 - val_accuracy: 0.0951\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00005: early stopping\n",
            "\n",
            "SGD\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 4s 662us/step - loss: 0.0146 - accuracy: 0.9970 - val_loss: 4.3966 - val_accuracy: 0.0951\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 650us/step - loss: 0.0145 - accuracy: 0.9968 - val_loss: 4.3978 - val_accuracy: 0.0951\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 652us/step - loss: 0.0143 - accuracy: 0.9968 - val_loss: 4.3991 - val_accuracy: 0.0951\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 650us/step - loss: 0.0142 - accuracy: 0.9968 - val_loss: 4.4003 - val_accuracy: 0.0951\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "RMSProp\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 4s 743us/step - loss: 0.0195 - accuracy: 0.9956 - val_loss: 4.7824 - val_accuracy: 0.0924\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 721us/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 5.0127 - val_accuracy: 0.0872\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 721us/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 5.1578 - val_accuracy: 0.0872\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 722us/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 5.3305 - val_accuracy: 0.0898\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Adamax\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 4s 745us/step - loss: 0.0143 - accuracy: 0.9968 - val_loss: 4.9529 - val_accuracy: 0.0885\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 717us/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 5.0409 - val_accuracy: 0.0872\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 721us/step - loss: 0.0063 - accuracy: 0.9976 - val_loss: 5.1107 - val_accuracy: 0.0859\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 722us/step - loss: 0.0058 - accuracy: 0.9976 - val_loss: 5.1598 - val_accuracy: 0.0859\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Nadam\n",
            "\n",
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 5s 857us/step - loss: 0.0218 - accuracy: 0.9957 - val_loss: 5.1876 - val_accuracy: 0.0924\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 827us/step - loss: 0.0159 - accuracy: 0.9961 - val_loss: 5.2961 - val_accuracy: 0.0898\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 822us/step - loss: 0.0172 - accuracy: 0.9965 - val_loss: 5.4214 - val_accuracy: 0.1003\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 824us/step - loss: 0.0089 - accuracy: 0.9965 - val_loss: 5.5614 - val_accuracy: 0.1003\n",
            "Epoch 5/25\n",
            "5395/5395 [==============================] - 4s 820us/step - loss: 0.0087 - accuracy: 0.9968 - val_loss: 5.6596 - val_accuracy: 0.0990\n",
            "Epoch 6/25\n",
            "5395/5395 [==============================] - 4s 821us/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 5.8166 - val_accuracy: 0.1055\n",
            "Epoch 7/25\n",
            "5395/5395 [==============================] - 4s 822us/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 5.9888 - val_accuracy: 0.1081\n",
            "Epoch 8/25\n",
            "5395/5395 [==============================] - 4s 820us/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 6.1374 - val_accuracy: 0.1081\n",
            "Epoch 9/25\n",
            "5395/5395 [==============================] - 4s 823us/step - loss: 0.0064 - accuracy: 0.9968 - val_loss: 6.2913 - val_accuracy: 0.1081\n",
            "Epoch 10/25\n",
            "5395/5395 [==============================] - 4s 821us/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 6.4523 - val_accuracy: 0.1081\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00010: early stopping\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SM_ApKFQ0b0",
        "colab_type": "code",
        "outputId": "901921dd-21a4-409f-f382-336b2b416a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "# Choose best OP and LR and fit the model\n",
        "bow_model.compile(optimizer = 'Adadelta', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "bow_history = bow_model.fit(train_feature_matrix, train_numbers, batch_size=100, \n",
        "                 verbose=1, epochs=25, validation_data=(dev_feature_matrix, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 5s 852us/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 6.0066 - val_accuracy: 0.1094\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 4s 819us/step - loss: 0.0046 - accuracy: 0.9981 - val_loss: 6.0248 - val_accuracy: 0.1081\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 4s 821us/step - loss: 0.0045 - accuracy: 0.9981 - val_loss: 6.0430 - val_accuracy: 0.1068\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 4s 821us/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 6.0610 - val_accuracy: 0.1068\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cgUwD-ug2zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# predictions = model.predict(dev_feature_matrix)\n",
        "# pred_classes = np.argmax(predictions,axis=-1)\n",
        "# for pred, correct, txt_line in zip(pred_classes, dev_labels, dev_text):\n",
        "#     pred_label=label_encoder.classes_[pred]\n",
        "#     if pred_label!=correct:\n",
        "#         print(\"Prediction:\",pred_label,\"Correct:\",correct,\"Text:\",txt_line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_zfbGvKjIL1",
        "colab_type": "code",
        "outputId": "1a936957-3e0b-4cdc-b25d-8bbe9a9db4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# form feature matrix for test data set\n",
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "\n",
        "print(test_feature_matrix.shape)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1513, 85000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuTlFg7i-gdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Code for plot from:\n",
        "# # http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import itertools\n",
        "# %matplotlib inline \n",
        "\n",
        "# def plot_confusion_matrix(cm, classes,\n",
        "#                           normalize = False,\n",
        "#                           title = 'Confusion matrix',\n",
        "#                           cmap = plt.cm.Blues):\n",
        "    \n",
        "    \n",
        "#     \"\"\"\n",
        "#     This function prints and plots the confusion matrix.\n",
        "#     Normalization can be applied by setting `normalize=True`.\n",
        "#     \"\"\"\n",
        "#     if normalize:\n",
        "#         cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
        "#         print(\"Normalized confusion matrix\")\n",
        "#     else:\n",
        "#         print('Confusion matrix, without normalization')\n",
        "\n",
        "#     print(cm)\n",
        "\n",
        "#     plt.imshow(cm, interpolation='nearest', cmap = cmap)\n",
        "#     plt.title(title)\n",
        "#     plt.colorbar()\n",
        "#     tick_marks = np.arange(len(classes))\n",
        "#     plt.xticks(tick_marks, classes, rotation=45)\n",
        "#     plt.yticks(tick_marks, classes)\n",
        "\n",
        "#     fmt = '.2f' if normalize else 'd'\n",
        "#     thresh = cm.max() / 2.\n",
        "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "#         plt.text(j, i, format(cm[i, j], fmt),\n",
        "#                  horizontalalignment = \"center\",\n",
        "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "#     plt.ylabel('True label')\n",
        "#     plt.xlabel('Predicted label')\n",
        "#     plt.tight_layout() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MR-tO01ge6V",
        "colab_type": "code",
        "outputId": "6c92e496-d5f3-4d9b-c119-7b4fb0217d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "#print(\"Network output=\",model.predict(test_feature_matrix))\n",
        "predictions = np.argmax(bow_model.predict(test_feature_matrix), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "print(\"true labels: \\n\", test_labels)\n",
        "# target_labels = label_encoder.inverse_transform(list(target))\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"predicted labels: \\n\", predicted_labels)\n",
        "print()\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")\n",
        "\n",
        "# print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true labels: \n",
            " 4123          RV OP \n",
            "986     NE NA DS IG \n",
            "1283          IP IG \n",
            "337             HI  \n",
            "969           DT IN \n",
            "            ...     \n",
            "1167          NE NA \n",
            "1372          EN IN \n",
            "1184          RV OP \n",
            "294           NE NA \n",
            "870           MT OS \n",
            "Name: label, Length: 1513, dtype: object\n",
            "predicted labels: \n",
            " ['NE NA ' 'NE NA ' 'PB NA ' ... 'NE NA ' 'PB NA ' 'DT IN ']\n",
            "\n",
            "Classification accuracy:  11.1 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHF-M4emLu-h",
        "colab_type": "text"
      },
      "source": [
        "Why does the model perform so badly? 119 classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrzogBR6abt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "9efa3c69-9abf-4043-9d2c-9db18eecf25f"
      },
      "source": [
        "\n",
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_numbers)))\n",
        "print(\"-development data: \", len(np.unique(dev_numbers)))\n",
        "print(\"-test data: \", len(np.unique(test_numbers)))\n",
        "print()\n",
        "inter = np.intersect1d(train_numbers, dev_numbers)\n",
        "inter2 = np.intersect1d(train_numbers, test_numbers)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  119\n",
            "-development data:  56\n",
            "-test data:  101\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  56\n",
            "-train and test data:  101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U28dT9xjZYKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# # Confusion matrix has the true labels on rows, and predicted labels on columns in sorted order\n",
        "# print(cnf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8AaFnYV_DRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot confusion matrix\n",
        "# plt.figure()\n",
        "# plot_confusion_matrix(cnf_matrix, classes = all_labels, normalize = False)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JdOntcnMyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # np-argmaxin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "# print(predictions[0])\n",
        "# print(model.predict(test_feature_matrix)[0][25])\n",
        "# print(model.predict(test_feature_matrix)[0])\n",
        "# print(sum(model.predict(test_feature_matrix)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqG5963ZhDa",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.2: Recurrent Neural Network Classifier (multi-class)\n",
        "\n",
        "Modify your codes from milestone 1.1 to use recurrent neural networks (e.g. LSTM or biLSTM) in the classifier. Evaluate your model and report your results with different hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOfJs2YLeAtA",
        "colab_type": "text"
      },
      "source": [
        "For RNN-calssifier we use Tokenizer which turns tokens, in our case the words of training data to integers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7j5rqbNzm3O",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KY7mtEZl-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=97000, # max num of most common words\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6eKbcvnd9m4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "aee00198-60c7-4bb1-fe73-628027874cc9"
      },
      "source": [
        "from pprint import pprint    # pretty-printer\n",
        "\n",
        "def truncate_dict(d, count=10):\n",
        "    # Returns at most count items from the given dictionary.  \n",
        "    return dict(i for i, _ in zip(d.items(), range(count)))\n",
        "\n",
        "# Check if 0 is in the index, and print examples of the mapping\n",
        "# 0 is reserved for padding!\n",
        "print(tokenizer.word_index.get(0))\n",
        "pprint(truncate_dict(tokenizer.word_index))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "{'ei': 3,\n",
            " 'että': 4,\n",
            " 'ja': 1,\n",
            " 'kun': 9,\n",
            " 'mutta': 7,\n",
            " 'oli': 8,\n",
            " 'on': 2,\n",
            " 'ovat': 10,\n",
            " 'se': 5,\n",
            " 'tai': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDmMZZfg62w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "eadc7ebc-cae9-4ab1-fcd4-ca0d5270394e"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "print(len(train_sequences)) \n",
        "\n",
        "# Print an example text, its corresponding sequence, and the tokens it represents\n",
        "print('Text:', train_text.head(1)[0:200]) # first item of the suffled data (index not 0!)\n",
        "print('Sequence:', train_sequences[0][:10])\n",
        "print('Mapped back:', [tokenizer.index_word[i] for i in train_sequences[0][:10]])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5395\n",
            "Text: 104     MAAPALLOUUTISET WWF etsii Suomenlahden suojel...\n",
            "Name: text, dtype: object\n",
            "Sequence: [20072, 3350, 21213, 61223, 20072, 114, 15206, 23896, 5404, 6082]\n",
            "Mapped back: ['wwf', 'etsii', 'suomenlahden', 'suojelijoita', 'wwf', 'suomen', 'vuotuinen', 'panda', 'palkinto', 'myönnetään']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js1rksZ6AA6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6a4b1a9c-d918-4451-ac66-ec7c6337e2d8"
      },
      "source": [
        "lengths = [len(s) for s in train_sequences]\n",
        "print('Lengths:', lengths[:10], 'min:', min(lengths), 'max:', max(lengths), 'mean:', np.mean(lengths))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengths: [203, 197, 60, 101, 62, 85, 1182, 592, 90, 39] min: 0 max: 85774 mean: 567.8354031510657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcr2XZ8zg1H",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Since Keras demands for all of the input items (separate documents of our training data) to have the same length, we need to \"pad\" all but the longest document by filling in the \"missing\" number of words with zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA209Lqxzd8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "332ad7a5-9eea-4028-9ba1-133ffbfd5938"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequence_length = np.floor(np.mean(lengths)).astype(int) # based on mean value of input length: we will cut sequences longer than this and pad with zeros sequeces shorter than this\n",
        "\n",
        "type(sequence_length)\n",
        "\n",
        "padded_X = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_X.shape)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5395, 567)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk9M_AtI1Jc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bd9b99c7-f3dd-482e-dde0-f1ef4f0dcbb6"
      },
      "source": [
        "# Prepare model development data\n",
        "\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_text)\n",
        "padded_dev = pad_sequences(\n",
        "    dev_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_dev.shape)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(768, 567)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HXbVzAF6UH",
        "colab_type": "text"
      },
      "source": [
        "# Build LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vn7FWLFGeJQ",
        "colab_type": "text"
      },
      "source": [
        "# HUOMAA!!!\n",
        "Tämä teksti alkuperäisestä RNN-classification notebookista!\n",
        "\n",
        "We define a basic RNN model that takes the RNN cell class (RNN_class) as an argument:\n",
        "\n",
        "- input: sequence of sequence_length integers corresponding to words\n",
        "- embedding: randomly initialized mapping from integers to embedding_dim-dimensional vectors\n",
        "- rnn: recurrent neural network with rnn_units-dimensional state\n",
        "- output: num_classes-dimensional fully connected layer with softmax activation\n",
        "\n",
        "# KATSO NÄITÄ!\n",
        "We're intentionally leaving out a few fairly obvious things that would be expected to help here, including\n",
        "\n",
        "- Any form of regularization, e.g. dropout\n",
        "- Initializing the embeddings with pre-trained word vectors (ks. fasttext)\n",
        "- Masking to ignore padding (see Masking and padding with Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhRaStPGH7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# We'll use these model parameters for all of our examples here.\n",
        "embedding_dim = 50 # input vector\n",
        "rnn_units = 100\n",
        "\n",
        "def build_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "    input_ = Input(shape=(sequence_length,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized. Layer turns positive integers (indexes) into dense vectors of fixed size\n",
        "    rnn = RNN_class(rnn_units)(embedding) # can support different RNNs\n",
        "    output = Dense(num_classes, activation='softmax')(rnn)\n",
        "    return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "sequence_length = padded_X.shape[1]\n",
        "vocab_size = tokenizer.num_words\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5y48zddjkf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ce4482ee-7878-491a-8c91-58891f35e2ff"
      },
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4icO32GGX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "ac094afd-d194-4f61-bf16-9070920e1830"
      },
      "source": [
        "lstm_model = build_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 567)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 567, 50)           4850000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 119)               12019     \n",
            "=================================================================\n",
            "Total params: 4,922,419\n",
            "Trainable params: 4,922,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikBgdQhpVwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 100\n",
        "stop_cb = EarlyStopping(monitor = 'val_acc', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbxk_lppo_tC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "f8802b6b-30f6-4dec-cb3a-f25d59a4ba45"
      },
      "source": [
        "lstm_history = lstm_model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])\n",
        "# , callbacks=[stop_cb]"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 56s 10ms/sample - loss: 0.0757 - acc: 0.9848 - val_loss: 4.6309 - val_acc: 0.2604\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 55s 10ms/sample - loss: 0.0436 - acc: 0.9917 - val_loss: 4.6467 - val_acc: 0.2891\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 55s 10ms/sample - loss: 0.0394 - acc: 0.9896 - val_loss: 4.9632 - val_acc: 0.2708\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 55s 10ms/sample - loss: 0.0553 - acc: 0.9887 - val_loss: 4.6747 - val_acc: 0.2917\n",
            "Epoch 5/25\n",
            "5395/5395 [==============================] - 55s 10ms/sample - loss: 0.0329 - acc: 0.9930 - val_loss: 4.9627 - val_acc: 0.2656\n",
            "Epoch 6/25\n",
            "5395/5395 [==============================] - 55s 10ms/sample - loss: 0.0259 - acc: 0.9931 - val_loss: 4.9625 - val_acc: 0.2643\n",
            "Epoch 7/25\n",
            "5300/5395 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9942Restoring model weights from the end of the best epoch\n",
            "5395/5395 [==============================] - 55s 10ms/sample - loss: 0.0232 - acc: 0.9943 - val_loss: 5.0499 - val_acc: 0.2747\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXAKNMb5PbM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d511702a-0f1f-42d7-f073-ad4836d688f3"
      },
      "source": [
        "# Prepare model test data\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "padded_test = pad_sequences(\n",
        "    test_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_test.shape)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1513, 567)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrcF1iyLzBkW",
        "colab_type": "text"
      },
      "source": [
        "Predict with LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3MGvq6CzAIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "417f9174-a9ba-4063-d2c4-62dd62f9112b"
      },
      "source": [
        "predictions = np.argmax(lstm_model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  29.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIycPktw_m39",
        "colab_type": "text"
      },
      "source": [
        "# Bidirectional LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFoIroA0_sMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tämä ei toimi! Virhe nimeämisestä, selvittämättä!\n",
        "# from keras.layers import Bidirectional as Bi\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCG5imFKY_Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_bi_model = build_bi_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "# lstm_bi_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1QpSnLZY6jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_bi_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xnzHNvLWwOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# # model = Sequential()\n",
        "# # model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
        "# #                         input_shape=(5, 10)))\n",
        "# # model.add(Bidirectional(LSTM(10)))\n",
        "# # model.add(Dense(5))\n",
        "# # model.add(Activation('softmax'))\n",
        "# # model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Dense(90, input_dim=sequence_length))\n",
        "# model.add(Embedding(vocab_size, embedding_dim))\n",
        "# model.add(Bidirectional(LSTM(rnn_units)))\n",
        "# model.add(Dense(num_classes))\n",
        "# model.add(Activation('softmax'))\n",
        "\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8vdw-jcfAKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Input(shape=(sequence_length,)\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWl2MJjjGmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "9940a55b-b327-4dc7-a219-42e13a4997bd"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 567, 50)           4850000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 200)               120800    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 119)               23919     \n",
            "=================================================================\n",
            "Total params: 4,994,719\n",
            "Trainable params: 4,994,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7L2tJHok6_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "6587e1d8-a8a4-4f20-a3bc-6f572187d447"
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "hist_bi_lstm = model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5395 samples, validate on 768 samples\n",
            "Epoch 1/25\n",
            "5395/5395 [==============================] - 67s 12ms/step - loss: 3.1821 - accuracy: 0.1837 - val_loss: 2.9672 - val_accuracy: 0.2135\n",
            "Epoch 2/25\n",
            "5395/5395 [==============================] - 68s 13ms/step - loss: 2.8342 - accuracy: 0.2958 - val_loss: 2.7440 - val_accuracy: 0.3099\n",
            "Epoch 3/25\n",
            "5395/5395 [==============================] - 68s 13ms/step - loss: 2.4475 - accuracy: 0.4046 - val_loss: 2.6771 - val_accuracy: 0.3346\n",
            "Epoch 4/25\n",
            "5395/5395 [==============================] - 68s 13ms/step - loss: 2.0166 - accuracy: 0.5008 - val_loss: 2.7578 - val_accuracy: 0.3216\n",
            "Epoch 5/25\n",
            "5395/5395 [==============================] - 66s 12ms/step - loss: 1.6411 - accuracy: 0.5854 - val_loss: 2.8821 - val_accuracy: 0.3125\n",
            "Epoch 6/25\n",
            "5395/5395 [==============================] - 68s 13ms/step - loss: 1.3280 - accuracy: 0.6769 - val_loss: 3.0248 - val_accuracy: 0.3073\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvFxjZZ2ZnQ",
        "colab_type": "text"
      },
      "source": [
        "Predict with bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQIaS_tR2Tiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "02626156-3c24-41df-ea14-9d958b5f81c8"
      },
      "source": [
        "predictions = np.argmax(model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  32.8 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1bZmgfNnWlm",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.1: Deep contextual representations with Bert (multi-class)\n",
        "Train a Bert classifier to predict the register categories. Similar to Milestone 1, the setting is multi-class, and the evaluations should include results with different hyperparameters.\n",
        "\n",
        "# Milestone 2.2: Error analysis\n",
        "Compare the errors made by the classifiers you have trained from milestones 1 and 2.1. Are there any patterns? How do the errors one model makes differ from those made by another.\n",
        "\n",
        "# Milestone 3.1: Bert (multi-LABEL)\n",
        "Train two multi-label classifiers, one using non-deep contextual representations, the other using Bert. In this setting, each label is assigned independently. Do hyperparameter optimization on these classifiers.\n",
        "\n",
        "# Milestone 3.2: Model comparison\n",
        "Compare the results of these two classifiers. Do the two models predict in the same way? Analyze the predictions in terms of label-specific differences.\n",
        "\n",
        "# Register classes and abbreviations\n"
      ]
    }
  ]
}