{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Text_Classification_task.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/priva_DL_HLT/blob/master/Text_Classification_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VGkCcicEVPSF"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "This project is about text classification. You will develop a text classification system that identifies different kinds of online texts, such as news, blogs and opinionated texts. We will refer to these text categories as registers. If you want to learn more about online registers and their automatic identification, you can read, e.g., our paper [Toward Multilingual Identification of Online Registers] (https://www.aclweb.org/anthology/W19-6130/).\n",
        "\n",
        "# Data and register labels\n",
        "The data for this project consist of ~7500 documents with manual annotations on their register. You can download it from http://dl.turkunlp.org/TKO_8965-projects/classification/ . The documents are based on a (almost) random sample of the Finnish Internet. The registers are identified using a relatively detailed, hierarchical taxonomy. The taxonomy consists of 8 main categories that are divided into a large number of subregisters. The taxonomy is described at the end of this page. The table includes also the abbreviations that are used in the data.\n",
        "\n",
        "The challenge with online documents is that it is not always easy to identify the specific registers categories of the documents. Furthermore, another issue is that a document may display characteristics of several registers. For instance, a blog post may simultaneously seem like a product review. To deal with these challenges, we have followed the following guidelines:\n",
        "* For each document, the annotators have aimed at marking the specific subregister category. When this is possible, the document has two register labels: the subregister label and the main register label to which the subregister belongs. For instance, a document annotated as a news article would have the label NE for News and the corresponding higher level register label NA for Narrative. \n",
        "* In some cases, the document does not seem to fit any of the subregisters. In this case, the document can be given only one label: the main register label, such as NA for Narrative. \n",
        "* Some documents may display characteristics of several register categories. In this case, the annotator can mark several register labels for one single document. Consequently, the document may have up to four labels. This would be the case case if a document is annotated both as a Personal blog (subregister label PB + corresponding higher level register label NA) and Review (subregister label RV + corresponding higher level register label OP).\n",
        "\n",
        "\n",
        "# Register classes and abbreviations\n",
        "\n",
        "NA Narrative\n",
        "\n",
        "* NE NA    New reports / news blogs\n",
        "* SR NA    Sports reports\n",
        "* PB NA    Personal blog\n",
        "* HA NA    Historical article\n",
        "* FC NA    Fiction\n",
        "* TB NA    Travel blog\n",
        "* CB NA    Community blogs\n",
        "* OA NA    Online article\n",
        "\n",
        "OP  Opinion\n",
        "* OB OP  Personal opinion blogs\n",
        "* RV OP  Reviews\n",
        "* RS OP  Religious blogs/sermons\n",
        "* AV OP  Advice\n",
        "\n",
        "IN Informational description\n",
        "* JD IN  Job description\n",
        "* FA IN  FAQs\n",
        "* DT IN  Description of a thing\n",
        "* IB IN  Information blogs\n",
        "* DP IN  Description of a person\n",
        "* RA IN  Research articles\n",
        "* LT IN  Legal terms / conditions\n",
        "* CM IN  Course materials\n",
        "* EN IN  Encyclopedia articles\n",
        "* RP IN  Report\n",
        "\n",
        "ID Interactive discussion\n",
        "* DF ID  Discussion forums\n",
        "* QA ID  Question-answer forums\n",
        "\n",
        "HI  How-to/instructions\n",
        "* RE HI  Recipes\n",
        "\n",
        "IP IG  Informational persuasion\n",
        "* DS IG  Description with intent to sell\n",
        "* EB IG  News-opinion blogs / editorials\n",
        "\n",
        "Lyrical LY\n",
        "* PO LY  Poems\n",
        "* SL LY  Songs\n",
        "\n",
        "Spoken SP\n",
        "* IT SP Interviews\n",
        "* FS SP Formal speeches\n",
        "\n",
        "Others OS\n",
        "* MT OS Machine-translated / generated texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1_c9eZnWlG",
        "colab_type": "text"
      },
      "source": [
        "# Preparations\n",
        "\n",
        "Download and open data, explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsWUzLj5NOG",
        "colab_type": "code",
        "outputId": "684d6045-4d0e-496d-a237-8ded08af5116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get rid of old tf at some point!\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# to run with old tf with which the code was made\n",
        "# The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.\n",
        "# https://colab.research.google.com/notebooks/tensorflow_version.ipynb\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHGSBx5nWlM",
        "colab_type": "code",
        "outputId": "54bd0cc6-85c7-4e86-c6cb-f0ef905cf7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "# Download development data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
        "# Download test data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
        "# Download train data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-13 10:37:27--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4035578 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘fincore-dev.tsv’\n",
            "\n",
            "fincore-dev.tsv     100%[===================>]   3.85M  1.65MB/s    in 2.3s    \n",
            "\n",
            "2020-04-13 10:37:30 (1.65 MB/s) - ‘fincore-dev.tsv’ saved [4035578/4035578]\n",
            "\n",
            "--2020-04-13 10:37:32--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8512687 (8.1M) [application/octet-stream]\n",
            "Saving to: ‘fincore-test.tsv’\n",
            "\n",
            "fincore-test.tsv    100%[===================>]   8.12M  3.08MB/s    in 2.6s    \n",
            "\n",
            "2020-04-13 10:37:36 (3.08 MB/s) - ‘fincore-test.tsv’ saved [8512687/8512687]\n",
            "\n",
            "--2020-04-13 10:37:37--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29379580 (28M) [application/octet-stream]\n",
            "Saving to: ‘fincore-train.tsv’\n",
            "\n",
            "fincore-train.tsv   100%[===================>]  28.02M  4.69MB/s    in 7.1s    \n",
            "\n",
            "2020-04-13 10:37:45 (3.95 MB/s) - ‘fincore-train.tsv’ saved [29379580/29379580]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8TezCY6pR1",
        "colab_type": "text"
      },
      "source": [
        "Data split\n",
        "  - Train data - all training based on it (this includes the vectorizer!)\n",
        "  - Development data - set the parameters (a.k.a validation data set)\n",
        "  - Test data - used for nothing during training, produce final results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYWpfRbIDrH0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KciwyumAEz_b",
        "colab_type": "code",
        "outputId": "3b924da7-486a-4194-d0a7-c98907f3bb5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('fincore-train.tsv', sep='\\t', header=None)\n",
        "\n",
        "train = train.sample(frac=1, random_state = 4) # suffle the data\n",
        "train.columns = ['label','text']\n",
        "print(train.head())\n",
        "print(train.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       label                                               text\n",
            "3982  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "2640  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...\n",
            "119   NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...\n",
            "4916  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...\n",
            "775   MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...\n",
            "(5295, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4KLkfAf7UB_",
        "colab_type": "code",
        "outputId": "bf1c8f1a-6283-44e6-e152-c880471e54cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "dev = pd.read_csv('fincore-dev.tsv', sep='\\t', header=None)\n",
        "dev.columns = ['label','text']\n",
        "print(dev.head())\n",
        "print(dev.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    label                                               text\n",
            "0  OA NA    Luonnonhoito Maaperän siemenpankkia avattiin ...\n",
            "1  DS IG    • Jokainen ripsi on erittäin kevyt ja muodolt...\n",
            "2  DS IG    Mukavuudet Hotel Dila Vain muutaman metrin pä...\n",
            "3  DF ID    Vastaa viestiin Otsikko Viesti ensin omaishoi...\n",
            "4  OA NA    Dinosaur Jr 30.5.2010 Tavastia , Helsinki 198...\n",
            "(756, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrEJSdgyZJ8",
        "colab_type": "code",
        "outputId": "b7e13d8e-9448-47e5-81f9-490ff0707266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "test = pd.read_csv('fincore-test.tsv', sep='\\t', header=None)\n",
        "test.columns = ['label','text']\n",
        "print(test.head())\n",
        "print(test.shape)\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    label                                               text\n",
            "0    HI     Tehkää nollaleimaus . Jos rekisteröinti onnis...\n",
            "1    NA     1 kommenttia : Syyslomallelähtijät kirjoitti ...\n",
            "2  DT IN    Ammattikoulutuksen perustana on ajatus siitä ...\n",
            "3  DP IN    Ulkonäkö : Silveriä voisi kuvata tietyllä tap...\n",
            "4  DT IN    Laulupelimannien puheenjohtajina ovat toimine...\n",
            "(1513, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j80A7RBn0KOY",
        "colab_type": "code",
        "outputId": "89399cb0-770a-41aa-bfa4-81dd36c486be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "# Gather features and labels of the data\n",
        "\n",
        "# Separate text and the associated label\n",
        "train_text = train['text']\n",
        "train_labels = train['label']\n",
        "\n",
        "print(train_text.head())\n",
        "print(train_labels.head())\n",
        "print()\n",
        "\n",
        "dev_text = dev['text']\n",
        "dev_labels = dev['label']\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "all_labels = pd.concat(labels)\n",
        "\n",
        "print(all_labels.head(10))\n",
        "print()\n",
        "class_count = len(all_labels.unique())\n",
        "print(\"Number of unique labels in data: \", class_count)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3982     Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "2640     Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...\n",
            "119      Koulutuspaikka jokaiselle peruskoulun päättän...\n",
            "4916     1 Cardiff C–Everton Tasainen kohde . Cardiff ...\n",
            "775      Northrop Grumman Q4 2009 tulokset Northrop Gr...\n",
            "Name: text, dtype: object\n",
            "3982    DS IG \n",
            "2640    RS OP \n",
            "119     NE NA \n",
            "4916    SR NA \n",
            "775     MT OS \n",
            "Name: label, dtype: object\n",
            "\n",
            "3982    DS IG \n",
            "2640    RS OP \n",
            "119     NE NA \n",
            "4916    SR NA \n",
            "775     MT OS \n",
            "5264      NA  \n",
            "5040    DP IN \n",
            "3446    DS IG \n",
            "216     DF ID \n",
            "863     DT IN \n",
            "Name: label, dtype: object\n",
            "\n",
            "Number of unique labels in data:  119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_AQXZLHkXz",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.1: Bag-of-words classifier (multi-class)\n",
        "\n",
        "Train a bag-of-words classifier to predict the register categories. In this milestone, the setting is multi-class, so the register label combinations form the classes, e.g. NA_NE and NA_NE_OP_OB. \n",
        "\n",
        "- Evaluate your model and report your results with different hyperparameters\n",
        "- Ideas to try:\n",
        "  - Different activation functions\n",
        "  - Altering the learning rate\n",
        "  - Use different optimizers\n",
        "  - Adjusting the vocabulary size of the embeddings\n",
        "\n",
        "- Activation functions and optimizers supported by Keras can be found here: https://keras.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FGFhSxaVye",
        "colab_type": "text"
      },
      "source": [
        "Bow classifier is only interested in the multiplicity or appearance of words (or to be precise n-garms). Hence we loose the textual context and order of the words (n-grams). This inevitably leads to some information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ3qKWoKbeEU",
        "colab_type": "text"
      },
      "source": [
        "We will use CountVectorizer from sklearn package to transform out text data to numerical format with which our classifier is able to deal with. CountVectorizer converts the collection of text documents (our training data) to a matrix of token counts. Since we are only interested whether a particular word ot the vocabulary is in a single document or not, our vectorizer is set on \"binary\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDSR0PC0J0vC",
        "colab_type": "code",
        "outputId": "811f221e-f9e7-41fc-c7d3-b38856271aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features = 97000, binary = True, ngram_range = (1,1))\n",
        "# form feature matrix\n",
        "train_feature_matrix = vectorizer.fit_transform(train_text)\n",
        "dev_feature_matrix = vectorizer.fit_transform(dev_text)\n",
        "\n",
        "print(\"shape of the training data: \", train_feature_matrix.shape)\n",
        "print(\"shape of the development data: \", dev_feature_matrix.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5295, 97000)\n",
            "shape of the development data:  (756, 97000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymmdQP4aq2E",
        "colab_type": "text"
      },
      "source": [
        "The shape of the feature matrix tells us that we have 5295 items (documents) in our training data. The number of unique n-grams exceeds 97 000 but we are including only the first 97 000 most common of them. Since our CountVectorizer has parameter setting \"ngram_range = 1, 1\" this means we are forming the vector with unigrams, separate words or charachters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTY0uwjd25w",
        "colab_type": "text"
      },
      "source": [
        "## Label encoding\n",
        "\n",
        "Next we will encode the labels. This means transforming the textula labels no numeric values, which our model is able to deal with. This step is made with LabelEncoder class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_yBCz62umP",
        "colab_type": "code",
        "outputId": "b7558dfa-2eb1-41d2-88d2-c87a27da5607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder() # Create the instance of LabelEncoder we use to turn class labels into integers\n",
        "\n",
        "label_encoder.fit(all_labels) # encode labels to integers\n",
        "\n",
        "train_numbers = label_encoder.transform(train_labels) \n",
        "dev_numbers = label_encoder.transform(dev_labels) \n",
        "test_numbers = label_encoder.transform(test_labels) \n",
        "\n",
        "print(\"Inverse transform gives unique labels in each data set: \", label_encoder.inverse_transform(train_numbers))\n",
        "print(\"Sanity checks, do we have as many labels and texts in our data sets\")\n",
        "print(len(train_numbers), len(train_text))\n",
        "print(len(dev_numbers), len(dev_text))\n",
        "print(len(test_numbers), len(test_text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inverse transform gives unique labels in each data set:  ['DS IG ' 'RS OP ' 'NE NA ' ... 'DS IG ' 'MT OS ' 'MT OS ']\n",
            "Sanity checks, do we have as many labels and texts in our data sets\n",
            "5295 5295\n",
            "756 756\n",
            "1513 1513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3RDtU7J2di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LabelEncoderin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode tee koodaus\n",
        "# print(\"Koodaukset: \", list(le.classes_))\n",
        "# print()\n",
        "# le.transform([\"tokyo\", \"tokyo\", \"paris\"]) # käytä koodausta arvojen transformointiin --> numeeriset arvot\n",
        "# num = le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
        "# print(\"Sovitetulla encoderilla tuotetut numeeriset arvot muuttujalistasta: \", num)\n",
        "# print()\n",
        "# print(\"Inverse transform: \", list(le.inverse_transform([2, 2, 1])))\n",
        "# print()\n",
        "# test = le.fit_transform([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit kolmella\", test)\n",
        "\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"helsinki\"])\n",
        "# #print(list(le.classes_))\n",
        "\n",
        "# test = le.fit_transform([\"paris\", \"helsinki\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit neljällä\", test)\n",
        "\n",
        "# print(list(le.inverse_transform([2, 2, 1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9_7r6FeRS5",
        "colab_type": "text"
      },
      "source": [
        "Now the data is prepared and we move on to building the classifier itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcK4sbI5ecs8",
        "colab_type": "code",
        "outputId": "a7d51cae-7048-4f9a-b019-d0e9b791e914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "# from tensorflow.python.keras.models import Model\n",
        "# from tensorflow.python.keras.layers import Input, Dense\n",
        "\n",
        "example_count, feature_count = train_feature_matrix.shape\n",
        "\n",
        "inp = Input(shape = (feature_count, ))                  # Tuple. The size of the inputlayer is the number of the vectors\n",
        "hidden = Dense(200, activation=\"tanh\")(inp)             # Non-linear activation function. tanh or relu? \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden) # As many output possibilities as we have input classes. ALL THE POSSIBLE CLASSES!!?!??! \n",
        "                                                        # Softmax: produces probability distribution of the classes\n",
        "bow_model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "bow_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 97000)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 200)               19400200  \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 119)               23919     \n",
            "=================================================================\n",
            "Total params: 19,424,119\n",
            "Trainable params: 19,424,119\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGvX2ZUVB_6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_model.compile(optimizer=\"adam\", loss = \"sparse_categorical_crossentropy\", metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQnwLuHcDntV"
      },
      "source": [
        "Now we will fit the data. Here we will also need the validation data.\n",
        "\n",
        "batch_size kuinka monta inputtia kerralla sisaan. jokaisen batchin jalkeen paivitetaan painokertoimet gradientien keskiarvolla\n",
        "epochs kuinka monta kertaa mennaan lapi koko data\n",
        "validation_split: kuinka paljon dataa kaytetaan accuracyn laskemiseen\n",
        "\n",
        "- jos näyttää sille, että mallin oppiminen paranisi vaikka malli on jo treenattu (val_acc kehittyy paremmaksi), lisaa epocheja. Käytä early stoppingia estään ylisovittaminen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2HwFxZzwOP",
        "colab_type": "text"
      },
      "source": [
        "## Fitting BOW-classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49kAxppBIzR_",
        "colab_type": "text"
      },
      "source": [
        "Let's try with different optimizers available in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTEzmMg8HKic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "940a09bb-5b21-4510-91d0-79e597bc9137"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl\n",
        "# Lisää tähän loopiin eri learningratet?\n",
        "ops = ('Adadelta', 'Adagrad', 'Adam', 'SGD', 'RMSProp', 'Adamax', 'Nadam')\n",
        "\n",
        "# tee uusi muuttuja, jossa LR: liitetty loopin avulla optimizeriin!\n",
        "\n",
        "for op in ops:\n",
        "  print(op)\n",
        "  print()\n",
        "  bow_model.compile(optimizer = op, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  bow_history = bow_model.fit(train_feature_matrix, train_numbers, batch_size=100, \n",
        "                 verbose=1, epochs=25, validation_data=(dev_feature_matrix, dev_numbers), callbacks=[stop_cb])\n",
        "  print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adadelta\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 5s 980us/step - loss: 0.6906 - accuracy: 0.8844 - val_loss: 3.8670 - val_accuracy: 0.0873\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 5s 860us/step - loss: 0.3708 - accuracy: 0.9477 - val_loss: 3.8192 - val_accuracy: 0.0939\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 845us/step - loss: 0.2163 - accuracy: 0.9741 - val_loss: 3.8148 - val_accuracy: 0.0992\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 833us/step - loss: 0.1357 - accuracy: 0.9873 - val_loss: 3.8126 - val_accuracy: 0.0966\n",
            "Epoch 5/25\n",
            "5295/5295 [==============================] - 4s 817us/step - loss: 0.0885 - accuracy: 0.9928 - val_loss: 3.8117 - val_accuracy: 0.1005\n",
            "Epoch 6/25\n",
            "5295/5295 [==============================] - 4s 809us/step - loss: 0.0609 - accuracy: 0.9955 - val_loss: 3.8164 - val_accuracy: 0.0979\n",
            "Epoch 7/25\n",
            "5295/5295 [==============================] - 4s 809us/step - loss: 0.0448 - accuracy: 0.9957 - val_loss: 3.8259 - val_accuracy: 0.1005\n",
            "Epoch 8/25\n",
            "5295/5295 [==============================] - 4s 806us/step - loss: 0.0348 - accuracy: 0.9958 - val_loss: 3.8302 - val_accuracy: 0.1045\n",
            "Epoch 9/25\n",
            "5295/5295 [==============================] - 4s 811us/step - loss: 0.0276 - accuracy: 0.9966 - val_loss: 3.8390 - val_accuracy: 0.1045\n",
            "Epoch 10/25\n",
            "5295/5295 [==============================] - 4s 810us/step - loss: 0.0233 - accuracy: 0.9962 - val_loss: 3.8420 - val_accuracy: 0.1045\n",
            "Epoch 11/25\n",
            "5295/5295 [==============================] - 4s 814us/step - loss: 0.0206 - accuracy: 0.9958 - val_loss: 3.8523 - val_accuracy: 0.1032\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00011: early stopping\n",
            "\n",
            "Adagrad\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 4s 738us/step - loss: 0.1467 - accuracy: 0.9788 - val_loss: 5.1842 - val_accuracy: 0.0926\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 4s 749us/step - loss: 0.0282 - accuracy: 0.9955 - val_loss: 5.1461 - val_accuracy: 0.0913\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 762us/step - loss: 0.0132 - accuracy: 0.9964 - val_loss: 5.1598 - val_accuracy: 0.0926\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 770us/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 5.1747 - val_accuracy: 0.0952\n",
            "Epoch 5/25\n",
            "5295/5295 [==============================] - 4s 771us/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 5.1935 - val_accuracy: 0.0966\n",
            "Epoch 6/25\n",
            "5295/5295 [==============================] - 4s 772us/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 5.1983 - val_accuracy: 0.0966\n",
            "Epoch 7/25\n",
            "5295/5295 [==============================] - 4s 771us/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 5.2076 - val_accuracy: 0.0966\n",
            "Epoch 8/25\n",
            "5295/5295 [==============================] - 4s 773us/step - loss: 0.0056 - accuracy: 0.9979 - val_loss: 5.2154 - val_accuracy: 0.0966\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00008: early stopping\n",
            "\n",
            "Adam\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 5s 856us/step - loss: 0.0161 - accuracy: 0.9970 - val_loss: 5.3872 - val_accuracy: 0.0966\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 4s 798us/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 5.3667 - val_accuracy: 0.0966\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 800us/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 5.3958 - val_accuracy: 0.0939\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 793us/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 5.3827 - val_accuracy: 0.0913\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "SGD\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 4s 682us/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 5.3868 - val_accuracy: 0.0966\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 4s 664us/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 5.3864 - val_accuracy: 0.0966\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 3s 659us/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 5.3860 - val_accuracy: 0.0966\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 662us/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 5.3857 - val_accuracy: 0.0966\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "RMSProp\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 4s 764us/step - loss: 0.0126 - accuracy: 0.9974 - val_loss: 5.5261 - val_accuracy: 0.0966\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 4s 755us/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 5.5401 - val_accuracy: 0.0939\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 752us/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 5.6116 - val_accuracy: 0.0952\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 747us/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 5.6299 - val_accuracy: 0.0939\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Adamax\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 4s 764us/step - loss: 0.0090 - accuracy: 0.9975 - val_loss: 5.6710 - val_accuracy: 0.0979\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 4s 747us/step - loss: 0.0050 - accuracy: 0.9981 - val_loss: 5.6517 - val_accuracy: 0.0966\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 743us/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 5.6502 - val_accuracy: 0.0966\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 747us/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 5.6419 - val_accuracy: 0.0952\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n",
            "Nadam\n",
            "\n",
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 5s 889us/step - loss: 0.0168 - accuracy: 0.9962 - val_loss: 5.5976 - val_accuracy: 0.0952\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 5s 860us/step - loss: 0.0175 - accuracy: 0.9960 - val_loss: 5.5379 - val_accuracy: 0.0952\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 847us/step - loss: 0.0153 - accuracy: 0.9958 - val_loss: 5.5063 - val_accuracy: 0.0952\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 846us/step - loss: 0.0081 - accuracy: 0.9960 - val_loss: 5.4956 - val_accuracy: 0.0952\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00004: early stopping\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SM_ApKFQ0b0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "f2963f0b-42c3-45b3-f02c-83b80ccb5ecf"
      },
      "source": [
        "# Choose best OP and LR and fit the model\n",
        "bow_model.compile(optimizer = 'adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "bow_history = bow_model.fit(train_feature_matrix, train_numbers, batch_size=100, \n",
        "                 verbose=1, epochs=25, validation_data=(dev_feature_matrix, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 4s 801us/step - loss: 0.2646 - accuracy: 0.9632 - val_loss: 4.0338 - val_accuracy: 0.0899\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 4s 787us/step - loss: 0.0516 - accuracy: 0.9949 - val_loss: 4.1494 - val_accuracy: 0.0939\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 4s 779us/step - loss: 0.0221 - accuracy: 0.9964 - val_loss: 4.2196 - val_accuracy: 0.0939\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 4s 774us/step - loss: 0.0168 - accuracy: 0.9966 - val_loss: 4.2510 - val_accuracy: 0.0939\n",
            "Epoch 5/25\n",
            "5295/5295 [==============================] - 4s 769us/step - loss: 0.0113 - accuracy: 0.9974 - val_loss: 4.2842 - val_accuracy: 0.0966\n",
            "Epoch 6/25\n",
            "5295/5295 [==============================] - 4s 774us/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 4.3118 - val_accuracy: 0.0952\n",
            "Epoch 7/25\n",
            "5295/5295 [==============================] - 4s 777us/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 4.3330 - val_accuracy: 0.0979\n",
            "Epoch 8/25\n",
            "5295/5295 [==============================] - 4s 771us/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 4.3499 - val_accuracy: 0.1032\n",
            "Epoch 9/25\n",
            "5295/5295 [==============================] - 4s 761us/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 4.3674 - val_accuracy: 0.1032\n",
            "Epoch 10/25\n",
            "5295/5295 [==============================] - 4s 760us/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 4.3844 - val_accuracy: 0.1045\n",
            "Epoch 11/25\n",
            "5295/5295 [==============================] - 4s 759us/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 4.3968 - val_accuracy: 0.1058\n",
            "Epoch 12/25\n",
            "5295/5295 [==============================] - 4s 765us/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 4.4078 - val_accuracy: 0.1045\n",
            "Epoch 13/25\n",
            "5295/5295 [==============================] - 4s 760us/step - loss: 0.0068 - accuracy: 0.9968 - val_loss: 4.4186 - val_accuracy: 0.1045\n",
            "Epoch 14/25\n",
            "5295/5295 [==============================] - 4s 764us/step - loss: 0.0061 - accuracy: 0.9970 - val_loss: 4.4288 - val_accuracy: 0.1045\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cgUwD-ug2zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# predictions = model.predict(dev_feature_matrix)\n",
        "# pred_classes = np.argmax(predictions,axis=-1)\n",
        "# for pred, correct, txt_line in zip(pred_classes, dev_labels, dev_text):\n",
        "#     pred_label=label_encoder.classes_[pred]\n",
        "#     if pred_label!=correct:\n",
        "#         print(\"Prediction:\",pred_label,\"Correct:\",correct,\"Text:\",txt_line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_zfbGvKjIL1",
        "colab_type": "code",
        "outputId": "33e48e2b-fe13-4cd5-ef9d-2bd142e9c5f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# form feature matrix for test data set\n",
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "\n",
        "print(test_feature_matrix.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1513, 97000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuTlFg7i-gdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Code for plot from:\n",
        "# # http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import itertools\n",
        "# %matplotlib inline \n",
        "\n",
        "# def plot_confusion_matrix(cm, classes,\n",
        "#                           normalize = False,\n",
        "#                           title = 'Confusion matrix',\n",
        "#                           cmap = plt.cm.Blues):\n",
        "    \n",
        "    \n",
        "#     \"\"\"\n",
        "#     This function prints and plots the confusion matrix.\n",
        "#     Normalization can be applied by setting `normalize=True`.\n",
        "#     \"\"\"\n",
        "#     if normalize:\n",
        "#         cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
        "#         print(\"Normalized confusion matrix\")\n",
        "#     else:\n",
        "#         print('Confusion matrix, without normalization')\n",
        "\n",
        "#     print(cm)\n",
        "\n",
        "#     plt.imshow(cm, interpolation='nearest', cmap = cmap)\n",
        "#     plt.title(title)\n",
        "#     plt.colorbar()\n",
        "#     tick_marks = np.arange(len(classes))\n",
        "#     plt.xticks(tick_marks, classes, rotation=45)\n",
        "#     plt.yticks(tick_marks, classes)\n",
        "\n",
        "#     fmt = '.2f' if normalize else 'd'\n",
        "#     thresh = cm.max() / 2.\n",
        "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "#         plt.text(j, i, format(cm[i, j], fmt),\n",
        "#                  horizontalalignment = \"center\",\n",
        "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "#     plt.ylabel('True label')\n",
        "#     plt.xlabel('Predicted label')\n",
        "#     plt.tight_layout() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MR-tO01ge6V",
        "colab_type": "code",
        "outputId": "2b995eca-cef4-469e-f3f3-3d0a98eff4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "#print(\"Network output=\",model.predict(test_feature_matrix))\n",
        "predictions = np.argmax(bow_model.predict(test_feature_matrix), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "print(\"true labels: \\n\", test_labels)\n",
        "# target_labels = label_encoder.inverse_transform(list(target))\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"predicted labels: \\n\", predicted_labels)\n",
        "print()\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")\n",
        "\n",
        "# print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true labels: \n",
            " 0         HI  \n",
            "1         NA  \n",
            "2       DT IN \n",
            "3       DP IN \n",
            "4       DT IN \n",
            "         ...  \n",
            "1508    OB OP \n",
            "1509    MT OS \n",
            "1510    OA NA \n",
            "1511    PB NA \n",
            "1512    MT OS \n",
            "Name: label, Length: 1513, dtype: object\n",
            "predicted labels: \n",
            " ['NE NA IP IG ' 'DP IN ' 'CB NA EB IG ' ... 'ID  FC NA ' 'DS IG ' 'PO LY ']\n",
            "\n",
            "Classification accuracy:  0.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHF-M4emLu-h",
        "colab_type": "text"
      },
      "source": [
        "Why does the model perform so badly? 119 classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrzogBR6abt8",
        "colab_type": "code",
        "outputId": "2c856441-d21b-480f-8d53-1a6ba639a47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "import numpy as np\n",
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_numbers)))\n",
        "print(\"-development data: \", len(np.unique(dev_numbers)))\n",
        "print(\"-test data: \", len(np.unique(test_numbers)))\n",
        "print()\n",
        "inter = np.intersect1d(train_numbers, dev_numbers)\n",
        "inter2 = np.intersect1d(train_numbers, test_numbers)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  58\n",
            "-test data:  70\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U28dT9xjZYKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# # Confusion matrix has the true labels on rows, and predicted labels on columns in sorted order\n",
        "# print(cnf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8AaFnYV_DRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot confusion matrix\n",
        "# plt.figure()\n",
        "# plot_confusion_matrix(cnf_matrix, classes = all_labels, normalize = False)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JdOntcnMyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # np-argmaxin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "# print(predictions[0])\n",
        "# print(model.predict(test_feature_matrix)[0][25])\n",
        "# print(model.predict(test_feature_matrix)[0])\n",
        "# print(sum(model.predict(test_feature_matrix)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqG5963ZhDa",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.2: Recurrent Neural Network Classifier (multi-class)\n",
        "\n",
        "Modify your codes from milestone 1.1 to use recurrent neural networks (e.g. LSTM or biLSTM) in the classifier. Evaluate your model and report your results with different hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOfJs2YLeAtA",
        "colab_type": "text"
      },
      "source": [
        "For RNN-calssifier we use Tokenizer which turns tokens, in our case the words of training data to integers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7j5rqbNzm3O",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KY7mtEZl-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=97000, # max num of most common words\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6eKbcvnd9m4",
        "colab_type": "code",
        "outputId": "a2e3983e-7968-4b9b-9947-bb499895a737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from pprint import pprint    # pretty-printer\n",
        "\n",
        "def truncate_dict(d, count=10):\n",
        "    # Returns at most count items from the given dictionary.  \n",
        "    return dict(i for i, _ in zip(d.items(), range(count)))\n",
        "\n",
        "# Check if 0 is in the index, and print examples of the mapping\n",
        "# 0 is reserved for padding!\n",
        "print(tokenizer.word_index.get(0))\n",
        "pprint(truncate_dict(tokenizer.word_index))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "{'ei': 3,\n",
            " 'että': 4,\n",
            " 'ja': 1,\n",
            " 'kun': 9,\n",
            " 'mutta': 8,\n",
            " 'oli': 7,\n",
            " 'on': 2,\n",
            " 'ovat': 10,\n",
            " 'se': 6,\n",
            " 'tai': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDmMZZfg62w",
        "colab_type": "code",
        "outputId": "c95de254-2307-4225-d894-7c4dd12151b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "print(len(train_sequences)) \n",
        "\n",
        "# Print an example text, its corresponding sequence, and the tokens it represents\n",
        "print('Text:', train_text.head(1)[0:200]) # first item of the suffled data (index not 0!)\n",
        "print('Sequence:', train_sequences[0][:10])\n",
        "print('Mapped back:', [tokenizer.index_word[i] for i in train_sequences[0][:10]])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5295\n",
            "Text: 3982     Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "Name: text, dtype: object\n",
            "Sequence: [21429, 13274, 40282, 11697, 15340, 796, 657, 388, 12826, 3002]\n",
            "Mapped back: ['logistiikka', 'jenni', 'lindholm', 'laskutus', 'ritva', 'ota', 'yhteyttä', 'nimi', 'puhelinnumero', 'sähköposti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js1rksZ6AA6f",
        "colab_type": "code",
        "outputId": "69fc9239-11ed-4ead-dd6a-4d34b1867c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "lengths = [len(s) for s in train_sequences]\n",
        "print('Lengths:', lengths[:10], 'min:', min(lengths), 'max:', max(lengths), 'mean:', np.mean(lengths))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengths: [384, 921, 158, 194, 932, 388, 123, 384, 1577, 167] min: 0 max: 79136 mean: 583.0457034938621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcr2XZ8zg1H",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Since Keras demands for all of the input items (separate documents of our training data) to have the same length, we need to \"pad\" all but the longest document by filling in the \"missing\" number of words with zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA209Lqxzd8t",
        "colab_type": "code",
        "outputId": "d198c61e-5e0c-47e1-80b3-c9d054c7c2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequence_length = 585 # based on mean value of input length: we will cut sequences longer than this and pad with zeros sequeces shorter than this\n",
        "\n",
        "padded_X = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_X.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 585)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk9M_AtI1Jc",
        "colab_type": "code",
        "outputId": "2c8f0071-b0f2-4512-bfec-e45dc1da6d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Prepare model development data\n",
        "\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_text)\n",
        "padded_dev = pad_sequences(\n",
        "    dev_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_dev.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(756, 585)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HXbVzAF6UH",
        "colab_type": "text"
      },
      "source": [
        "# Build LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vn7FWLFGeJQ",
        "colab_type": "text"
      },
      "source": [
        "# HUOMAA!!!\n",
        "Tämä teksti alkuperäisestä RNN-classification notebookista!\n",
        "\n",
        "We define a basic RNN model that takes the RNN cell class (RNN_class) as an argument:\n",
        "\n",
        "- input: sequence of sequence_length integers corresponding to words\n",
        "- embedding: randomly initialized mapping from integers to embedding_dim-dimensional vectors\n",
        "- rnn: recurrent neural network with rnn_units-dimensional state\n",
        "- output: num_classes-dimensional fully connected layer with softmax activation\n",
        "\n",
        "# KATSO NÄITÄ!\n",
        "We're intentionally leaving out a few fairly obvious things that would be expected to help here, including\n",
        "\n",
        "- Any form of regularization, e.g. dropout\n",
        "- Initializing the embeddings with pre-trained word vectors (ks. fasttext)\n",
        "- Masking to ignore padding (see Masking and padding with Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhRaStPGH7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# We'll use these model parameters for all of our examples here.\n",
        "embedding_dim = 50 # input vector\n",
        "rnn_units = 100\n",
        "\n",
        "def build_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "    input_ = Input(shape=(sequence_length,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized. Layer turns positive integers (indexes) into dense vectors of fixed size\n",
        "    rnn = RNN_class(rnn_units)(embedding) # can support different RNNs\n",
        "    output = Dense(num_classes, activation='softmax')(rnn)\n",
        "    return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "sequence_length = padded_X.shape[1]\n",
        "vocab_size = tokenizer.num_words\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5y48zddjkf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4e017eae-e4d5-4f0b-b172-8d1be1a9e60a"
      },
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4icO32GGX1",
        "colab_type": "code",
        "outputId": "83956144-53b9-4fb0-d19f-fb1a49084b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "lstm_model = build_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 585)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 585, 50)           4850000   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 119)               12019     \n",
            "=================================================================\n",
            "Total params: 4,922,419\n",
            "Trainable params: 4,922,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikBgdQhpVwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbxk_lppo_tC",
        "colab_type": "code",
        "outputId": "38d5d593-d999-489c-b137-f021f43a0a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "lstm_history = lstm_model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 45s 9ms/sample - loss: 2.5684 - acc: 0.3707 - val_loss: 2.7282 - val_acc: 0.3135\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 44s 8ms/sample - loss: 2.2642 - acc: 0.4274 - val_loss: 2.6562 - val_acc: 0.3479\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 45s 8ms/sample - loss: 1.9368 - acc: 0.5025 - val_loss: 2.6190 - val_acc: 0.3426\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 44s 8ms/sample - loss: 1.6130 - acc: 0.5847 - val_loss: 2.7263 - val_acc: 0.3585\n",
            "Epoch 5/25\n",
            "5295/5295 [==============================] - 44s 8ms/sample - loss: 1.3635 - acc: 0.6591 - val_loss: 2.8424 - val_acc: 0.3519\n",
            "Epoch 6/25\n",
            "5295/5295 [==============================] - 45s 8ms/sample - loss: 1.1233 - acc: 0.7212 - val_loss: 2.9840 - val_acc: 0.3161\n",
            "Epoch 7/25\n",
            "5200/5295 [============================>.] - ETA: 0s - loss: 0.9027 - acc: 0.7835Restoring model weights from the end of the best epoch\n",
            "5295/5295 [==============================] - 45s 8ms/sample - loss: 0.9020 - acc: 0.7847 - val_loss: 3.2058 - val_acc: 0.3003\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXAKNMb5PbM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIycPktw_m39",
        "colab_type": "text"
      },
      "source": [
        "# Bidirectional LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFoIroA0_sMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tämä ei toimi! Virhe nimeämisestä, selvittämättä!\n",
        "# from keras.layers import Bidirectional as Bi\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCG5imFKY_Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_bi_model = build_bi_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "# lstm_bi_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1QpSnLZY6jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_bi_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xnzHNvLWwOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# # model = Sequential()\n",
        "# # model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
        "# #                         input_shape=(5, 10)))\n",
        "# # model.add(Bidirectional(LSTM(10)))\n",
        "# # model.add(Dense(5))\n",
        "# # model.add(Activation('softmax'))\n",
        "# # model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Dense(90, input_dim=sequence_length))\n",
        "# model.add(Embedding(vocab_size, embedding_dim))\n",
        "# model.add(Bidirectional(LSTM(rnn_units)))\n",
        "# model.add(Dense(num_classes))\n",
        "# model.add(Activation('softmax'))\n",
        "\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8vdw-jcfAKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "7ede13a5-6648-4cde-8471-3084225d29b5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Input(shape=(sequence_length,)\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 585, 50)           4850000   \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 200)               120800    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 119)               23919     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 119)               0         \n",
            "=================================================================\n",
            "Total params: 4,994,719\n",
            "Trainable params: 4,994,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWl2MJjjGmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "6c0e6838-c56c-476d-f87a-ba20e5ff6da9"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 585, 50)           4850000   \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 200)               120800    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 119)               23919     \n",
            "=================================================================\n",
            "Total params: 4,994,719\n",
            "Trainable params: 4,994,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7L2tJHok6_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "32a962b4-e912-431d-deb7-f9e747271d5c"
      },
      "source": [
        "hist_bi_lstm = model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5295 samples, validate on 756 samples\n",
            "Epoch 1/25\n",
            "5295/5295 [==============================] - 54s 10ms/step - loss: 3.5674 - accuracy: 0.1063 - val_loss: 3.1816 - val_accuracy: 0.1270\n",
            "Epoch 2/25\n",
            "5295/5295 [==============================] - 52s 10ms/step - loss: 3.1508 - accuracy: 0.1569 - val_loss: 3.1199 - val_accuracy: 0.1852\n",
            "Epoch 3/25\n",
            "5295/5295 [==============================] - 52s 10ms/step - loss: 3.0942 - accuracy: 0.2000 - val_loss: 3.0066 - val_accuracy: 0.2077\n",
            "Epoch 4/25\n",
            "5295/5295 [==============================] - 53s 10ms/step - loss: 2.7013 - accuracy: 0.3290 - val_loss: 2.6001 - val_accuracy: 0.3651\n",
            "Epoch 5/25\n",
            "5295/5295 [==============================] - 53s 10ms/step - loss: 2.3044 - accuracy: 0.4344 - val_loss: 2.5857 - val_accuracy: 0.3492\n",
            "Epoch 6/25\n",
            "5295/5295 [==============================] - 52s 10ms/step - loss: 1.8914 - accuracy: 0.5265 - val_loss: 2.5611 - val_accuracy: 0.3810\n",
            "Epoch 7/25\n",
            "5295/5295 [==============================] - 53s 10ms/step - loss: 1.5054 - accuracy: 0.6402 - val_loss: 2.7781 - val_accuracy: 0.3347\n",
            "Epoch 8/25\n",
            "5295/5295 [==============================] - 52s 10ms/step - loss: 1.1810 - accuracy: 0.7301 - val_loss: 2.8854 - val_accuracy: 0.3373\n",
            "Epoch 9/25\n",
            "5295/5295 [==============================] - 52s 10ms/step - loss: 0.9609 - accuracy: 0.7737 - val_loss: 2.9732 - val_accuracy: 0.3571\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1bZmgfNnWlm",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.1: Deep contextual representations with Bert (multi-class)\n",
        "Train a Bert classifier to predict the register categories. Similar to Milestone 1, the setting is multi-class, and the evaluations should include results with different hyperparameters.\n",
        "\n",
        "# Milestone 2.2: Error analysis\n",
        "Compare the errors made by the classifiers you have trained from milestones 1 and 2.1. Are there any patterns? How do the errors one model makes differ from those made by another.\n",
        "\n",
        "# Milestone 3.1: Bert (multi-LABEL)\n",
        "Train two multi-label classifiers, one using non-deep contextual representations, the other using Bert. In this setting, each label is assigned independently. Do hyperparameter optimization on these classifiers.\n",
        "\n",
        "# Milestone 3.2: Model comparison\n",
        "Compare the results of these two classifiers. Do the two models predict in the same way? Analyze the predictions in terms of label-specific differences.\n",
        "\n",
        "# Register classes and abbreviations\n"
      ]
    }
  ]
}