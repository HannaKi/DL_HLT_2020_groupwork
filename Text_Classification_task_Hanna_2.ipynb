{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Text_Classification_task_Hyperp_opt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PfqG5963ZhDa",
        "-7j5rqbNzm3O",
        "DTcr2XZ8zg1H",
        "V3HXbVzAF6UH",
        "9vn7FWLFGeJQ",
        "zIycPktw_m39",
        "tPskSfawiO5I",
        "NYSi-7hwqqTs",
        "i-HcrqoxqxiI",
        "Y1bZmgfNnWlm"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HannaKi/priva_DL_HLT/blob/master/Text_Classification_task_Hyperp_opt_One_hot_encodings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VGkCcicEVPSF"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "This project is about text classification. You will develop a text classification system that identifies different kinds of online texts, such as news, blogs and opinionated texts. We will refer to these text categories as registers. If you want to learn more about online registers and their automatic identification, you can read, e.g., our paper [Toward Multilingual Identification of Online Registers] (https://www.aclweb.org/anthology/W19-6130/).\n",
        "\n",
        "# Data and register labels\n",
        "The data for this project consist of ~7500 documents with manual annotations on their register. You can download it from http://dl.turkunlp.org/TKO_8965-projects/classification/ . The documents are based on a (almost) random sample of the Finnish Internet. The registers are identified using a relatively detailed, hierarchical taxonomy. The taxonomy consists of 8 main categories that are divided into a large number of subregisters. The taxonomy is described at the end of this page. The table includes also the abbreviations that are used in the data.\n",
        "\n",
        "The challenge with online documents is that it is not always easy to identify the specific registers categories of the documents. Furthermore, another issue is that a document may display characteristics of several registers. For instance, a blog post may simultaneously seem like a product review. To deal with these challenges, we have followed the following guidelines:\n",
        "* For each document, the annotators have aimed at marking the specific subregister category. When this is possible, the document has two register labels: the subregister label and the main register label to which the subregister belongs. For instance, a document annotated as a news article would have the label NE for News and the corresponding higher level register label NA for Narrative. \n",
        "* In some cases, the document does not seem to fit any of the subregisters. In this case, the document can be given only one label: the main register label, such as NA for Narrative. \n",
        "* Some documents may display characteristics of several register categories. In this case, the annotator can mark several register labels for one single document. Consequently, the document may have up to four labels. This would be the case case if a document is annotated both as a Personal blog (subregister label PB + corresponding higher level register label NA) and Review (subregister label RV + corresponding higher level register label OP).\n",
        "\n",
        "\n",
        "# Register classes and abbreviations\n",
        "\n",
        "NA Narrative\n",
        "\n",
        "* NE NA    New reports / news blogs\n",
        "* SR NA    Sports reports\n",
        "* PB NA    Personal blog\n",
        "* HA NA    Historical article\n",
        "* FC NA    Fiction\n",
        "* TB NA    Travel blog\n",
        "* CB NA    Community blogs\n",
        "* OA NA    Online article\n",
        "\n",
        "OP  Opinion\n",
        "* OB OP  Personal opinion blogs\n",
        "* RV OP  Reviews\n",
        "* RS OP  Religious blogs/sermons\n",
        "* AV OP  Advice\n",
        "\n",
        "IN Informational description\n",
        "* JD IN  Job description\n",
        "* FA IN  FAQs\n",
        "* DT IN  Description of a thing\n",
        "* IB IN  Information blogs\n",
        "* DP IN  Description of a person\n",
        "* RA IN  Research articles\n",
        "* LT IN  Legal terms / conditions\n",
        "* CM IN  Course materials\n",
        "* EN IN  Encyclopedia articles\n",
        "* RP IN  Report\n",
        "\n",
        "ID Interactive discussion\n",
        "* DF ID  Discussion forums\n",
        "* QA ID  Question-answer forums\n",
        "\n",
        "HI  How-to/instructions\n",
        "* RE HI  Recipes\n",
        "\n",
        "IP IG  Informational persuasion\n",
        "* DS IG  Description with intent to sell\n",
        "* EB IG  News-opinion blogs / editorials\n",
        "\n",
        "Lyrical LY\n",
        "* PO LY  Poems\n",
        "* SL LY  Songs\n",
        "\n",
        "Spoken SP\n",
        "* IT SP Interviews\n",
        "* FS SP Formal speeches\n",
        "\n",
        "Others OS\n",
        "* MT OS Machine-translated / generated texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1_c9eZnWlG",
        "colab_type": "text"
      },
      "source": [
        "# Preparations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1s5CYypylU1",
        "colab_type": "text"
      },
      "source": [
        "Import packages, set up TF version 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsWUzLj5NOG",
        "colab_type": "code",
        "outputId": "dc6aa2b1-46b5-4dff-ff81-066b1b9275d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Get rid of old tf at some point!\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# to run with old tf with which the code was made\n",
        "# The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.\n",
        "# https://colab.research.google.com/notebooks/tensorflow_version.ipynb\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svG58YMGXsNr",
        "colab_type": "code",
        "outputId": "a494ecdb-4de1-4ea4-b142-6a44fe5e5ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from datetime import datetime\n",
        "# import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7bPGqEaPC7",
        "colab_type": "text"
      },
      "source": [
        "For saving the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYdjqCtdaNJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "df45d6bd-fc3b-40ee-e491-e60eac42d6c7"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERs_vfbfabkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%script bash\n",
        "mkdir -p models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIHMK6DxXy6r",
        "colab_type": "text"
      },
      "source": [
        "Download and open data, explore it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHGSBx5nWlM",
        "colab_type": "code",
        "outputId": "dd58efae-0adc-4a17-bdd1-39f9075cf08e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "source": [
        "# Download development data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
        "# Download test data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
        "# Download train data\n",
        "!wget http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-05 18:21:47--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-dev.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4035578 (3.8M) [application/octet-stream]\n",
            "Saving to: ‘fincore-dev.tsv’\n",
            "\n",
            "fincore-dev.tsv     100%[===================>]   3.85M  1.64MB/s    in 2.3s    \n",
            "\n",
            "2020-05-05 18:21:50 (1.64 MB/s) - ‘fincore-dev.tsv’ saved [4035578/4035578]\n",
            "\n",
            "--2020-05-05 18:21:52--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-test.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8512687 (8.1M) [application/octet-stream]\n",
            "Saving to: ‘fincore-test.tsv’\n",
            "\n",
            "fincore-test.tsv    100%[===================>]   8.12M  1.85MB/s    in 4.4s    \n",
            "\n",
            "2020-05-05 18:21:57 (1.85 MB/s) - ‘fincore-test.tsv’ saved [8512687/8512687]\n",
            "\n",
            "--2020-05-05 18:21:58--  http://dl.turkunlp.org/TKO_8965-projects/classification/fincore-train.tsv\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29379580 (28M) [application/octet-stream]\n",
            "Saving to: ‘fincore-train.tsv’\n",
            "\n",
            "fincore-train.tsv   100%[===================>]  28.02M  4.45MB/s    in 8.1s    \n",
            "\n",
            "2020-05-05 18:22:07 (3.46 MB/s) - ‘fincore-train.tsv’ saved [29379580/29379580]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8TezCY6pR1",
        "colab_type": "text"
      },
      "source": [
        "Data split\n",
        "\n",
        "- Training set: used for training the models\n",
        "- Validation (or model assessment) set: when comparing\n",
        "di\u000berent models trained on training set, select one with lowest\n",
        "error on validation set\n",
        "- Test set: test the \fnal hypothesis on test set to get unbiased\n",
        "error estimate for it\n",
        "\n",
        "After error estimation the \fnal model is often trained on\n",
        "combined training, validation and test set, using best\n",
        "hyperparameters found during model selection\n",
        "- Complication: randomized optimization approaches where\n",
        "di\u000berent runs with same hyperparameters can lead to very\n",
        "di\u000berent quality solutions (e.g. neural network)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KciwyumAEz_b",
        "colab_type": "code",
        "outputId": "c031d1bf-72a5-4706-b7a9-2bee773649be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "train = pd.read_csv('fincore-train.tsv', sep='\\t', header=None)\n",
        "\n",
        "train = train.sample(frac=1, random_state = 4) # suffle the data\n",
        "train.columns = ['label','text']\n",
        "print(\"train data\")\n",
        "print(train.head())\n",
        "print(train.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(train['label'].value_counts())\n",
        "\n",
        "dev = pd.read_csv('fincore-dev.tsv', sep='\\t', header=None)\n",
        "dev.columns = ['label','text']\n",
        "print(\"dev data\")\n",
        "print(dev.head())\n",
        "print(dev.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(dev['label'].value_counts())\n",
        "\n",
        "test = pd.read_csv('fincore-test.tsv', sep='\\t', header=None)\n",
        "test.columns = ['label','text']\n",
        "print(\"test data\")\n",
        "print(test.head())\n",
        "print(test.shape)\n",
        "#print(\"Label counts:\")\n",
        "# print(test['label'].value_counts())\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data\n",
            "       label                                               text\n",
            "3982  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...\n",
            "2640  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...\n",
            "119   NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...\n",
            "4916  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...\n",
            "775   MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...\n",
            "(5295, 2)\n",
            "dev data\n",
            "    label                                               text\n",
            "0  OA NA    Luonnonhoito Maaperän siemenpankkia avattiin ...\n",
            "1  DS IG    • Jokainen ripsi on erittäin kevyt ja muodolt...\n",
            "2  DS IG    Mukavuudet Hotel Dila Vain muutaman metrin pä...\n",
            "3  DF ID    Vastaa viestiin Otsikko Viesti ensin omaishoi...\n",
            "4  OA NA    Dinosaur Jr 30.5.2010 Tavastia , Helsinki 198...\n",
            "(756, 2)\n",
            "test data\n",
            "    label                                               text\n",
            "0    HI     Tehkää nollaleimaus . Jos rekisteröinti onnis...\n",
            "1    NA     1 kommenttia : Syyslomallelähtijät kirjoitti ...\n",
            "2  DT IN    Ammattikoulutuksen perustana on ajatus siitä ...\n",
            "3  DP IN    Ulkonäkö : Silveriä voisi kuvata tietyllä tap...\n",
            "4  DT IN    Laulupelimannien puheenjohtajina ovat toimine...\n",
            "(1513, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwxqapN9J7PH",
        "colab_type": "code",
        "outputId": "571b373c-1ce1-47fc-e2c9-b31b082c5642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Prepare stratified data sets for training, development and testing:\n",
        "# Stratification aims to ensure that all the data sets (train, development and test) have the same distribution of labels. \n",
        "# This minimizes chances that a model has to try to predict labes it has not seen during training.\n",
        "\n",
        "# test-train-split causes error: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
        "# Best solution: More data\n",
        "# Second best solution: If you cannot have another dataset, you will have to play with what you have. Suggestion: remove the sample that has the lonely target. \n",
        "\n",
        "# Join all the data and re-divide it with stratification and to illustrate how skewed the data is!\n",
        "train_e = train\n",
        "train_e['data'] = 'train'\n",
        "\n",
        "dev_e = dev\n",
        "dev_e['data'] = 'dev'\n",
        "\n",
        "test_e = test\n",
        "test_e['data'] = 'test'\n",
        "\n",
        "frames = [train_e, dev_e, test_e]\n",
        "df = pd.concat(frames).reset_index(drop=True) # get rid of the old indexes\n",
        "\n",
        "# Separating out the target\n",
        "y = df['label'] # pd df\n",
        "# # Separating out the features\n",
        "X = df['text'] # pd df\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "nums = dict(zip(unique, counts))\n",
        "\n",
        "df.head()\n",
        "\n",
        "#pprint(sorted(nums.items(), key = lambda kv:(kv[1], kv[0])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWSDvMthCZFl",
        "colab_type": "code",
        "outputId": "64a63bf2-78da-4a04-a43c-ab624b8a16f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "plt.figure(figsize=(26, 6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.countplot(x=\"label\", hue=\"data\", data=df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fca67e23978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1872x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuGhgHP5DJkZ",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the distribution of labels is highly skewed. This might be fixed with data augmentation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzVzQOvux5lJ",
        "colab_type": "text"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "Since we now know that 'MT OS ' is the most common label in the data with 957 occurences, we can set a naive baseline prediction: We will predict that an unlabeled new text belongs to this biggest class. In this case our prediction accuracy is the pure share of the biggest class in the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vU1dkUOyhQH",
        "colab_type": "code",
        "outputId": "cfd5c6d6-bd13-4b73-97b0-950da924606c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification baseline: \", round((957/df.shape[0])*100,1), \"percent\") # here the original data without modifications for training of classificators "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification baseline:  12.7 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh_AQXZLHkXz",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.1: Bag-of-words classifier (multi-class)\n",
        "\n",
        "Train a bag-of-words classifier to predict the register categories. In this milestone, the setting is multi-class, so the register label combinations form the classes, e.g. NA_NE and NA_NE_OP_OB. \n",
        "\n",
        "- Evaluate your model and report your results with different hyperparameters\n",
        "- Ideas to try:\n",
        "  - Different activation functions\n",
        "  - Altering the learning rate\n",
        "  - Use different optimizers\n",
        "  - Adjusting the vocabulary size of the embeddings\n",
        "\n",
        "- Activation functions and optimizers supported by Keras can be found here: https://keras.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FGFhSxaVye",
        "colab_type": "text"
      },
      "source": [
        "Bow classifier is only interested in the multiplicity or appearance of words (or to be precise n-garms). Hence we loose the textual context and order of the words (n-grams). This inevitably leads to some information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLA0Alu_y6c2",
        "colab_type": "text"
      },
      "source": [
        "## Data filtering  for milestones 1 and 2\n",
        "\n",
        "Since we can not train the model to predict labels it has not seen during training phase, we will keep only rows in dev and test data which have the labels that appear also in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j80A7RBn0KOY",
        "colab_type": "code",
        "outputId": "25c76694-132a-4712-982c-b3ddc0f72934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "# Optional way to treat data if stratification is bad idea...\n",
        "\n",
        "# Gather features and labels of the data\n",
        "\n",
        "# Separate text and the associated label\n",
        "train_text = train['text']\n",
        "train_labels = train['label']\n",
        "\n",
        "# print(train_text.head())\n",
        "# print(train_labels.head())\n",
        "# print()\n",
        "\n",
        "dev_text = dev['text']\n",
        "dev_labels = dev['label']\n",
        "\n",
        "test_text = test['text']\n",
        "test_labels = test['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "all_labels = pd.concat(labels)\n",
        "\n",
        "print(all_labels.head(10))\n",
        "print()\n",
        "print(\"Number of unique labels in data: \", len(all_labels.unique()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3982    DS IG \n",
            "2640    RS OP \n",
            "119     NE NA \n",
            "4916    SR NA \n",
            "775     MT OS \n",
            "5264      NA  \n",
            "5040    DP IN \n",
            "3446    DS IG \n",
            "216     DF ID \n",
            "863     DT IN \n",
            "Name: label, dtype: object\n",
            "\n",
            "Number of unique labels in data:  119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8zX0o3bxaA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "09d8817c-a1ce-4fa1-f2dc-55006d651555"
      },
      "source": [
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_labels)))\n",
        "print(\"-development data: \", len(np.unique(dev_labels)))\n",
        "print(\"-test data: \", len(np.unique(test_labels)))\n",
        "print()\n",
        "inter = np.intersect1d(train_labels, dev_labels)\n",
        "inter2 = np.intersect1d(train_labels, test_labels)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  58\n",
            "-test data:  70\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RtQipNUc-7S",
        "colab_type": "code",
        "outputId": "4a232a2f-3a64-4454-f705-174990ec7b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Filter out labels not found in training data\n",
        "\n",
        "test_ = test[test['label'].isin(train_labels.tolist())]\n",
        "dev_ = dev[dev['label'].isin(train_labels.tolist())] \n",
        "\n",
        "print(test_.shape)\n",
        "print(dev_.shape)\n",
        "\n",
        "dev_text = dev_['text'] # development data for milestones 1 and 2\n",
        "dev_labels = dev_['label']\n",
        "\n",
        "test_text = test_['text'] # test data for milestones 1 and 2\n",
        "test_labels = test_['label']\n",
        "\n",
        "labels = [train_labels, dev_labels, test_labels]\n",
        "labels = pd.concat(labels)\n",
        "\n",
        "print(\"Number of unique labels in data: \", len(labels.unique()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 3)\n",
            "(750, 3)\n",
            "Number of unique labels in data:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7VZBC6QCWVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "d3ed041c-57d0-44a3-b8eb-f6ce13c05bbf"
      },
      "source": [
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_labels)))\n",
        "print(\"-development data: \", len(np.unique(dev_labels)))\n",
        "print(\"-test data: \", len(np.unique(test_labels)))\n",
        "print()\n",
        "inter = np.intersect1d(train_labels, dev_labels)\n",
        "inter2 = np.intersect1d(train_labels, test_labels)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in\n",
            "-train data:  100\n",
            "-development data:  52\n",
            "-test data:  57\n",
            "\n",
            "Number of shared labels in\n",
            "-train and development data:  52\n",
            "-train and test data:  57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ3qKWoKbeEU",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation, step  1: form feature matrix\n",
        "\n",
        "We will use CountVectorizer / TfidfVectorizer from sklearn package to transform out text data to numerical format with which our classifier is able to deal with. CountVectorizer converts the collection of text documents (our training data) to a matrix of token counts. Since we are only interested in whether a particular word of the vocabulary is in a single document or not, our vectorizer is set on \"binary\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt1fNJ7ovGge",
        "colab_type": "code",
        "outputId": "03cd8a9a-96f6-4aac-d53f-91c6444a19f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 20000, ngram_range = (1,1)) # Grid search fails: OOM when allocating tensor of shape [85000,200]\n",
        "\n",
        "# form feature matrix\n",
        "train_feature_matrix = vectorizer.fit_transform(train_text)\n",
        "dev_feature_matrix = vectorizer.transform(dev_text)\n",
        "test_feature_matrix = vectorizer.transform(test_text)\n",
        "\n",
        "print(\"shape of the training data: \", train_feature_matrix.shape)\n",
        "print(\"shape of the development data: \", dev_feature_matrix.shape)\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5295, 20000)\n",
            "shape of the development data:  (750, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ymmdQP4aq2E",
        "colab_type": "text"
      },
      "source": [
        "The shape of the feature matrix tells us that we have XXX items (documents) in our training data. The number of unique n-grams exceeds XXXX but we are including only the first XXXX most common of them. Since our CountVectorizer has parameter setting \"ngram_range = 1, 1\" this means we are forming the vector with unigrams, separate words or charachters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vTY0uwjd25w",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation, step 2: Label encoding and one hot encoding\n",
        "\n",
        "Next we will encode the labels. This means transforming the textual labels to numeric values, which our model is able to deal with. This step is made with LabelEncoder class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_yBCz62umP",
        "colab_type": "code",
        "outputId": "b8cc60d2-a574-414f-9c0a-d52881612a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "# label encoding \n",
        "\n",
        "label_encoder = LabelEncoder() # Create the instance of LabelEncoder we use to turn class labels into integers\n",
        "\n",
        "train_numbers = label_encoder.fit_transform(train_labels) # encode labels to integers\n",
        "dev_numbers = label_encoder.transform(dev_labels) \n",
        "test_numbers = label_encoder.transform(test_labels) \n",
        "\n",
        "print(\"Inverse transform gives unique labels in each data set: \", label_encoder.inverse_transform(train_numbers))\n",
        "print(\"Sanity checks, do we have as many labels and texts in our data sets?\")\n",
        "print(\"Train data: \", len(train_numbers), len(train_text))\n",
        "print(\"Dev data: \", len(dev_numbers), len(dev_text))\n",
        "print(\"Test data: \", len(test_numbers), len(test_text))\n",
        "\n",
        "# one hot encoding\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_hot=one_hot_encoder.fit_transform(train_numbers.reshape(-1,1))\n",
        "dev_hot=one_hot_encoder.transform(dev_numbers.reshape(-1,1))\n",
        "test_hot=one_hot_encoder.transform(test_numbers.reshape(-1,1))\n",
        "print()\n",
        "print(train_hot.shape)\n",
        "print(dev_hot.shape)\n",
        "print(test_hot.shape)\n",
        "train_hot"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inverse transform gives unique labels in each data set:  ['DS IG ' 'RS OP ' 'NE NA ' ... 'DS IG ' 'MT OS ' 'MT OS ']\n",
            "Sanity checks, do we have as many labels and texts in our data sets?\n",
            "Train data:  5295 5295\n",
            "Dev data:  750 750\n",
            "Test data:  1500 1500\n",
            "\n",
            "(5295, 100)\n",
            "(750, 100)\n",
            "(1500, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3RDtU7J2di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LabelEncoderin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode tee koodaus\n",
        "# print(\"Koodaukset: \", list(le.classes_))\n",
        "# print()\n",
        "# le.transform([\"tokyo\", \"tokyo\", \"paris\"]) # käytä koodausta arvojen transformointiin --> numeeriset arvot\n",
        "# num = le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
        "# print(\"Sovitetulla encoderilla tuotetut numeeriset arvot muuttujalistasta: \", num)\n",
        "# print()\n",
        "# print(\"Inverse transform: \", list(le.inverse_transform([2, 2, 1])))\n",
        "# print()\n",
        "# test = le.fit_transform([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit kolmella\", test)\n",
        "\n",
        "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"helsinki\"])\n",
        "# #print(list(le.classes_))\n",
        "\n",
        "# test = le.fit_transform([\"paris\", \"helsinki\", \"tokyo\", \"amsterdam\"]) # encode and transform\n",
        "# print(\"Koodit neljällä\", test)\n",
        "\n",
        "# print(list(le.inverse_transform([2, 2, 1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9_7r6FeRS5",
        "colab_type": "text"
      },
      "source": [
        "Now the data is prepared for milestone 1.\n",
        "\n",
        "## Cross validation and hyperparameter optimization with Gridsearch\n",
        "\n",
        "Join train and dev data for CV grid search.\n",
        "\n",
        "Approach: Select prominent optimizers and optimize rest of the hyperparams for it. Since other hyperparams affect the optimizer performance it is not a \"fair competition\" to optimize optimizers with all the rest hyperparams set to some values. We have to deal with memory constraints also. This significantly limits the set of hyperparametrs we can optimize at once.\n",
        "\n",
        "Here Adam and SGD are explored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAoJ6ZArns0j",
        "colab_type": "code",
        "outputId": "41058cb0-fcc8-48c0-ec40-966da64018cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "# features \n",
        "frames = [train, dev_]\n",
        "joint = pd.concat(frames)\n",
        "joint_feature_matrix = vectorizer.fit_transform(joint['text'])\n",
        "\n",
        "# labels, one hot encoded\n",
        "print(type(train_hot))\n",
        "print(train_hot.shape)\n",
        "print(dev_hot.shape)\n",
        "joint_labels= np.concatenate((train_hot, dev_hot))\n",
        "print(joint_labels.shape)\n",
        "\n",
        "print(joint_feature_matrix.shape)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(5295, 100)\n",
            "(750, 100)\n",
            "(6045, 100)\n",
            "(6045, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWUSL5YoSARM",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer: SGD\n",
        "\n",
        "### Step 1.1 Build and compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cdWVH9iQpVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_count, feature_count = joint_feature_matrix.shape\n",
        "class_count = len(labels.unique())\n",
        "\n",
        "def create_bow_model_SGD(learning_rate=0.01, nodes=200):\n",
        "  inp = Input(shape=(feature_count,))\n",
        "  hidden1 = Dense(nodes, activation=\"relu\")(inp)\n",
        " # hidden2 = Dense(nodes, activation=\"relu\")(hidden1) # multiple hidden layers possible but slow down hyperparameter optimization\n",
        "  dropout = Dropout(0.3)(hidden1)\n",
        "  outp = Dense(class_count, activation='softmax')(dropout)\n",
        "  model = Model(inputs=[inp], outputs=[outp])\n",
        "  optimizer = optimizers.SGD(learning_rate=learning_rate) # also hyperparameter momentum could be optimized\n",
        "  model.compile(optimizer = optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkzqF6nkT98F",
        "colab_type": "code",
        "outputId": "0dc59dca-742d-4d3c-e07b-d4ff3d5ccff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "model = create_bow_model_SGD()\n",
        "model.summary()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               4000200   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "=================================================================\n",
            "Total params: 4,020,300\n",
            "Trainable params: 4,020,300\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjQl3cVXXota",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.2: GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KInxI7pznHD",
        "colab_type": "code",
        "outputId": "8ac15ba4-9b68-4ded-fa6d-1c1a170dedcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnyQupms1ln9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%script bash\n",
        "mkdir -p models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNXj8TvOQN6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tensorflow.compat.v1.disable_eager_execution()\n",
        "stop_cb = EarlyStopping(monitor = 'accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# If the error only occurs when you use smaller datasets, you're very likely using datasets small enough to not have a single sample in the validation set. \n",
        "\n",
        "bow_model_SGD = KerasClassifier(build_fn=create_bow_model_SGD, epochs=40, batch_size=32, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvHQGoNB0q9e",
        "colab_type": "code",
        "outputId": "88593b52-8012-4ed5-b62f-f0a8f8468011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = datetime.now()\n",
        "print(\"start time =\", start)\n",
        "\n",
        "# set the hyperparameter: example: np.logspace(-10, 1, num = 7, base = 2)\n",
        "learning_rate = np.logspace(-7, 0, num = 5, base = 2).tolist()\n",
        "params = dict(learning_rate = learning_rate)\n",
        "\n",
        "grid = GridSearchCV(estimator = bow_model_SGD, param_grid = params, n_jobs = 1, cv = 3 , verbose  = 1)  # cv: For integer/None inputs, if the estimator is a classifier and y is \n",
        "                                                                                                        # either binary or multiclass, StratifiedKFold is used.\n",
        "X = joint_feature_matrix.toarray()\n",
        "    \n",
        "grid_result_SGD = grid.fit(X, joint_numbers, callbacks=[stop_cb, mc_cb])\n",
        "\n",
        "end = datetime.now()\n",
        "print(\"end time =\", end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start time = 2020-05-01 10:54:05.028913\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 273us/step - loss: 4.5955 - accuracy: 0.0787\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.5714 - accuracy: 0.1328\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.5463 - accuracy: 0.1340\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 4.5197 - accuracy: 0.1380\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4928 - accuracy: 0.1385\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4656 - accuracy: 0.1355\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.4367 - accuracy: 0.1414\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4078 - accuracy: 0.1422\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.3778 - accuracy: 0.1370\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.3469 - accuracy: 0.1372\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.3147 - accuracy: 0.1375\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.2808 - accuracy: 0.1380\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 4.2448 - accuracy: 0.1367\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.2087 - accuracy: 0.1350\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.1698 - accuracy: 0.1370\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1279 - accuracy: 0.1367\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.0828 - accuracy: 0.1355\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.0411 - accuracy: 0.1370\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.9944 - accuracy: 0.1370\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.9491 - accuracy: 0.1340\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8981 - accuracy: 0.1370\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.8489 - accuracy: 0.1370\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8041 - accuracy: 0.1385\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7584 - accuracy: 0.1330\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7140 - accuracy: 0.1347\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.6775 - accuracy: 0.1328\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 3.6398 - accuracy: 0.1333\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.6073 - accuracy: 0.1290\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00028: early stopping\n",
            "2015/2015 [==============================] - 1s 362us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 280us/step - loss: 4.5940 - accuracy: 0.0794\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5681 - accuracy: 0.1295\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5404 - accuracy: 0.1313\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.5117 - accuracy: 0.1278\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4821 - accuracy: 0.1303\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.4521 - accuracy: 0.1305\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.4210 - accuracy: 0.1238\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.3893 - accuracy: 0.1256\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 4.3556 - accuracy: 0.1223\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.3208 - accuracy: 0.1261\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.2853 - accuracy: 0.1253\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 4.2472 - accuracy: 0.1223\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.2067 - accuracy: 0.1231\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1631 - accuracy: 0.1236\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.1222 - accuracy: 0.1223\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.0756 - accuracy: 0.1213\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 4.0311 - accuracy: 0.1231\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.9801 - accuracy: 0.1191\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.9317 - accuracy: 0.1231\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8820 - accuracy: 0.1213\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.8359 - accuracy: 0.1206\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.7866 - accuracy: 0.1233\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.7439 - accuracy: 0.1223\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00023: early stopping\n",
            "2015/2015 [==============================] - 1s 370us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 276us/step - loss: 4.5909 - accuracy: 0.1122\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.5637 - accuracy: 0.1330\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.5335 - accuracy: 0.1318\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.5024 - accuracy: 0.1325\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.4696 - accuracy: 0.1323\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.4358 - accuracy: 0.1323\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.4015 - accuracy: 0.1325\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.3664 - accuracy: 0.1323\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 148us/step - loss: 4.3290 - accuracy: 0.1323\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.2906 - accuracy: 0.1323\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 4.2499 - accuracy: 0.1323\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 4.2083 - accuracy: 0.1323\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 4.1619 - accuracy: 0.1323\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.1156 - accuracy: 0.1323\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.0661 - accuracy: 0.1323\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.0153 - accuracy: 0.1323\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.9642 - accuracy: 0.1325\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.9124 - accuracy: 0.1323\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.8603 - accuracy: 0.1323\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.8083 - accuracy: 0.1323\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7589 - accuracy: 0.1323\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7127 - accuracy: 0.1320\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00022: early stopping\n",
            "2015/2015 [==============================] - 1s 347us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 273us/step - loss: 4.5639 - accuracy: 0.1226\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.4678 - accuracy: 0.1288\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 4.3597 - accuracy: 0.1283\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.2368 - accuracy: 0.1283\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 4.0924 - accuracy: 0.1283\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.9304 - accuracy: 0.1283\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.7719 - accuracy: 0.1283\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.6367 - accuracy: 0.1283\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.5401 - accuracy: 0.1303\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.4633 - accuracy: 0.1315\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.4147 - accuracy: 0.1419\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3753 - accuracy: 0.1414\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.3407 - accuracy: 0.1449\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3157 - accuracy: 0.1474\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2879 - accuracy: 0.1603\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.2673 - accuracy: 0.1533\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2559 - accuracy: 0.1593\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.2456 - accuracy: 0.1573\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.2311 - accuracy: 0.1633\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.2205 - accuracy: 0.1586\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.2107 - accuracy: 0.1618\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1988 - accuracy: 0.1707\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1939 - accuracy: 0.1650\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1850 - accuracy: 0.1677\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1841 - accuracy: 0.1819\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1686 - accuracy: 0.1839\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1664 - accuracy: 0.1789\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1558 - accuracy: 0.1898\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1573 - accuracy: 0.1816\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1473 - accuracy: 0.1792\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.1459 - accuracy: 0.1931\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1362 - accuracy: 0.1903\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 149us/step - loss: 3.1370 - accuracy: 0.1903\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1245 - accuracy: 0.2030\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 150us/step - loss: 3.1287 - accuracy: 0.2040\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.1166 - accuracy: 0.1965\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 151us/step - loss: 3.1153 - accuracy: 0.2060\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 152us/step - loss: 3.1076 - accuracy: 0.2176\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1028 - accuracy: 0.2082\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.0989 - accuracy: 0.2129\n",
            "2015/2015 [==============================] - 1s 360us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 308us/step - loss: 4.5604 - accuracy: 0.1084\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.4510 - accuracy: 0.1263\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 4.3284 - accuracy: 0.1194\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 4.1866 - accuracy: 0.1221\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 4.0256 - accuracy: 0.1226\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.8519 - accuracy: 0.1201\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.6923 - accuracy: 0.1213\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.5733 - accuracy: 0.1258\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.4928 - accuracy: 0.1263\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.4376 - accuracy: 0.1335\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.3943 - accuracy: 0.1337\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.3574 - accuracy: 0.1315\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3316 - accuracy: 0.1370\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.3045 - accuracy: 0.1412\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2892 - accuracy: 0.1409\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2701 - accuracy: 0.1352\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2526 - accuracy: 0.1424\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2475 - accuracy: 0.1382\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2353 - accuracy: 0.1504\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2234 - accuracy: 0.1496\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.2227 - accuracy: 0.1514\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.2132 - accuracy: 0.1471\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2006 - accuracy: 0.1558\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1969 - accuracy: 0.1566\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1891 - accuracy: 0.1603\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1820 - accuracy: 0.1553\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.1776 - accuracy: 0.1588\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1722 - accuracy: 0.1670\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1666 - accuracy: 0.1715\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1563 - accuracy: 0.1734\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1546 - accuracy: 0.1710\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1554 - accuracy: 0.1653\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1483 - accuracy: 0.1772\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1442 - accuracy: 0.1787\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1409 - accuracy: 0.1844\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.1399 - accuracy: 0.1752\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 154us/step - loss: 3.1322 - accuracy: 0.1901\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1252 - accuracy: 0.1950\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 155us/step - loss: 3.1229 - accuracy: 0.1886\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 153us/step - loss: 3.1191 - accuracy: 0.1841\n",
            "2015/2015 [==============================] - 1s 368us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 292us/step - loss: 4.5551 - accuracy: 0.1313\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 4.4396 - accuracy: 0.1476\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 4.3093 - accuracy: 0.1496\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 4.1581 - accuracy: 0.1489\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.9852 - accuracy: 0.1504\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.8041 - accuracy: 0.1367\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.6473 - accuracy: 0.1437\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.5358 - accuracy: 0.1432\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.4634 - accuracy: 0.1432\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.4072 - accuracy: 0.1499\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.3683 - accuracy: 0.1427\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.3325 - accuracy: 0.1459\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.3051 - accuracy: 0.1548\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2868 - accuracy: 0.1533\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2653 - accuracy: 0.1563\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2471 - accuracy: 0.1568\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.2373 - accuracy: 0.1563\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 163us/step - loss: 3.2265 - accuracy: 0.1536\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.2120 - accuracy: 0.1603\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.2049 - accuracy: 0.1620\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1996 - accuracy: 0.1667\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1886 - accuracy: 0.1685\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1835 - accuracy: 0.1727\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1742 - accuracy: 0.1737\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1747 - accuracy: 0.1754\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1642 - accuracy: 0.1754\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1561 - accuracy: 0.1829\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1492 - accuracy: 0.1794\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1410 - accuracy: 0.1908\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.1381 - accuracy: 0.1906\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1369 - accuracy: 0.1888\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1336 - accuracy: 0.1826\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1253 - accuracy: 0.1998\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1213 - accuracy: 0.1913\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1108 - accuracy: 0.2062\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.1145 - accuracy: 0.2079\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.1050 - accuracy: 0.2060\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0991 - accuracy: 0.2082\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0982 - accuracy: 0.2117\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.0924 - accuracy: 0.2072\n",
            "2015/2015 [==============================] - 1s 367us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 300us/step - loss: 4.4479 - accuracy: 0.1251\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 4.0008 - accuracy: 0.1280\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.5616 - accuracy: 0.1290\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 3.3820 - accuracy: 0.1407\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2938 - accuracy: 0.1538\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2451 - accuracy: 0.1603\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2065 - accuracy: 0.1695\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 3.1815 - accuracy: 0.1697\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1643 - accuracy: 0.1801\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.1492 - accuracy: 0.1968\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.1296 - accuracy: 0.2020\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1086 - accuracy: 0.2154\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.0908 - accuracy: 0.2233\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.0811 - accuracy: 0.2342\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0627 - accuracy: 0.2434\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0432 - accuracy: 0.2462\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 3.0202 - accuracy: 0.2581\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.9936 - accuracy: 0.2749\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.9722 - accuracy: 0.2759\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9437 - accuracy: 0.2859\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9298 - accuracy: 0.2898\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.8986 - accuracy: 0.2975\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.8764 - accuracy: 0.3035\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.8438 - accuracy: 0.3151\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.8265 - accuracy: 0.3176\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.7936 - accuracy: 0.3305\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.7654 - accuracy: 0.3370\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.7404 - accuracy: 0.3395\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.7157 - accuracy: 0.3536\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.6954 - accuracy: 0.3596\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.6755 - accuracy: 0.3556\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.6498 - accuracy: 0.3672\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.6343 - accuracy: 0.3720\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.6053 - accuracy: 0.3846\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5855 - accuracy: 0.3826\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5657 - accuracy: 0.3896\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.5489 - accuracy: 0.3960\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.5315 - accuracy: 0.4000\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 2.5057 - accuracy: 0.4015\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 164us/step - loss: 2.4955 - accuracy: 0.4030\n",
            "2015/2015 [==============================] - 1s 377us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 292us/step - loss: 4.4613 - accuracy: 0.1124\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 4.0571 - accuracy: 0.1206\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.5967 - accuracy: 0.1275\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.3862 - accuracy: 0.1414\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2978 - accuracy: 0.1397\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2459 - accuracy: 0.1449\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2165 - accuracy: 0.1556\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1922 - accuracy: 0.1692\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1709 - accuracy: 0.1777\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1599 - accuracy: 0.1794\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.1414 - accuracy: 0.1864\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1267 - accuracy: 0.1993\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1054 - accuracy: 0.2042\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.0947 - accuracy: 0.2144\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0702 - accuracy: 0.2288\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.0579 - accuracy: 0.2261\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0347 - accuracy: 0.2409\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0195 - accuracy: 0.2548\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9923 - accuracy: 0.2655\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9740 - accuracy: 0.2640\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.9487 - accuracy: 0.2744\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9253 - accuracy: 0.2811\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9001 - accuracy: 0.2960\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.8733 - accuracy: 0.2953\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.8416 - accuracy: 0.3124\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8174 - accuracy: 0.3228\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.7881 - accuracy: 0.3385\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7618 - accuracy: 0.3352\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7404 - accuracy: 0.3407\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.7135 - accuracy: 0.3509\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6874 - accuracy: 0.3573\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.6646 - accuracy: 0.3638\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.6444 - accuracy: 0.3667\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6199 - accuracy: 0.3774\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6014 - accuracy: 0.3787\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5787 - accuracy: 0.3881\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.5562 - accuracy: 0.3933\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.5377 - accuracy: 0.3931\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.5271 - accuracy: 0.3916\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5014 - accuracy: 0.4007\n",
            "2015/2015 [==============================] - 1s 380us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 293us/step - loss: 4.4408 - accuracy: 0.1283\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.9804 - accuracy: 0.1323\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.5475 - accuracy: 0.1350\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.3675 - accuracy: 0.1380\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 3.2821 - accuracy: 0.1442\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.2304 - accuracy: 0.1531\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 3.2043 - accuracy: 0.1650\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.1718 - accuracy: 0.1655\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1617 - accuracy: 0.1749\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1428 - accuracy: 0.1769\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1266 - accuracy: 0.1958\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.1162 - accuracy: 0.1861\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.0931 - accuracy: 0.2089\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.0755 - accuracy: 0.2179\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 3.0602 - accuracy: 0.2283\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0457 - accuracy: 0.2459\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0189 - accuracy: 0.2514\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0054 - accuracy: 0.2605\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9789 - accuracy: 0.2749\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.9561 - accuracy: 0.2849\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9364 - accuracy: 0.2868\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.9032 - accuracy: 0.3022\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8899 - accuracy: 0.3032\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8568 - accuracy: 0.3141\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8279 - accuracy: 0.3266\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.7980 - accuracy: 0.3310\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.7746 - accuracy: 0.3362\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.7536 - accuracy: 0.3489\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.7240 - accuracy: 0.3531\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6980 - accuracy: 0.3615\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6778 - accuracy: 0.3690\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6527 - accuracy: 0.3715\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6297 - accuracy: 0.3801\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.6043 - accuracy: 0.3878\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5872 - accuracy: 0.3854\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5666 - accuracy: 0.4010\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5426 - accuracy: 0.4035\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.5224 - accuracy: 0.4037\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.4969 - accuracy: 0.4112\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 2.4872 - accuracy: 0.4164\n",
            "2015/2015 [==============================] - 1s 379us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 305us/step - loss: 3.9474 - accuracy: 0.1238\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.2762 - accuracy: 0.1452\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 3.1772 - accuracy: 0.1638\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 3.1176 - accuracy: 0.1963\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 3.0618 - accuracy: 0.2266\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.9878 - accuracy: 0.2588\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.9086 - accuracy: 0.2826\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.8199 - accuracy: 0.3055\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.7373 - accuracy: 0.3395\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.6550 - accuracy: 0.3571\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.5899 - accuracy: 0.3797\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.5296 - accuracy: 0.3851\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.4699 - accuracy: 0.4047\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 2.4257 - accuracy: 0.4099\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.3732 - accuracy: 0.4300\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.3298 - accuracy: 0.4372\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.2773 - accuracy: 0.4434\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.2452 - accuracy: 0.4509\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 2.2084 - accuracy: 0.4581\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.1798 - accuracy: 0.4630\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 2.1373 - accuracy: 0.4759\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 2.1014 - accuracy: 0.4811\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.0704 - accuracy: 0.4911\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 2.0372 - accuracy: 0.4968\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 2.0119 - accuracy: 0.5119\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 170us/step - loss: 1.9791 - accuracy: 0.5139\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.9604 - accuracy: 0.5169\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.9210 - accuracy: 0.5320\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.8881 - accuracy: 0.5439\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.8580 - accuracy: 0.5546\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 1.8280 - accuracy: 0.5643\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 166us/step - loss: 1.7989 - accuracy: 0.5685\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.7690 - accuracy: 0.5854\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 168us/step - loss: 1.7335 - accuracy: 0.5928\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.7047 - accuracy: 0.6022\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.6761 - accuracy: 0.6082\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 1.6499 - accuracy: 0.6169\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.6250 - accuracy: 0.6241\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 167us/step - loss: 1.5933 - accuracy: 0.6365\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 165us/step - loss: 1.5701 - accuracy: 0.6417\n",
            "2015/2015 [==============================] - 1s 395us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 296us/step - loss: 3.9822 - accuracy: 0.1159\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.2829 - accuracy: 0.1370\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 3.1852 - accuracy: 0.1548\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1309 - accuracy: 0.1881\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0800 - accuracy: 0.2040\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.0153 - accuracy: 0.2439\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.9382 - accuracy: 0.2737\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.8585 - accuracy: 0.2995\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.7669 - accuracy: 0.3293\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6840 - accuracy: 0.3610\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.6083 - accuracy: 0.3702\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.5403 - accuracy: 0.3873\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.4867 - accuracy: 0.4007\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.4288 - accuracy: 0.4141\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 162us/step - loss: 2.3861 - accuracy: 0.4258\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.3329 - accuracy: 0.4325\n",
            "Epoch 17/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2913 - accuracy: 0.4447\n",
            "Epoch 18/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2506 - accuracy: 0.4533\n",
            "Epoch 19/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.2092 - accuracy: 0.4667\n",
            "Epoch 20/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 2.1777 - accuracy: 0.4692\n",
            "Epoch 21/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.1412 - accuracy: 0.4764\n",
            "Epoch 22/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.1031 - accuracy: 0.4913\n",
            "Epoch 23/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.0672 - accuracy: 0.4926\n",
            "Epoch 24/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.0394 - accuracy: 0.5042\n",
            "Epoch 25/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 2.0045 - accuracy: 0.5119\n",
            "Epoch 26/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.9659 - accuracy: 0.5223\n",
            "Epoch 27/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.9356 - accuracy: 0.5357\n",
            "Epoch 28/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 1.9099 - accuracy: 0.5439\n",
            "Epoch 29/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.8734 - accuracy: 0.5553\n",
            "Epoch 30/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.8371 - accuracy: 0.5717\n",
            "Epoch 31/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.8073 - accuracy: 0.5777\n",
            "Epoch 32/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.7806 - accuracy: 0.5878\n",
            "Epoch 33/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.7415 - accuracy: 0.5935\n",
            "Epoch 34/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.7223 - accuracy: 0.6089\n",
            "Epoch 35/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 1.6936 - accuracy: 0.6156\n",
            "Epoch 36/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 1.6646 - accuracy: 0.6288\n",
            "Epoch 37/40\n",
            "4030/4030 [==============================] - 1s 160us/step - loss: 1.6329 - accuracy: 0.6310\n",
            "Epoch 38/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5962 - accuracy: 0.6467\n",
            "Epoch 39/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5811 - accuracy: 0.6469\n",
            "Epoch 40/40\n",
            "4030/4030 [==============================] - 1s 161us/step - loss: 1.5584 - accuracy: 0.6548\n",
            "2015/2015 [==============================] - 1s 384us/step\n",
            "Epoch 1/40\n",
            "4030/4030 [==============================] - 1s 296us/step - loss: 3.9500 - accuracy: 0.1333\n",
            "Epoch 2/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.2708 - accuracy: 0.1439\n",
            "Epoch 3/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 3.1771 - accuracy: 0.1707\n",
            "Epoch 4/40\n",
            "4030/4030 [==============================] - 1s 169us/step - loss: 3.1172 - accuracy: 0.1918\n",
            "Epoch 5/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0631 - accuracy: 0.2248\n",
            "Epoch 6/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 3.0014 - accuracy: 0.2459\n",
            "Epoch 7/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.9247 - accuracy: 0.2809\n",
            "Epoch 8/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.8372 - accuracy: 0.3057\n",
            "Epoch 9/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.7553 - accuracy: 0.3345\n",
            "Epoch 10/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.6773 - accuracy: 0.3553\n",
            "Epoch 11/40\n",
            "4030/4030 [==============================] - 1s 156us/step - loss: 2.5942 - accuracy: 0.3777\n",
            "Epoch 12/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.5318 - accuracy: 0.3918\n",
            "Epoch 13/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.4712 - accuracy: 0.4032\n",
            "Epoch 14/40\n",
            "4030/4030 [==============================] - 1s 157us/step - loss: 2.4164 - accuracy: 0.4208\n",
            "Epoch 15/40\n",
            "4030/4030 [==============================] - 1s 159us/step - loss: 2.3725 - accuracy: 0.4283\n",
            "Epoch 16/40\n",
            "4030/4030 [==============================] - 1s 158us/step - loss: 2.3298 - accuracy: 0.4404\n",
            "Epoch 17/40\n",
            "3300/4030 [=======================>......] - ETA: 0s - loss: 2.3048 - accuracy: 0.4473"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f129a95d4879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_feature_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_result_SGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_numbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 184\u001b[0;31m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0m\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLoC5f_WW42",
        "colab_type": "code",
        "outputId": "3cbdf930-6683-47d7-8747-15854e5419f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result_SGD.best_score_, grid_result_SGD.best_params_))\n",
        "\n",
        "means = grid_result_SGD.cv_results_['mean_test_score']\n",
        "stds = grid_result_SGD.cv_results_['std_test_score']\n",
        "params = grid_result_SGD.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.553846 using {'learning_rate': 1.0}\n",
            "0.130356 (0.007660) with: {'learning_rate': 0.0078125}\n",
            "0.234243 (0.024834) with: {'learning_rate': 0.026278012976678578}\n",
            "0.406286 (0.022258) with: {'learning_rate': 0.08838834764831845}\n",
            "0.526882 (0.004518) with: {'learning_rate': 0.29730177875068026}\n",
            "0.553846 (0.003991) with: {'learning_rate': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XK-bINWst5",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.3. Predict \n",
        "\n",
        "Now we can predict directly with optimized hyperparameters by calling \"grid_result.predict\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdrHjQcjWrKU",
        "colab_type": "code",
        "outputId": "1eb8799a-2a59-4aa7-eb86-85e674ddf0de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "print(test_feature_matrix.shape)\n",
        "\n",
        "predictions_SGD = grid_result_SGD.predict(test_feature_matrix)\n",
        "predicted_labels_SGD = label_encoder.inverse_transform(list(predictions_SGD))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_SGD)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 20000)\n",
            "1500/1500 [==============================] - 0s 202us/step\n",
            "Classification accuracy:  6.4 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngH0B3sRYcES",
        "colab_type": "text"
      },
      "source": [
        "### Nested CV with SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCmyydMlYf81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# How to choose number of splits: https://machinelearningmastery.com/k-fold-cross-validation/\n",
        "\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "# Inner CV: optimize number hyperparameter\n",
        "# Set up possible values of parameters to optimize over\n",
        "learning_rate = np.logspace(-7, 0, num = 5, base = 2).tolist()\n",
        "params = dict(learning_rate = learning_rate)\n",
        "\n",
        "nested_grid_SGD = GridSearchCV(estimator=bow_model_SGD, param_grid=params, n_jobs=1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "nested_grid_result_SGD = nested_grid_SGD.fit(X, joint_numbers, callbacks = [stop_cb])\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGlQVhbxbDML",
        "colab_type": "code",
        "outputId": "2a25ee71-a620-47a0-ca5b-5fe6cb435b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (nested_grid_result_SGD.best_score_, nested_grid_result_SGD.best_params_))\n",
        "\n",
        "means = nested_grid_result_SGD.cv_results_['mean_test_score']\n",
        "stds = nested_grid_result_SGD.cv_results_['std_test_score']\n",
        "params = nested_grid_result_SGD.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.557982 using {'learning_rate': 1.0}\n",
            "0.119438 (0.005350) with: {'learning_rate': 0.0078125}\n",
            "0.236559 (0.048157) with: {'learning_rate': 0.026278012976678578}\n",
            "0.409429 (0.003865) with: {'learning_rate': 0.08838834764831845}\n",
            "0.528371 (0.003851) with: {'learning_rate': 0.29730177875068026}\n",
            "0.557982 (0.005905) with: {'learning_rate': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkJ7wPURapWO",
        "colab_type": "code",
        "outputId": "15f828ff-3a42-433b-996f-38d444c7bdbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predictions_nested_SGD = nested_grid_result_SGD.predict(test_feature_matrix)\n",
        "predicted_labels_nested_SGD = label_encoder.inverse_transform(list(predictions_nested_SGD))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_nested_SGD)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500/1500 [==============================] - 1s 489us/step\n",
            "Classification accuracy:  7.4 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5JoCfs3LiU4",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tyq5l2bLT3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = joint_feature_matrix.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY2kbDqScdLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_count, feature_count = joint_feature_matrix.shape\n",
        "class_count = len(train_labels.unique())\n",
        "\n",
        "def create_bow_model_Adam(learning_rate=0.01, nodes=200):\n",
        "  inp = Input(shape=(feature_count,))\n",
        "  hidden1 = Dense(nodes, activation=\"relu\")(inp)\n",
        "  hidden2 = Dense(nodes, activation=\"relu\")(hidden1)                                          # two hidden layers\n",
        "  dropout = Dropout(0.3)(hidden2)\n",
        "  outp = Dense(class_count, activation='softmax')(dropout)                                    # also dropoutrate could be optimized\n",
        "  model = Model(inputs=[inp], outputs=[outp])\n",
        "  optimizer = optimizers.Adam(learning_rate=learning_rate)                                    # also hyperparameter momentum could be optimized\n",
        "  model.compile(optimizer = optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy']) # for one hot encodings\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz9l0NRWkRkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "b3b03a2f-a6f1-4c3d-cf2d-8aa0f420220b"
      },
      "source": [
        "model = create_bow_model_Adam()\n",
        "model.summary()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 200)               4000200   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               20100     \n",
            "=================================================================\n",
            "Total params: 4,060,500\n",
            "Trainable params: 4,060,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqtIMrYQftS2",
        "colab_type": "text"
      },
      "source": [
        "## Nested cross-validation\n",
        "- nested, aka external cross-validation\n",
        "- cross-validation within cross-validation\n",
        "- inner loop for model selection, outer for evaluation\n",
        "- Split the data to K non-overlapping folds\n",
        "1. designate i:th fold as the outer cross-validation test fold, set it\n",
        "aside\n",
        "2. use the remaining K-1 folds to perform model selection with\n",
        "cross-validation as usual\n",
        "3. train the model with selected parameters on the K-1 training\n",
        "folds\n",
        "4. compute predictions using the remaining ith fold as test set\n",
        "I afterwards collect the together predictions made at step 4 for\n",
        "all the K folds and compute your error rate or other\n",
        "performance measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmKrE2zoblrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7741249c-5d80-4a48-c0b0-76c6f31347ed"
      },
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
        "# https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/\n",
        "# https://mlfromscratch.com/nested-cross-validation-python-code/#/\n",
        "\n",
        "# Choose cross-validation techniques for the inner and outer loops\n",
        "# outer loop: X folds\n",
        "# inner loop: each fold of outer loop is folded again! To avoid too heavy computaion do not fold too many times or try running parallel \n",
        "# (n_jobs=-1): parallel, (n_jobs=1): sequential\n",
        "\n",
        "start = datetime.now()\n",
        "print(\"start time =\", start)\n",
        "\n",
        "bow_model_Adam = KerasClassifier(build_fn=create_bow_model_Adam, epochs=40, batch_size=100, verbose=0)\n",
        "\n",
        "# Call backs won't work / not needed with gridsearch... \n",
        "# stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "# Warning: 'val_accuracy' not available: Stack overflow search suggestion: \"If the error only occurs when you use smaller datasets, \n",
        "# you're very likely using datasets small enough to not have a single sample in the validation set. \"\n",
        "# mc_cb = ModelCheckpoint(filepath='models/bow_model.h5', monitor='val_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "# Warning: memory leak\n",
        "\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "learning_rate = [0.005, 0.001, 0.01, 0.1]\n",
        "epochs = [20, 40]\n",
        "params = dict(learning_rate=learning_rate, epochs = epochs )\n",
        "\n",
        "nested_grid_Adam = GridSearchCV(estimator=bow_model_Adam, param_grid=params, n_jobs=-1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "nested_grid_result_Adam = nested_grid_Adam.fit(X, joint_labels, verbose = 1, callbacks=[mc_cb])\n",
        "\n",
        "end = datetime.now()\n",
        "print(\"start time =\", end)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start time = 2020-05-05 16:06:13.958064\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6045 samples\n",
            "Epoch 1/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 3.5174 - acc: 0.2223WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 2s 295us/sample - loss: 3.4626 - acc: 0.2328\n",
            "Epoch 2/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 2.1627 - acc: 0.4897WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 2.1551 - acc: 0.4907\n",
            "Epoch 3/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 1.5263 - acc: 0.6478WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 128us/sample - loss: 1.5263 - acc: 0.6467\n",
            "Epoch 4/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 1.0318 - acc: 0.7658WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 128us/sample - loss: 1.0313 - acc: 0.7658\n",
            "Epoch 5/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.6426 - acc: 0.8642WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.6438 - acc: 0.8653\n",
            "Epoch 6/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.3970 - acc: 0.9312WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 0.3959 - acc: 0.9327\n",
            "Epoch 7/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9640WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.2492 - acc: 0.9643\n",
            "Epoch 8/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.1708 - acc: 0.9739WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 131us/sample - loss: 0.1689 - acc: 0.9745\n",
            "Epoch 9/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 0.1264 - acc: 0.9800WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 127us/sample - loss: 0.1270 - acc: 0.9800\n",
            "Epoch 10/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9846WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0982 - acc: 0.9846\n",
            "Epoch 11/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9865WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0795 - acc: 0.9858\n",
            "Epoch 12/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0654 - acc: 0.9889WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 124us/sample - loss: 0.0649 - acc: 0.9891\n",
            "Epoch 13/40\n",
            "5800/6045 [===========================>..] - ETA: 0s - loss: 0.0557 - acc: 0.9893WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.0554 - acc: 0.9894\n",
            "Epoch 14/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9908WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 125us/sample - loss: 0.0463 - acc: 0.9909\n",
            "Epoch 15/40\n",
            "5900/6045 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9919WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0380 - acc: 0.9917\n",
            "Epoch 16/40\n",
            "6000/6045 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9937WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 126us/sample - loss: 0.0346 - acc: 0.9935\n",
            "Epoch 17/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0308 - acc: 0.9946WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 131us/sample - loss: 0.0306 - acc: 0.9945\n",
            "Epoch 18/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0204 - acc: 0.9975WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 132us/sample - loss: 0.0230 - acc: 0.9970\n",
            "Epoch 19/40\n",
            "5700/6045 [===========================>..] - ETA: 0s - loss: 0.0211 - acc: 0.9965WARNING:tensorflow:Can save best model only with accuracy available, skipping.\n",
            "6045/6045 [==============================] - 1s 133us/sample - loss: 0.0202 - acc: 0.9967\n",
            "Epoch 20/40\n",
            "4500/6045 [=====================>........] - ETA: 0s - loss: 0.0165 - acc: 0.9978"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-bd735814dff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnested_grid_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbow_model_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnested_grid_result_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_grid_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3459\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3461\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhzTAqY7brC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "ea930ab0-0384-4386-eef6-62f89f13ace7"
      },
      "source": [
        "print(\"Best: %f using %s\" % (nested_grid_result_Adam.best_score_, nested_grid_result_Adam.best_params_))\n",
        "\n",
        "means = nested_grid_result_Adam.cv_results_['mean_test_score']\n",
        "stds = nested_grid_result_Adam.cv_results_['std_test_score']\n",
        "params = nested_grid_result_Adam.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "print()\n",
        "predictions_nested_Adam = nested_grid_result_Adam.predict(test_feature_matrix)\n",
        "predicted_labels_nested_Adam = label_encoder.inverse_transform(list(predictions_nested_Adam))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels_nested_Adam)*100,1), \"percent\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.548883 using {'epochs': 20, 'learning_rate': 0.001}\n",
            "0.535318 (0.004333) with: {'epochs': 20, 'learning_rate': 0.005}\n",
            "0.548883 (0.007505) with: {'epochs': 20, 'learning_rate': 0.001}\n",
            "0.522581 (0.010179) with: {'epochs': 20, 'learning_rate': 0.01}\n",
            "0.303888 (0.021451) with: {'epochs': 20, 'learning_rate': 0.1}\n",
            "0.190240 (0.041566) with: {'epochs': 20, 'learning_rate': 0.2}\n",
            "0.532672 (0.014120) with: {'epochs': 40, 'learning_rate': 0.005}\n",
            "0.541935 (0.008314) with: {'epochs': 40, 'learning_rate': 0.001}\n",
            "0.489992 (0.001638) with: {'epochs': 40, 'learning_rate': 0.01}\n",
            "0.338792 (0.025922) with: {'epochs': 40, 'learning_rate': 0.1}\n",
            "0.119107 (0.009950) with: {'epochs': 40, 'learning_rate': 0.2}\n",
            "\n",
            "Classification accuracy:  13.8 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug4n17gTf9Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This took like ages and ended up with disconnected runtime!!!!!!!!!!!!!!\n",
        "\n",
        "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\n",
        "# # https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/\n",
        "# # https://mlfromscratch.com/nested-cross-validation-python-code/#/\n",
        "\n",
        "# # Choose cross-validation techniques for the inner and outer loops\n",
        "# # outer loop: N folds\n",
        "# # inner loop: each fold of outer loop is folded again! To avoid too heavy computaion do not fold too many times or try running parallel\n",
        "\n",
        "# bow_model_Adam = KerasClassifier(build_fn=create_bow_model_Adam, epochs=40, batch_size=100, verbose=0)\n",
        "\n",
        "# stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "# mc_cb = ModelCheckpoint(filepath='models/bow_model.h5', monitor='accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "# inner_cv = KFold(n_splits=3, shuffle=True, random_state=4) # Should use StratifiedKFold but can not because the smallest classes!\n",
        "# outer_cv = KFold(n_splits=3, shuffle=True, random_state=4)\n",
        "\n",
        "# # Set the hyperparameter grid. Also possible hyperparams (when parametrized for the functions called!):\n",
        "# # - dropout rate\n",
        "# # - if optimizer is SGD, momentum\n",
        "# # - max_feat (input shape)\n",
        "# nodes = [100, 200]\n",
        "# epochs = [40, 64]\n",
        "# batch_size = [4, 10, 32]\n",
        "# learning_rate = [0.0001, 0.001, 0.01]\n",
        "# params = dict(learning_rate = learning_rate, nodes = nodes, epochs = epochs, batch_size = batch_size)\n",
        "\n",
        "# nested_grid_Adam = GridSearchCV(estimator=bow_model_Adam, param_grid=params, n_jobs=1, cv=inner_cv, verbose = 1)\n",
        "\n",
        "# nested_grid_result_Adam = nested_grid_Adam.fit(X, joint_labels, verbose = 1) #, callbacks=[stop_cb])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2HwFxZzwOP",
        "colab_type": "text"
      },
      "source": [
        "## Fitting BOW-classifier OLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mQnwLuHcDntV"
      },
      "source": [
        "Now we will fit the data. Here we will also need the validation data. \n",
        "Let's try with different optimizers available in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwm-SO6ciArq",
        "colab_type": "code",
        "outputId": "a8221790-2150-41f9-d989-c844c7fdfc39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "example_count, feature_count = train_feature_matrix.shape\n",
        "class_count = len(train_labels.unique()) \n",
        "\n",
        "inp = Input(shape = (feature_count, ))                   # Tuple. The size of the inputlayer is the number of the vectors\n",
        "hidden = Dense(300, activation=\"relu\")(inp)  \n",
        "hidden2 = Dense(300, activation=\"relu\")(hidden)\n",
        "dropout = Dropout(0.3)(hidden2)                          # Non-linear activation function. tanh or relu? \n",
        "outp = Dense(class_count, activation=\"softmax\")(hidden2) # As many output possibilities as we have input classes. \n",
        "                                                         # Softmax: produces probability distribution of the classes\n",
        "bow_model = Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "bow_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               6000300   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               30100     \n",
            "=================================================================\n",
            "Total params: 6,120,700\n",
            "Trainable params: 6,120,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SM_ApKFQ0b0",
        "colab_type": "code",
        "outputId": "c70c7131-710d-4558-99b1-6fa665e9e392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Choose best OP and LR and fit the model\n",
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=40, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "bow_model.compile(optimizer = 'Adam', loss=\"categorical_crossentropy\", metrics=['accuracy']) # with one-hot encodings loss = categorical_crossentropy!!!\n",
        "\n",
        "bow_history = bow_model.fit(train_feature_matrix, train_hot, batch_size=32, \n",
        "                 verbose=1, epochs=100, validation_data=(dev_feature_matrix, dev_hot), callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 5295 samples, validate on 750 samples\n",
            "Epoch 1/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 2.6677 - accuracy: 0.3804 - val_loss: 1.9671 - val_accuracy: 0.5320\n",
            "Epoch 2/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 1.2771 - accuracy: 0.7005 - val_loss: 1.7759 - val_accuracy: 0.5733\n",
            "Epoch 3/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.4830 - accuracy: 0.8997 - val_loss: 1.8270 - val_accuracy: 0.5640\n",
            "Epoch 4/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.1798 - accuracy: 0.9726 - val_loss: 1.9260 - val_accuracy: 0.5627\n",
            "Epoch 5/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0993 - accuracy: 0.9817 - val_loss: 1.9714 - val_accuracy: 0.5747\n",
            "Epoch 6/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0696 - accuracy: 0.9858 - val_loss: 2.1091 - val_accuracy: 0.5547\n",
            "Epoch 7/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0519 - accuracy: 0.9883 - val_loss: 2.1371 - val_accuracy: 0.5680\n",
            "Epoch 8/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0404 - accuracy: 0.9924 - val_loss: 2.1620 - val_accuracy: 0.5733\n",
            "Epoch 9/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0256 - accuracy: 0.9962 - val_loss: 2.2527 - val_accuracy: 0.5547\n",
            "Epoch 10/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0174 - accuracy: 0.9960 - val_loss: 2.2290 - val_accuracy: 0.5747\n",
            "Epoch 11/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0137 - accuracy: 0.9968 - val_loss: 2.2201 - val_accuracy: 0.5667\n",
            "Epoch 12/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0119 - accuracy: 0.9957 - val_loss: 2.3929 - val_accuracy: 0.5333\n",
            "Epoch 13/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 2.2785 - val_accuracy: 0.5613\n",
            "Epoch 14/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 2.3964 - val_accuracy: 0.5600\n",
            "Epoch 15/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 2.3295 - val_accuracy: 0.5640\n",
            "Epoch 16/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 2.4246 - val_accuracy: 0.5467\n",
            "Epoch 17/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 2.6989 - val_accuracy: 0.5347\n",
            "Epoch 18/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 2.5332 - val_accuracy: 0.5440\n",
            "Epoch 19/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0080 - accuracy: 0.9966 - val_loss: 2.4533 - val_accuracy: 0.5587\n",
            "Epoch 20/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 2.4860 - val_accuracy: 0.5507\n",
            "Epoch 21/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0079 - accuracy: 0.9972 - val_loss: 2.4765 - val_accuracy: 0.5573\n",
            "Epoch 22/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0066 - accuracy: 0.9975 - val_loss: 2.5494 - val_accuracy: 0.5627\n",
            "Epoch 23/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0071 - accuracy: 0.9958 - val_loss: 2.5375 - val_accuracy: 0.5613\n",
            "Epoch 24/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 2.5119 - val_accuracy: 0.5613\n",
            "Epoch 25/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0057 - accuracy: 0.9974 - val_loss: 2.5735 - val_accuracy: 0.5587\n",
            "Epoch 26/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0073 - accuracy: 0.9966 - val_loss: 2.5949 - val_accuracy: 0.5520\n",
            "Epoch 27/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0061 - accuracy: 0.9970 - val_loss: 2.6181 - val_accuracy: 0.5613\n",
            "Epoch 28/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 2.6174 - val_accuracy: 0.5507\n",
            "Epoch 29/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0062 - accuracy: 0.9974 - val_loss: 2.6717 - val_accuracy: 0.5573\n",
            "Epoch 30/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0063 - accuracy: 0.9972 - val_loss: 2.7221 - val_accuracy: 0.5520\n",
            "Epoch 31/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0060 - accuracy: 0.9968 - val_loss: 2.7433 - val_accuracy: 0.5587\n",
            "Epoch 32/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0059 - accuracy: 0.9977 - val_loss: 2.7393 - val_accuracy: 0.5587\n",
            "Epoch 33/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0052 - accuracy: 0.9974 - val_loss: 2.7310 - val_accuracy: 0.5627\n",
            "Epoch 34/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0052 - accuracy: 0.9972 - val_loss: 2.9057 - val_accuracy: 0.5493\n",
            "Epoch 35/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0058 - accuracy: 0.9970 - val_loss: 2.7401 - val_accuracy: 0.5587\n",
            "Epoch 36/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0051 - accuracy: 0.9979 - val_loss: 2.8739 - val_accuracy: 0.5387\n",
            "Epoch 37/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0061 - accuracy: 0.9972 - val_loss: 2.8903 - val_accuracy: 0.5427\n",
            "Epoch 38/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0059 - accuracy: 0.9974 - val_loss: 2.8280 - val_accuracy: 0.5453\n",
            "Epoch 39/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0055 - accuracy: 0.9970 - val_loss: 3.0793 - val_accuracy: 0.5507\n",
            "Epoch 40/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0064 - accuracy: 0.9970 - val_loss: 2.9128 - val_accuracy: 0.5533\n",
            "Epoch 41/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0054 - accuracy: 0.9974 - val_loss: 2.8703 - val_accuracy: 0.5587\n",
            "Epoch 42/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0054 - accuracy: 0.9970 - val_loss: 2.9243 - val_accuracy: 0.5587\n",
            "Epoch 43/100\n",
            "5295/5295 [==============================] - 14s 3ms/step - loss: 0.0046 - accuracy: 0.9972 - val_loss: 3.0769 - val_accuracy: 0.5480\n",
            "Epoch 44/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0046 - accuracy: 0.9979 - val_loss: 2.9347 - val_accuracy: 0.5573\n",
            "Epoch 45/100\n",
            "5295/5295 [==============================] - 13s 3ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 2.9236 - val_accuracy: 0.5667\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00045: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_zfbGvKjIL1",
        "colab_type": "code",
        "outputId": "eaf0ecb8-308b-427b-a0c7-2340ab33d4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# form feature matrix for test data set\n",
        "test_feature_matrix = vectorizer.fit_transform(test_text)\n",
        "print(test_feature_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MR-tO01ge6V",
        "colab_type": "code",
        "outputId": "8fec90f7-944c-4b4a-f9c5-ea6b22ac6487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "predictions = np.argmax(bow_model.predict(test_feature_matrix), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"predicted labels: \\n\", predicted_labels)\n",
        "print()\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: \n",
            " ['NE NA ' 'NA  ' 'NA  ' ... 'NA  ' 'NA  ' 'PB NA ']\n",
            "\n",
            "Classification accuracy:  9.0 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqmyFiO0CSrd",
        "colab_type": "code",
        "outputId": "a16ba3a7-26b2-486c-9f1d-c95fc30b1cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "# Without np.argmax:\n",
        "\n",
        "# predictions = bow_model.predict(test_feature_matrix) \n",
        "# predictions = one_hot_encoder.inverse_transform(predictions) # transfer form one hot encodings to numerical labels \n",
        "# predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "# print(\"predicted labels: \\n\", predicted_labels)\n",
        "# print()\n",
        "# print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: \n",
            " ['NE NA ' 'NA  ' 'NA  ' ... 'NA  ' 'NA  ' 'PB NA ']\n",
            "\n",
            "Classification accuracy:  9.0 percent\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHF-M4emLu-h",
        "colab_type": "text"
      },
      "source": [
        "Why does the model perform so badly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrzogBR6abt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"Number of unique labels in\")\n",
        "print(\"-train data: \", len(np.unique(train_numbers)))\n",
        "print(\"-development data: \", len(np.unique(dev_numbers)))\n",
        "print(\"-test data: \", len(np.unique(test_numbers)))\n",
        "print()\n",
        "inter = np.intersect1d(train_numbers, dev_numbers)\n",
        "inter2 = np.intersect1d(train_numbers, test_numbers)\n",
        "print(\"Number of shared labels in\")\n",
        "print(\"-train and development data: \", len(inter))\n",
        "print(\"-train and test data: \", len(inter2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U28dT9xjZYKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# # Confusion matrix has the true labels on rows, and predicted labels on columns in sorted order\n",
        "# print(cnf_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8AaFnYV_DRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot confusion matrix\n",
        "# plt.figure()\n",
        "# plot_confusion_matrix(cnf_matrix, classes = all_labels, normalize = False)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JdOntcnMyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # np-argmaxin testailua ÄLÄ HÄVITÄ!!!!!!\n",
        "# print(predictions[0])\n",
        "# print(model.predict(test_feature_matrix)[0][25])\n",
        "# print(model.predict(test_feature_matrix)[0])\n",
        "# print(sum(model.predict(test_feature_matrix)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqG5963ZhDa",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1.2: Recurrent Neural Network Classifier (multi-class)\n",
        "\n",
        "Modify your codes from milestone 1.1 to use recurrent neural networks (e.g. LSTM or biLSTM) in the classifier. Evaluate your model and report your results with different hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOfJs2YLeAtA",
        "colab_type": "text"
      },
      "source": [
        "For RNN-calssifier we use Tokenizer which turns tokens, in our case the words of training data to integers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7j5rqbNzm3O",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2KY7mtEZl-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=97000, # max num of most common words\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6eKbcvnd9m4",
        "colab_type": "code",
        "outputId": "931e1f43-ad55-4aba-fb02-8cc6863e06fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from pprint import pprint    # pretty-printer\n",
        "\n",
        "def truncate_dict(d, count=10):\n",
        "    # Returns at most count items from the given dictionary.  \n",
        "    return dict(i for i, _ in zip(d.items(), range(count)))\n",
        "\n",
        "# Check if 0 is in the index, and print examples of the mapping\n",
        "# 0 is reserved for padding!\n",
        "print(tokenizer.word_index.get(0))\n",
        "pprint(truncate_dict(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "{'ei': 3,\n",
            " 'että': 4,\n",
            " 'ja': 1,\n",
            " 'kun': 9,\n",
            " 'mutta': 8,\n",
            " 'oli': 7,\n",
            " 'on': 2,\n",
            " 'ovat': 10,\n",
            " 'se': 6,\n",
            " 'tai': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDmMZZfg62w",
        "colab_type": "code",
        "outputId": "dac7f186-30cf-42ee-8f87-84545360ed40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "print(len(train_sequences)) \n",
        "\n",
        "# Print an example text, its corresponding sequence, and the tokens it represents\n",
        "print('Text:', train_text[0][0:200]) # first item of the suffled data (index not 0!)\n",
        "#print('Text:', train_text.head(1)[0:200]) # first item of the suffled data (index not 0!)\n",
        "print('Sequence:', train_sequences[0][:10])\n",
        "print('Mapped back:', [tokenizer.index_word[i] for i in train_sequences[0][:10]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5295\n",
            "Text:  Tämähän menee ihan hurjaksi muotihurjasteluksi . Jo toistamiseen tälle syksylle postauksen aiheena vaatetus . Mutta tämä liittyy tavallaan matkailuun . Ja katsokaa nyt näitä , mitkä maailman ihanimma\n",
            "Sequence: [21429, 13274, 40282, 11697, 15340, 796, 657, 388, 12826, 3002]\n",
            "Mapped back: ['logistiikka', 'jenni', 'lindholm', 'laskutus', 'ritva', 'ota', 'yhteyttä', 'nimi', 'puhelinnumero', 'sähköposti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js1rksZ6AA6f",
        "colab_type": "code",
        "outputId": "1acf0d02-6125-42e2-c3e3-f9d5bfb2b2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "lengths = [len(s) for s in train_sequences]\n",
        "print('Lengths:', lengths[:10], 'min:', min(lengths), 'max:', max(lengths), 'mean:', np.mean(lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lengths: [384, 921, 158, 194, 932, 388, 123, 384, 1577, 167] min: 0 max: 79136 mean: 583.0457034938621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcr2XZ8zg1H",
        "colab_type": "text"
      },
      "source": [
        "## Padding\n",
        "\n",
        "Since Keras demands for all of the input items (separate documents of our training data) to have the same length, we need to \"pad\" all but the longest document by filling in the \"missing\" number of words with zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA209Lqxzd8t",
        "colab_type": "code",
        "outputId": "09347272-6e76-4e09-eab6-b391a06c77af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequence_length = np.floor(np.mean(lengths)).astype(int) # based on mean value of input length: we will cut sequences longer than this and pad with zeros sequeces shorter than this\n",
        "\n",
        "type(sequence_length)\n",
        "\n",
        "padded_X = pad_sequences(\n",
        "    train_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk9M_AtI1Jc",
        "colab_type": "code",
        "outputId": "79c30b03-d73f-430b-9fb4-d151a98a6f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Prepare model development data\n",
        "\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_text)\n",
        "padded_dev = pad_sequences(\n",
        "    dev_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_dev.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(750, 583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HXbVzAF6UH",
        "colab_type": "text"
      },
      "source": [
        "## LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vn7FWLFGeJQ",
        "colab_type": "text"
      },
      "source": [
        "## HUOMAA!!!\n",
        "Tämä teksti alkuperäisestä RNN-classification notebookista!\n",
        "\n",
        "We define a basic RNN model that takes the RNN cell class (RNN_class) as an argument:\n",
        "\n",
        "- input: sequence of sequence_length integers corresponding to words\n",
        "- embedding: randomly initialized mapping from integers to embedding_dim-dimensional vectors\n",
        "- rnn: recurrent neural network with rnn_units-dimensional state\n",
        "- output: num_classes-dimensional fully connected layer with softmax activation\n",
        "\n",
        "# KATSO NÄITÄ!\n",
        "We're intentionally leaving out a few fairly obvious things that would be expected to help here, including\n",
        "\n",
        "- Any form of regularization, e.g. dropout\n",
        "- Initializing the embeddings with pre-trained word vectors (ks. fasttext)\n",
        "- Masking to ignore padding (see Masking and padding with Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJhRaStPGH7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "# We'll use these model parameters for all of our examples here.\n",
        "embedding_dim = 50 # input vector\n",
        "rnn_units = 100\n",
        "\n",
        "def build_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "    input_ = Input(shape=(sequence_length,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized. Layer turns positive integers (indexes) into dense vectors of fixed size\n",
        "    rnn = RNN_class(rnn_units)(embedding) # can support different RNNs\n",
        "    output = Dense(num_classes, activation='softmax')(rnn)\n",
        "    return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "sequence_length = padded_X.shape[1]\n",
        "vocab_size = tokenizer.num_words\n",
        "num_classes = len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5y48zddjkf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh4icO32GGX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = build_rnn_model(LSTM, sequence_length, vocab_size, num_classes)\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikBgdQhpVwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 100\n",
        "stop_cb = EarlyStopping(monitor = 'val_acc', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbxk_lppo_tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_history = lstm_model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])\n",
        "# , callbacks=[stop_cb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXAKNMb5PbM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model test data\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "padded_test = pad_sequences(\n",
        "    test_sequences,\n",
        "    maxlen = sequence_length, \n",
        "    value=0\n",
        ")\n",
        "\n",
        "print(padded_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrcF1iyLzBkW",
        "colab_type": "text"
      },
      "source": [
        "Predict with LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3MGvq6CzAIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(lstm_model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIycPktw_m39",
        "colab_type": "text"
      },
      "source": [
        "## Bidirectional LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8vdw-jcfAKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(Input(shape=(sequence_length,)\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWl2MJjjGmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM\n",
        "\n",
        "# def build_bi_rnn_model(RNN_class, sequence_length, vocab_size, num_classes):\n",
        "#     input_ = Input(shape=(sequence_length,))\n",
        "#     embedding = Embedding(vocab_size, embedding_dim)(input_) # randomly initialized\n",
        "#     rnn = Bi(RNN_class(rnn_units))(embedding) # can support different RNNs\n",
        "#     output = Dense(num_classes, activation='softmax')(rnn)\n",
        "#     return Model(inputs=[input_], outputs=[output])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(rnn_units))) # bidirectional: num of neurons gets doubled\n",
        "model.add(Dense(num_classes, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7L2tJHok6_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_accuracy', patience=3, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "hist_bi_lstm = model.fit(padded_X, train_numbers, epochs = epochs, batch_size = batch_size, validation_data=(padded_dev, dev_numbers), callbacks=[stop_cb])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvFxjZZ2ZnQ",
        "colab_type": "text"
      },
      "source": [
        "Predict with bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQIaS_tR2Tiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(padded_test), axis=1) # np.argmax gives the index which has the highest value e.g. class\n",
        "predicted_labels = label_encoder.inverse_transform(list(predictions))\n",
        "print(\"Classification accuracy: \", round(accuracy_score(test_labels, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPskSfawiO5I",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.1: Deep contextual representations with Bert (multi-class)\n",
        "Train a Bert classifier to predict the register categories. Similar to Milestone 1, the setting is multi-class, and the evaluations should include results with different hyperparameters.\n",
        "\n",
        "Neural language models trained on large\n",
        "unannotated corpora can be used to create\n",
        "contextualized representations of\n",
        "meaning\n",
        "\n",
        "[How-to](https://github.com/HannaKi/Deep_Learning_in_LangTech_course/blob/master/bert_text_classification_extended_comments.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSfx6EwPjmR",
        "colab_type": "text"
      },
      "source": [
        "Keep only labels (and texts which correspond to those labels) which appear in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798Cth7XK4Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # #To select rows whose column value is in list \n",
        "# # years = [1952, 2007]\n",
        "# # gapminder.year.isin(years)\n",
        "\n",
        "# train_labels = train['label'].tolist()\n",
        "\n",
        "# # test_ = test.label.isin(train_labels)\n",
        "# # dev_ = dev.label.isin(train_labels)\n",
        "\n",
        "# test_ = test[test.label.isin(train_labels)]\n",
        "# dev_ = dev[dev.label.isin(train_labels)]\n",
        "\n",
        "# # test_ = test[test.label.isin(test_)]\n",
        "# # dev_ = dev[dev.label.isin(dev_)]\n",
        "\n",
        "# frames = [train, dev_, test_]\n",
        "# for d in frames:\n",
        "#   print(d.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSo2NQTzEB1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ist = train_['label'] == 'OA NA DS IG '\n",
        "# train_[ist]\n",
        "# #train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p95jUdzYHGLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ist = dev_['label'] == 'OA NA DS IG '\n",
        "# dev_[ist]\n",
        "# #train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqrKmnsLI1AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 2000\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Kwvy7nKMYt",
        "colab_type": "text"
      },
      "source": [
        "To avoid out of memory we have to limit the input size to MAX_EXAMPLES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82iZbp0N0FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncate_data(data, MAX_EXAMPLES):\n",
        "  if len(data) > MAX_EXAMPLES: # truncate data if needed to avoid OOM\n",
        "    print('Note: truncating examples from {} to {}'.format(len(data), MAX_EXAMPLES)) # should take stratified subsample?\n",
        "    data = data[:MAX_EXAMPLES]\n",
        "  return(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3LbSG4Vxgp_",
        "colab_type": "code",
        "outputId": "c8e956a2-abfb-42d7-85ab-a31131d36db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# We use dev_ and test_ data sets which only have labels that appear in train data\n",
        "# All the datas will be truncated if needed\n",
        "\n",
        "train_ = truncate_data(train, MAX_EXAMPLES)\n",
        "dev_ = truncate_data(dev_, MAX_EXAMPLES)\n",
        "test_ = truncate_data(test_, MAX_EXAMPLES)\n",
        "\n",
        "frames = [train_, dev_, test_]\n",
        "for d in frames:\n",
        "  print(d.shape)\n",
        "  #print(d.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: truncating examples from 5295 to 2000\n",
            "(2000, 3)\n",
            "(750, 3)\n",
            "(1500, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMN0rcLOK4rx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "defe499b-e1dc-461d-d7d6-62ff3133f0a8"
      },
      "source": [
        "# Miksi ei onnistu ensin ottaa päällekäiset labelit pois ja sitten leikata 1000:een?!?!?\n",
        "\n",
        "train_labels = train_['label']\n",
        "\n",
        "test_ = test_[test_['label'].isin(train_labels.tolist())]\n",
        "dev_ = dev_[dev_['label'].isin(train_labels.tolist())] \n",
        "\n",
        "frames = [train_, dev_, test_]\n",
        "for d in frames:\n",
        "  print(d.shape)\n",
        "  #print(d.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 3)\n",
            "(748, 3)\n",
            "(1493, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95rtEhpXJVdo",
        "colab_type": "text"
      },
      "source": [
        "Install Keras Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi49M60HJRiI",
        "colab_type": "code",
        "outputId": "9d4bd1ac-e3be-486e-df7e-955c3ca32395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "!pip3 install keras-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (0.81.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.3)\n",
            "Requirement already satisfied: keras-transformer>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (0.33.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.6.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.22.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.7.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.11.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.14.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert) (0.41.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fVv1axNJhVn",
        "colab_type": "text"
      },
      "source": [
        "Set an environment variable for keras-bert to use tensorflow.python.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Td8Gp40JeYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zEw1o-yKC3T",
        "colab_type": "text"
      },
      "source": [
        "## Download pretrained FinBERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRasDCPh1jUM",
        "colab_type": "text"
      },
      "source": [
        "Download pretrained TurkuNLP FinBERT from: https://github.com/TurkuNLP/FinBERT and prepare it for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J87QH8NiKFQm",
        "colab_type": "code",
        "outputId": "daf333f5-da32-47df-8e47-f510653fd6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!wget -nc http://dl.turkunlp.org/finbert/bert-base-finnish-cased-v1.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘bert-base-finnish-cased-v1.zip’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6sF2aEdYqhf",
        "colab_type": "code",
        "outputId": "bccb9ab0-5fc8-4711-dbec-c4fcb4c611eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Give -n argument so that existing files aren't overwritten \n",
        "!unzip -n bert-base-finnish-cased-v1.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bert-base-finnish-cased-v1.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fcLhH5_Z3LS",
        "colab_type": "text"
      },
      "source": [
        "Store paths to important files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yboNBfdZ2TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vocab_path = 'bert-base-finnish-cased-v1/vocab.txt'\n",
        "bert_config_path = 'bert-base-finnish-cased-v1/bert_config.json'\n",
        "bert_checkpoint_path = 'bert-base-finnish-cased-v1/bert_model.ckpt' # suffixes not required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ulKEdrFa5J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_is_cased = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cXqCpbSa-20",
        "colab_type": "text"
      },
      "source": [
        "## Load vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LyiI4clbCU1",
        "colab_type": "code",
        "outputId": "cb25e5ee-1a87-415b-c403-5ba209850cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "vocab = []\n",
        "with open(bert_vocab_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
        "\n",
        "\n",
        "# Print a list with every 500th vocabulary item (includes suffixes)\n",
        "print(vocab[0::500]) # 0 is for padding!\n",
        "print(vocab[103]) # tags are included"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '##hin', '##tila', 'tehtä', '##ce', 'avulla', '##uttua', 'Sil', '##iver', 'tarvits', 'kilometrin', 'sanotaan', '##ya', 'onnistuu', 'tapaus', 'rento', '##otin', '##kasvat', 'kauhe', 'puolin', 'ymmärrän', 'polttoain', 'arvot', 'ajattelu', '##impiin', 'huomasi', 'tietokoneen', 'tiedämme', 'johdossa', 'teemme', 'paikallisen', 'rekryt', 'vuosikymmeniä', 'kohdistuu', 'tyhmiä', 'baa', '##ipa', 'aero', '##ehtien', 'viimeisteli', '##llosta', 'luulevat', 'verenpain', 'tuottamaan', 'vahingot', 'opiskelijan', '##päivinä', '##uksellisesti', 'uskottava', '##elemaan', 'ilmennyt', 'määrätieto', 'leppo', 'yksityiset', 'kirjailijan', 'vastikään', 'samoista', '##misiin', '##pankkien', 'tuut', 'muutoinkin', 'säilyi', '##ivuoren', '##pauksia', 'kaupungilta', '##lalle', 'tervetulleeksi', 'lähteitä', 'isoin', 'Tuskinpa', 'hallinnut', '##kanslerin', '##jela', 'siniset', 'erikoiskokeella', 'todistavat', 'itsehallinto', 'Oran', 'vikaan', 'ISIS', 'hankinnan', '##eleitä', '##heitolla', '##upar', 'raam', 'edelläkävijä', 'tunnistettu', 'kohtaamis', 'punaisena', 'murtaa', '##ske', 'toimivuudesta', 'zo', '##ektro', '114', 'seinästä', 'Kuhmon', 'viesteissä', 'tukim', 'sellaisiksi', 'Rautio']\n",
            "[SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srU3eKQvbMbW",
        "colab_type": "text"
      },
      "source": [
        "## Load BERT configuration\n",
        "\n",
        "Just peek into the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHDZ0R-nbO6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "with open(bert_config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Print configuration contents\n",
        "#pprint(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5KmJ0OecMDp",
        "colab_type": "text"
      },
      "source": [
        "## Create BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYckpNLBcXMJ",
        "colab_type": "text"
      },
      "source": [
        "To create the tokenizer, we'll need a mapping from vocabulary items to their corresponding integer indices. We do this conventionally using enumerate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBV0vAhwcLqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create mapping from vocabulary items to their indices in the vocabulary\n",
        "token_dict = { v: i for i, v in enumerate(vocab) }\n",
        "\n",
        "# Print some random examples of the mapping\n",
        "#pprint(dict(random.choices(list(token_dict.items()), k=10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-T7zTcU08",
        "colab_type": "code",
        "outputId": "03b87f7f-7ac7-473b-f3d7-d5b6dd99b25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "# Test keras-Bert tokenizer\n",
        "from keras_bert import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(token_dict, cased=model_is_cased) \n",
        "\n",
        "# Let's test that out\n",
        "for s in ['Heippa BERT!', 'Tämä lause on suomeksi. tai ei', 'Kiinaa tai japania: 你', 'Yksi kirjain a on yksi']:\n",
        "    print('Original string:', s)\n",
        "    print('Tokenized:', tokenizer.tokenize(s))\n",
        "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
        "    print('Encoded:', indices)\n",
        "    print('Segments:', segments)\n",
        "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
        "    print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string: Heippa BERT!\n",
            "Tokenized: ['[CLS]', 'Hei', '##ppa', 'B', '##ER', '##T', '!', '[SEP]']\n",
            "Encoded: [102, 5050, 2096, 415, 9981, 50031, 380, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Hei ##ppa B ##ER ##T !\n",
            "\n",
            "Original string: Tämä lause on suomeksi. tai ei\n",
            "Tokenized: ['[CLS]', 'Tämä', 'lause', 'on', 'suomeksi', '.', 'tai', 'ei', '[SEP]']\n",
            "Encoded: [102, 1131, 17580, 145, 11695, 111, 337, 193, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Tämä lause on suomeksi . tai ei\n",
            "\n",
            "Original string: Kiinaa tai japania: 你\n",
            "Tokenized: ['[CLS]', 'Kiinaa', 'tai', 'japan', '##ia', ':', '你', '[SEP]']\n",
            "Encoded: [102, 39876, 337, 9780, 157, 236, 101, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Kiinaa tai japan ##ia : [UNK]\n",
            "\n",
            "Original string: Yksi kirjain a on yksi\n",
            "Tokenized: ['[CLS]', 'Yksi', 'kirjain', 'a', 'on', 'yksi', '[SEP]']\n",
            "Encoded: [102, 3420, 28106, 151, 145, 1034, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded: Yksi kirjain a on yksi\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFPCAVJd9gD",
        "colab_type": "text"
      },
      "source": [
        "## Prepare labels\n",
        "\n",
        "Y holds the labels the model will learn to predict. We will train with the labels the truncated training data contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ioyLbFPKqkS",
        "colab_type": "code",
        "outputId": "2c20ba65-dad4-4792-a702-6d9b06bad9f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# training data\n",
        "\n",
        "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
        "train_label = train_['label'].tolist() \n",
        "\n",
        "#label_encoder.fit(all_labels)\n",
        "label_encoder.fit(train_label)\n",
        "Y = label_encoder.transform(train_label) # encoded labels (not one hot!)\n",
        "\n",
        "# Take note of how many unique labels there are in the data\n",
        "num_labels = len(set(Y))\n",
        "\n",
        "# Print out some examples\n",
        "print(\"Training data\")\n",
        "print('Number of unique labels in training data:', num_labels)\n",
        "print(type(train_label), train_label[:10])\n",
        "print(type(Y), Y[:10])\n",
        "print()\n",
        "\n",
        "# development data\n",
        "print(\"Development data\")\n",
        "dev_label = dev_['label']#.tolist()\n",
        "y = label_encoder.transform(dev_label)\n",
        "print('Number of unique labels in training data:', len(set(y)))\n",
        "print(type(dev_label), dev_label[:10])\n",
        "print(type(y), y[:10])\n",
        "print()\n",
        "# test data\n",
        "print(\"Test data\")\n",
        "test_label = test_['label'].tolist()\n",
        "true_labels = label_encoder.transform(test_label)\n",
        "print('Number of unique labels in training data:', len(set(true_labels)))\n",
        "print(type(test_label), test_label[:10])\n",
        "print(type(true_labels), true_labels[:10])\n",
        "# print(len(test_label))\n",
        "# print(len(test_['text']))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data\n",
            "Number of unique labels in training data: 73\n",
            "<class 'list'> ['DS IG ', 'RS OP ', 'NE NA ', 'SR NA ', 'MT OS ', 'NA  ', 'DP IN ', 'DS IG ', 'DF ID ', 'DT IN ']\n",
            "<class 'numpy.ndarray'> [13 66 44 70 39 43  9 13  7 15]\n",
            "\n",
            "Development data\n",
            "Number of unique labels in training data: 50\n",
            "<class 'pandas.core.series.Series'> 0     OA NA \n",
            "1     DS IG \n",
            "2     DS IG \n",
            "3     DF ID \n",
            "4     OA NA \n",
            "6     DF ID \n",
            "7     MT OS \n",
            "8     CB NA \n",
            "9       HI  \n",
            "10      NA  \n",
            "Name: label, dtype: object\n",
            "<class 'numpy.ndarray'> [49 13 13  7 49  7 39  1 27 43]\n",
            "\n",
            "Test data\n",
            "Number of unique labels in training data: 50\n",
            "<class 'list'> ['HI  ', 'NA  ', 'DT IN ', 'DP IN ', 'DT IN ', 'AV OP ', 'CB NA ', 'IN  ', 'NE NA ', 'TB NA ']\n",
            "<class 'numpy.ndarray'> [27 43 15  9 15  0  1 33 44 72]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7z05ec1Hfr",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize input data\n",
        "Keep token indices and segment ids in separate lists and store as numpy arrays. X here is the final vectorized form of the input we'll be providing to the model for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYRRn2deAt7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model_inputs(text):\n",
        "  token_indices, segment_ids = [], []\n",
        "  for text in text:\n",
        "      # tokenizer.encode() returns a sequence of token indices\n",
        "      # and a sequence of segment IDs. BERT expects both as input,\n",
        "      # even if the segments IDs are just all zeros (like here).\n",
        "      tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
        "      token_indices.append(tid)\n",
        "      segment_ids.append(sid)\n",
        "  inp = [np.array(token_indices), np.array(segment_ids)] # Format input as list of two numpy arrays\n",
        "  return inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxBP2UFcBLFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = make_model_inputs(train_['text'])\n",
        "x = make_model_inputs(dev_['text'])\n",
        "test_inp = make_model_inputs(test_['text'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4WUr_OfBU1C",
        "colab_type": "code",
        "outputId": "53342062-3989-4931-f9db-757a6791adbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Print some examples\n",
        "print(X[0].shape)\n",
        "print('Token indices:')\n",
        "print(test_inp[0][:2])\n",
        "print('Decoded:')\n",
        "for i in test_inp[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(test_inp[1][:2])"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 300)\n",
            "Token indices:\n",
            "[[  102 41061  8448  6641 49802   111   609 36238  6575   142 30132  6684\n",
            "   4535 50051   166 30086 46629   171  8171  2181 35150   489  1620 15336\n",
            "  20129   364 10915  5034 12221  1236   111   609 15336 22463  8413  1904\n",
            "  22718   303  4767   422   119   206 24477   182 37947  6355 33845  1661\n",
            "    142   499   337  3381   422   943  8884  4535 50051   166 30896 32409\n",
            "  50009   111 10280   224  1611  2799 17213  6706  2264  8022   236  2065\n",
            "    111 18808  1642 35804 30114  9106   111 15671   145  7772  1091   236\n",
            "    998 46271   411   597  9258 10006 13111 12949  6360   166 42864 50009\n",
            "   5971  1827   193   257 14463 26420 34478 35403  2443   380  5971  1694\n",
            "    339  4960 17345   255 14281  2494  1642  4535 50051   166 30086   111\n",
            "  29174  1337  5253   145  3518 16883   170 20818   353 14798   111  5971\n",
            "   1232  8836   988 14589 34478   255  4896  2951  5962   111   609 34478\n",
            "    255  7656   193  5976 21841  8679   802  1038   145   652   138  3336\n",
            "   4884   776   303  4211   422 23868   176 23281   144  2177  4401 50069\n",
            "   2065 10254 50057  2960 42630   919 34478   145  3291   142 31383 15900\n",
            "    499 21399 34478 47485   303 34478   146   727   864   270 29940 21973\n",
            "   5187  2648   111  8605   358  8545  4100   126 28066  2055  3505 19048\n",
            "  20518 15869 34356   734 13368 22660 24249   170   111  3837  1322  5552\n",
            "  13368  8545  3524  4622 11619  1061   337 11841  1061 22277 15856   111\n",
            "  16572   936  4622 13953   145  5938   563  1002  1958   166   142 12171\n",
            "   2489 16620  8952   111 12905  4622 13953   145 29245  1337  5253   111\n",
            "  20601   126   145  5135  4313   142 39373   111 13613  1611  3271 13762\n",
            "  35735 27974   111  2938  6860   193   257 27308   111   103     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [  102   261 16634   236 15293 34209  3414   136  1742  6091   111   111\n",
            "    111 20675  1024 50016   119 25543  2342   380 29548 50011 15251 16666\n",
            "  12838   913   380  9181   119 19801 50011  6114   235  1354 31814  2167\n",
            "    427   559 38060   380 16026 50011   672  1385   590  8275   946 33124\n",
            "   2045 38636 17708   539   305  3323 25896  1442 22502 50006   142   672\n",
            "   2644   305 18308 50006   437   164 34276  9368  6054  1345  1527 26619\n",
            "   8247   105 38619   464   142 19278   333 22258   473 17671   119  5236\n",
            "    519 25134  1015 50006   142  8703 27427  8885 17856   142 27642   199\n",
            "    280   563  2010  5822 26129   519 15100   265 46444   725 29769   111\n",
            "    111   111  2297 15567   478  7657   106 30428   133 24915 50008   111\n",
            "    111   111  8159  9436 18317 18534   468   913 10981   368   142 34713\n",
            "  43256  5786   658  2073  8429   119   304   491  4658   250  7942  5323\n",
            "    119  1224   572  7065  2392   164   380 14973 50010   119  8784 16666\n",
            "  37516 20038  4361 37344   380  3586  6642   913  1671   672  1385   590\n",
            "   1676   774  3158 25353   403   119   606  2394  1107 15133  4655   845\n",
            "   1019  3430  3811   184  1251  1832  1429 19864   785   119  1224 18727\n",
            "  15601 12823   127  6670   142  9415 10101   572  7455 10997   113  6567\n",
            "   1736  1529   380  7407  7295  1346  1697  1236  5887 50007   341   567\n",
            "   3588 50012   639   111   111   111   142  4638 12318 50007  1706 34775\n",
            "  41574   207   427   119 29194  4829 29586  2073  8429   572  4019   142\n",
            "  19017   280   572 17893  1437 36443   142  2857  3870  2830  4329   127\n",
            "   3276   217   719   146   142 11451   537   464 15296  6369   425  4874\n",
            "  50009 37919  9451   142  5162 33909   119 16666   572 24701 34618 50007\n",
            "   4902  3496  1012  2313 50006   341 22823  3776  4354   119 12583   280\n",
            "  14429   111   111   111   142 19689   794   253 32252  2202  4638   103]]\n",
            "Decoded:\n",
            "['Tehkää', 'nolla', '##lei', '##maus', '.', 'Jos', 'rekisteröinti', 'onnistui', 'ja', 'käyttämä', '##nne', 'Q', '##R', '-', 'koodi', '##luki', '##ja', '##ohjelma', 'toimii', 'toivot', '##ulla', 'tavalla', 'saatte', 'kuitta', '##uksen', 'onnistun', '##eesta', 'leima', '##uksesta', '.', 'Jos', 'saatte', 'valituksen', 'puuttu', '##vasta', 'joukkueesta', 'niin', 'varmista', '##kaa', ',', 'että', 'teit', '##te', 'edellämain', '##itun', 'rekisteröin', '##nin', 'ja', '/', 'tai', 'kokeil', '##kaa', 'eri', 'ohjelmaa', 'Q', '##R', '-', 'koodin', 'tulkintaa', '##n', '.', 'Siir', '##ty', '##kää', 'lähtö', '##alueelle', 'viimeistään', 'klo', '09', ':', '50', '.', 'Alue', 'löytyy', 'hissi', '##linjan', 'edestä', '.', 'Lähtö', 'on', 'tasan', '10', ':', '00', 'Lähd', '##ön', 'jälkeen', 'julkaistaan', 'tapahtuman', 'sivulla', 'linkki', 'online', '-', 'seurantaa', '##n', 'Ras', '##teja', 'ei', 'ole', 'merkitty', 'maastoon', 'rasti', '##lip', '##uilla', '!', 'Ras', '##tit', 'ovat', 'pieniä', 'paperi', '##la', '##ppuja', 'joista', 'löytyy', 'Q', '##R', '-', 'koodi', '.', 'Joillakin', 'ras', '##teilla', 'on', 'puna', '##valko', '##ista', 'merkintä', '##na', '##uhaa', '.', 'Ras', '##tis', '##elit', '##teet', 'auttavat', 'rasti', '##la', '##pun', 'löytä', '##misessä', '.', 'Jos', 'rasti', '##la', '##ppua', 'ei', 'löydy', 'oikeasta', 'paikasta', 'eli', 'joku', 'on', 'lap', '##un', 'käynyt', 'poista', '##massa', 'niin', 'ilmoitta', '##kaa', 'tekstiviesti', '##llä', 'nro', '##on', '+', '35', '##8', '50', '58', '##3', '60', '##44', 'mikä', 'rasti', 'on', 'kyseessä', 'ja', 'ottakaa', 'valokuva', '/', 'valokuvia', 'rasti', '##alueesta', 'niin', 'rasti', '##lla', 'käy', '##misen', '##ne', 'tarkistetaan', 'maalissa', 'valoku', '##vista', '.', 'Myöh', '##äs', '##tyminen', 'sarjan', '##sa', 'takara', '##jasta', 'aiheuttaa', 'miinus', '##pisteen', 'jokaisesta', 'alkava', '##sta', 'myöhäs', '##tymis', '##minuut', '##ista', '.', 'Yli', '15', 'minuutin', 'myöhäs', '##tyminen', 'johtaa', 'joukkueen', 'hylkää', '##miseen', 'tai', 'siirtä', '##miseen', 'pitempään', 'sarjaan', '.', 'Kilpailun', 'aikana', 'joukkueen', 'jäsenten', 'on', 'oltava', 'koko', 'ajan', 'näkö', '-', 'ja', 'kuulo', '##etä', '##isyydellä', 'toisistaan', '.', 'Kaikkien', 'joukkueen', 'jäsenten', 'on', 'käytävä', 'ras', '##teilla', '.', 'Maalis', '##sa', 'on', 'tarjolla', 'vettä', 'ja', 'mehua', '.', 'Käyttä', '##kää', 'omia', 'juoma', '##pullo', '##janne', '.', 'Mu', '##keja', 'ei', 'ole', 'jaossa', '.']\n",
            "['1', 'kommenttia', ':', 'Syys', '##lomalle', '##läh', '##ti', '##jät', 'kirjoitti', '.', '.', '.', 'Ly', '##sti', '##ä', ',', 'lys', '##tiä', '!', 'Onpa', '##s', 'kivoja', 'lumi', '##kuvia', 'taas', '!', 'Juu', ',', 'enpä', '##s', 'taida', '##kaan', 'ottaa', 'filla', '##ria', 'mukaan', 'vaan', 'sukset', '!', 'Joko', '##s', 'ensi', '##lu', '##men', '##lad', '##ulle', 'pääsis', 'kohta', '##puolin', 'hiihtä', '##mään', '?', 'Siis', 'tulevana', 'viikon', '##vaihteen', '##a', 'ja', 'ensi', 'viikolla', '?', 'Kip', '##a', '##isin', 'jo', 'ulla', '##kolla', 'kerää', '##mässä', 'por', '##ukalle', 'talvi', '##ta', '##mine', '##ita', 'ja', 'laukku', '##jen', 'pakkaa', '##minen', 'aloitettu', ',', 'huomenna', 'vielä', 'vanhempain', '##ilta', '##a', 'ja', 'öljyn', '##vaihtoa', 'meno', '##peliin', 'ja', 'tänäänkin', 'se', 'oli', 'koko', 'päivän', 'korjaa', '##molla', 'vielä', 'jous', '##ien', 'laaker', '##eiden', 'vaihdossa', '.', '.', '.', 'piti', 'säätää', 'vain', 'pyöri', '##en', 'aura', '##us', '##kulma', '##t', '.', '.', '.', 'Tie', '##hallinnon', 'keli', '##kamer', '##oita', 'taas', 'sela', '##sin', 'ja', 'Salla', '##tuntu', '##rilla', 'hyvin', 'pyr', '##yttänyt', ',', 'mutta', 'nyt', 'vien', '##ee', 'Pyhä', 'voiton', ',', 'siellä', 'ihan', 'kin', '##oksia', 'jo', '!', 'Je', '##e', ',', 'päästään', 'lumi', '##ukkojen', 'tekoon', 'omalla', 'tontilla', '!', 'Aika', 'kivaa', 'taas', 'päästä', 'ensi', '##lu', '##men', 'kokem', '##iseen', 'syys', '##lomalla', '##mme', ',', 'vaikka', 'ollaan', 'me', 'Saari', '##sel', '##ällä', 'siv', '##ak', '##oitu', '##kin', 'tällä', 'sama', '##isella', 'lomalla', 'ennen', ',', 'siellä', 'laitettiin', 'pikaisesti', 'lad', '##ut', 'kuntoon', 'ja', 'porukkaa', 'riitti', 'ihan', 'kilpa', '##hiih', '##tä', '##jiin', 'asti', 'heti', '!', 'Tieto', 'lat', '##ujen', 'ava', '##uksesta', 'kiir', '##i', 'kuin', 'kul', '##ova', '##l', '##kea', '.', '.', '.', 'ja', 'saatiin', 'sillo', '##i', 'tosi', 'upeat', 'jouluku', '##vat', 'mukaan', ',', 'Kaunis', '##pään', 'huipulla', 'pyr', '##yttänyt', 'ihan', 'kunnolla', 'ja', 'mökki', 'oli', 'ihan', 'lumen', '##pe', '##itossa', 'ja', 'lapset', 'ton', '##ttu', '##hat', '##ut', 'päässä', 'ku', '##isti', '##lla', 'ja', 'upea', 'sin', '##ita', '##ivas', 'taustalla', 'kir', '##peä', '##n', 'pakkasen', 'kera', 'ja', 'aurinko', 'paistoi', ',', 'lumi', 'ihan', 'kim', '##als', '##i', 'vit', '##ival', '##koi', '##sen', '##a', 'kuin', 'timan', '##tteja', 'täynnä', ',', 'kaunista', 'oli', 'silloinkin', '.', '.', '.', 'ja', 'Kumm', '##itus', '##kä', '##mpä', '##ltä', 'saatiin']\n",
            "Segment ids:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O7-xnrg3nyT",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained BERT model form checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QBf-vQiWdcY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load pretrained BERT model\n",
        "\n",
        "We'll use the keras-bert function load_trained_model_from_checkpoint to load the model from the checkpoint we downloaded earlier.\n",
        "\n",
        "Explanation for a few parameters from keras-bert documentation:\n",
        "\n",
        "- training: If training, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
        "- trainable: Whether the model is trainable. The default value is the same with training.\n",
        "\n",
        "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use training=False but trainable=True.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReplOvS3nAQ",
        "colab_type": "code",
        "outputId": "7a7864f5-6adc-460f-d8bc-9b6086e34a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False, # ignore MLM [masked language modeling] and NSP [next sentence prediction] parts of the model\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH # define the size of input layer\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fca5db14588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7fca5db14588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fca5dfb5f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7fca5dfb5f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5764eb70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5764eb70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5764ec50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5764ec50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca5891f9e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca5891f9e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca57720860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca57720860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5890a630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5890a630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca576af240>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca576af240>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca57604668>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca57604668>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca5758fda0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca5758fda0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca57584128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca57584128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca575709b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca575709b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5751f710>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5751f710>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5752fb70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5752fb70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca57444cc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca57444cc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca57430080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca57430080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5740ea90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5740ea90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca574adeb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca574adeb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca574a6828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca574a6828>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca57367cc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca57367cc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca573540f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca573540f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5739bbe0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5739bbe0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca572d1400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca572d1400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca57351898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca57351898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca572229e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca572229e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5728e550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5728e550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca571799e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca571799e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5717c5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5717c5c0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5717c400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5717c400>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca570ccc18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca570ccc18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5713d9e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca5713d9e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca57103630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca57103630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca570b1588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca570b1588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5712fbe0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca5712fbe0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56f7ecc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56f7ecc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca570169e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca570169e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5702f898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca5702f898>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56fc2c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56fc2c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56fc26d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56fc26d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56ea8a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56ea8a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56f13048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56f13048>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56eddb38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56eddb38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56e037f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56e037f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56e905f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56e905f8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56d5ab00>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56d5ab00>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56dcd198>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56dcd198>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56d88cc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56d88cc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56d3d358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56d3d358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56d3d438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56d3d438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56c07eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56c07eb8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56cab080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56cab080>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56c3a978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56c3a978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56c5f748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56c5f748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56c5f358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56c5f358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56ab6cf8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca56ab6cf8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56b9b630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56b9b630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56b68f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56b68f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56a919b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56a919b0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56a61748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7fca56a61748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca569e6e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7fca569e6e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56a5a588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56a5a588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56a17d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7fca56a17d68>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56953b38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7fca56953b38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3jk07Kw4Lrm",
        "colab_type": "code",
        "outputId": "681b4579-54b5-4bd8-804f-8f81ea516305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# token and segment inputs, just as we made\n",
        "pretrained_model.inputs"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Input-Token_2:0' shape=(?, 300) dtype=float32>,\n",
              " <tf.Tensor 'Input-Segment_2:0' shape=(?, 300) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JPTQgDb4aUz",
        "colab_type": "code",
        "outputId": "3d65f835-9e42-4385-d968-427f4e406207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "pretrained_model.outputs"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Encoder-12-FeedForward-Norm_2/add_1:0' shape=(?, 300, 768) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwqTgaZ55vD0",
        "colab_type": "text"
      },
      "source": [
        "Size of the output layer does not match our label count. This must be fixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQNBB7Gk5etU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained_model.summary()\n",
        "# same transformer layer repeated 12 times: self\n",
        "# attention, feedforward, drop out, normalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss678QTb63-0",
        "colab_type": "text"
      },
      "source": [
        "## Build classification model by wrapping the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNV-IQ6dX4De",
        "colab_type": "text"
      },
      "source": [
        "We will \"catch\" the model output and plug our own output layer on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8G2WqT763Cd",
        "colab_type": "code",
        "outputId": "3e70ce02-4324-4e3d-de6d-082488de5699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLpSFbIA0jQv",
        "colab_type": "text"
      },
      "source": [
        "Wrapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8-fYS4y7ICV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "out = Dense(num_labels, activation='softmax')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
        "model = Model(\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuDLml6v83JD",
        "colab_type": "text"
      },
      "source": [
        "Now the size of the output layer matches the number of our data labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2eJUnT78uQX",
        "colab_type": "code",
        "outputId": "86b486dc-ccc0-4975-b5ee-5eecc4d29efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.output"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense_16/Softmax:0' shape=(?, 73) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BekdEFHw7jqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYwXxwPe9J1j",
        "colab_type": "text"
      },
      "source": [
        "## Create optimizer\n",
        "\n",
        "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We won't go into detail on these optimizer settings, but will instead largely copy parameters used in the original BERT work.\n",
        "\n",
        "(If you're interested in tuning the training process, trying different values of LEARNING_RATE is a good place to start!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs_9hj7ey_ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 40                             #16\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.001                    #0.01\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 16                          #8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoqDpD3C9LJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train_['text']),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2MKRaKT9iHk",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkYA99RO9grF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy', # encoded labels!\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR2OgW1B9rsY",
        "colab_type": "code",
        "outputId": "cfca70af-6084-4390-9b28-10a2865a1d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_sparse_categorical_accuracy', patience=8, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "mc_cb = ModelCheckpoint(filepath='models/BERT_multiclass.h5', monitor='val_sparse_categorical_accuracy', verbose=0,save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x, y,), # onko typo\n",
        "    callbacks=[stop_cb, mc_cb]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 2000 samples, validate on 748 samples\n",
            "Epoch 1/40\n",
            "  16/2000 [..............................] - ETA: 2:33:24 - loss: 5.3143 - sparse_categorical_accuracy: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn_hm0UDNsgK",
        "colab_type": "code",
        "outputId": "87ba03d7-37b8-4b7b-d65d-16c642cc8645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrP6xhZJmpV",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noTnBHs4cJcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load best model\n",
        "model.load_weights('models/BERT_multiclass.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvK_t2LXNWam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(test_inp), axis=1) # np.argmax gives the index which has the highest value e.g. class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWn3JBFpQSak",
        "colab_type": "code",
        "outputId": "a5dc3cae-d71d-43f8-e14b-7ea61b8633d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "predicted_labels"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ', 'MT OS ',\n",
              "       'MT OS ', 'MT OS '], dtype='<U12')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uamUmR_ZQMFq",
        "colab_type": "code",
        "outputId": "5d71fc3f-7dfe-43ee-a058-5e22103c9fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(\"Classification accuracy: \", round(accuracy_score(test_label, predicted_labels)*100,1), \"percent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy:  12.1 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ2lrJaZQ42Z",
        "colab_type": "text"
      },
      "source": [
        "Accuracy as bad as naive baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSi-7hwqqTs",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2.2: Error analysis\n",
        "Compare the errors made by the classifiers you have trained from milestones 1 and 2.1. Are there any patterns? How do the errors one model makes differ from those made by another.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5QbHGcldxep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy\n",
        "# confusion matrix\n",
        "# classification_report\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HcrqoxqxiI",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 3.1: Bert (multi-LABEL)\n",
        "Train two multi-label classifiers, one using non-deep contextual representations, the other using Bert. In this setting, each label is assigned independently. Do hyperparameter optimization on these classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrVf5bheeRNH",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation for milestone 3\n",
        "\n",
        "Since we do multi label classification we are dealing with highlevel and sublevel registers and have significantly less labels (for example 'NA OP' becomes 'NA' and 'OP'. This means we have to prepare the data differently from the previous tasks\n",
        "\n",
        "We will separate all the registers and one hot encode them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwa_GN5xdzy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data manipulation: high level registers\n",
        "\n",
        "hiregs = ['NA', 'OP', 'IN', 'ID', 'HI', 'IP', 'IG', 'LY', 'SP', 'OS']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXVJU0CQPIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean up the label(register) column:\n",
        "\n",
        "def from_registers(col):\n",
        "  lst = col.tolist() # column to list\n",
        "  registers = [[]] # list of lists, as long as the original column\n",
        "  idx = 0\n",
        "  for element in lst:    #NA OP\n",
        "      parts = element.split(' ') #NA #OP\n",
        "      for i in parts: \n",
        "        if i == '': # omit empty strings\n",
        "          continue\n",
        "        registers[idx].append(i) # append to list in list\n",
        "      if len(lst)-1 > idx :\n",
        "        idx = idx+1\n",
        "        registers.insert(idx, []) # add new empty list to list\n",
        "  return registers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5laiU_0WPw",
        "colab_type": "code",
        "outputId": "eb4bd921-9893-4ba1-e1d8-1ea06580c8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# use combined data formed at the beinning of the notebook\n",
        "\n",
        "print(df.shape)\n",
        "all_regs=from_registers(df['label']) # use custom made function to list the registers\n",
        "df['regs'] = all_regs\n",
        "df.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7564, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "      <th>regs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "      <td>[DS, IG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[RS, OP]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "      <td>[NE, NA]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "      <td>[SR, NA]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "      <td>[MT, OS]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data      regs\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train  [DS, IG]\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train  [RS, OP]\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train  [NE, NA]\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train  [SR, NA]\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train  [MT, OS]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwsgYM9Q0mVx",
        "colab_type": "text"
      },
      "source": [
        "Next we will turn those registers to one hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbMk-Mz9Uj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "dummies = pd.DataFrame(mlb.fit_transform(df['regs']), columns=mlb.classes_, index=df.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEmtJxJ-WQZ2",
        "colab_type": "code",
        "outputId": "92c5680e-dc8c-414d-c604-89a7a3922076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "# https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
        "\n",
        "df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('regs')),\n",
        "                          columns=mlb.classes_))\n",
        "df.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>data</th>\n",
              "      <th>AV</th>\n",
              "      <th>CB</th>\n",
              "      <th>CM</th>\n",
              "      <th>DF</th>\n",
              "      <th>DP</th>\n",
              "      <th>DS</th>\n",
              "      <th>DT</th>\n",
              "      <th>EB</th>\n",
              "      <th>EN</th>\n",
              "      <th>FA</th>\n",
              "      <th>FC</th>\n",
              "      <th>FS</th>\n",
              "      <th>HA</th>\n",
              "      <th>HI</th>\n",
              "      <th>IB</th>\n",
              "      <th>ID</th>\n",
              "      <th>IG</th>\n",
              "      <th>IN</th>\n",
              "      <th>IP</th>\n",
              "      <th>IT</th>\n",
              "      <th>JD</th>\n",
              "      <th>LT</th>\n",
              "      <th>LY</th>\n",
              "      <th>MT</th>\n",
              "      <th>NA</th>\n",
              "      <th>NE</th>\n",
              "      <th>OA</th>\n",
              "      <th>OB</th>\n",
              "      <th>OP</th>\n",
              "      <th>OS</th>\n",
              "      <th>PB</th>\n",
              "      <th>PO</th>\n",
              "      <th>QA</th>\n",
              "      <th>RA</th>\n",
              "      <th>RE</th>\n",
              "      <th>RP</th>\n",
              "      <th>RS</th>\n",
              "      <th>RV</th>\n",
              "      <th>SL</th>\n",
              "      <th>SP</th>\n",
              "      <th>SR</th>\n",
              "      <th>TB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DS IG</td>\n",
              "      <td>Logistiikka Jenni Lindholm Laskutus Ritva Lie...</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RS OP</td>\n",
              "      <td>Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NE NA</td>\n",
              "      <td>Koulutuspaikka jokaiselle peruskoulun päättän...</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SR NA</td>\n",
              "      <td>1 Cardiff C–Everton Tasainen kohde . Cardiff ...</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MT OS</td>\n",
              "      <td>Northrop Grumman Q4 2009 tulokset Northrop Gr...</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                               text   data  ...  SP  SR  TB\n",
              "0  DS IG    Logistiikka Jenni Lindholm Laskutus Ritva Lie...  train  ...   0   0   0\n",
              "1  RS OP    Tässä [ [ Ortodoksinen seminaari ( Joensuu ) ...  train  ...   0   0   0\n",
              "2  NE NA    Koulutuspaikka jokaiselle peruskoulun päättän...  train  ...   0   0   0\n",
              "3  SR NA    1 Cardiff C–Everton Tasainen kohde . Cardiff ...  train  ...   0   1   0\n",
              "4  MT OS    Northrop Grumman Q4 2009 tulokset Northrop Gr...  train  ...   0   0   0\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJz7I5ZbmlOo",
        "colab_type": "code",
        "outputId": "512e649a-89c4-4d17-92b8-7f05ac39958f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "subregs = np.setdiff1d(mlb.classes_, hiregs)\n",
        "subregs"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AV', 'CB', 'CM', 'DF', 'DP', 'DS', 'DT', 'EB', 'EN', 'FA', 'FC',\n",
              "       'FS', 'HA', 'IB', 'IT', 'JD', 'LT', 'MT', 'NE', 'OA', 'OB', 'PB',\n",
              "       'PO', 'QA', 'RA', 'RE', 'RP', 'RS', 'RV', 'SL', 'SR', 'TB'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FCQ2W_8f5vV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "9645e581-2919-43bc-b240-d6fe21d03fc4"
      },
      "source": [
        "# separate X (features) and Y (labels) for training data and x and y for development data\n",
        "\n",
        "# train\n",
        "X = train['text'] # feat\n",
        "Y = train.drop(['label','text', 'data'], axis=1) # label\n",
        "\n",
        "# dev\n",
        "x = dev['text']\n",
        "y = dev.drop(['label','text', 'data'], axis=1)\n",
        "\n",
        "# test\n",
        "test_feats = test['text']\n",
        "true_labels= test.drop(['label','text', 'data'], axis=1)\n",
        "Y"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AV</th>\n",
              "      <th>CB</th>\n",
              "      <th>CM</th>\n",
              "      <th>DF</th>\n",
              "      <th>DP</th>\n",
              "      <th>DS</th>\n",
              "      <th>DT</th>\n",
              "      <th>EB</th>\n",
              "      <th>EN</th>\n",
              "      <th>FA</th>\n",
              "      <th>FC</th>\n",
              "      <th>FS</th>\n",
              "      <th>HA</th>\n",
              "      <th>HI</th>\n",
              "      <th>IB</th>\n",
              "      <th>ID</th>\n",
              "      <th>IG</th>\n",
              "      <th>IN</th>\n",
              "      <th>IP</th>\n",
              "      <th>IT</th>\n",
              "      <th>JD</th>\n",
              "      <th>LT</th>\n",
              "      <th>LY</th>\n",
              "      <th>MT</th>\n",
              "      <th>NA</th>\n",
              "      <th>NE</th>\n",
              "      <th>OA</th>\n",
              "      <th>OB</th>\n",
              "      <th>OP</th>\n",
              "      <th>OS</th>\n",
              "      <th>PB</th>\n",
              "      <th>PO</th>\n",
              "      <th>QA</th>\n",
              "      <th>RA</th>\n",
              "      <th>RE</th>\n",
              "      <th>RP</th>\n",
              "      <th>RS</th>\n",
              "      <th>RV</th>\n",
              "      <th>SL</th>\n",
              "      <th>SP</th>\n",
              "      <th>SR</th>\n",
              "      <th>TB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5290</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5291</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5292</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5293</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5294</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5295 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AV  CB  CM  DF  DP  DS  DT  EB  EN  ...  RA  RE  RP  RS  RV  SL  SP  SR  TB\n",
              "0      0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "1      0   0   0   0   0   0   0   0   0  ...   0   0   0   1   0   0   0   0   0\n",
              "2      0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "3      0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   1   0\n",
              "4      0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
              "5290   0   0   0   0   0   0   1   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "5291   0   1   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "5292   0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "5293   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "5294   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0   0\n",
              "\n",
              "[5295 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inhn9BbbcJTl",
        "colab_type": "code",
        "outputId": "4cb62029-a15f-4be1-9f00-97061a600cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# separate train, test and dev datas from one hot encoded combined dataset\n",
        "\n",
        "train = df[df['data'] == 'train'] \n",
        "test = df[df['data'] == 'test'] \n",
        "dev = df[df['data'] == 'dev'] \n",
        "\n",
        "# for grid search (maybe not needed/used)\n",
        "joint = df[df['data'] != 'test'] \n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(dev.shape)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5295, 45)\n",
            "(1513, 45)\n",
            "(756, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtm800nFKgNE",
        "colab_type": "text"
      },
      "source": [
        "## Linear support vector machine (SVM)\n",
        "\n",
        "SVM which is widely regarded as one of the best text classification algorithms (although it’s also a bit slower than naïve Bayes). https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "\n",
        "\n",
        "Scikit documentation https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier\n",
        "\n",
        "tutorial: https://www.datatechnotes.com/2020/03/multi-output-classification-with-multioutputclassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dICpVrYx-jHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list = np.empty(len(Y))\n",
        "# list = []\n",
        "# type(list)\n",
        "# list.insert(0,(y.iloc[3].values))\n",
        "# list.insert(0,(y.iloc[4].values))\n",
        "# list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGKnrgw70nnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # for grid search:\n",
        "# XG = joint['text'] # features \n",
        "# YG = joint.drop(['label','text', 'data'], axis=1) # labels\n",
        "# print(YG.head())\n",
        "# print(YG.values[0])\n",
        "# YG.values.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAIrQZ5JlyXx",
        "colab_type": "code",
        "outputId": "4121b2cf-3f65-4fd8-b613-f0320bb4a1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# form feature matrixes\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features = 20000, ngram_range = (1,1)) \n",
        "\n",
        "train_feature_matrix = vectorizer.fit_transform(X)\n",
        "dev_feature_matrix = vectorizer.transform(x)\n",
        "test_feature_matrix = vectorizer.transform(test_feats)\n",
        "\n",
        "# for grid search:\n",
        "# XG = vectorizer.fit_transform(XG)\n",
        "\n",
        "print(\"shape of the training data: \", train_feature_matrix.shape)\n",
        "print(\"shape of the development data: \", dev_feature_matrix.shape)\n",
        "# print(\"shape of the  data: \", XG.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the training data:  (5295, 20000)\n",
            "shape of the development data:  (756, 20000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UawS5wlqCTzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a9e158f-9905-4263-9db8-c1c052b2f6b0"
      },
      "source": [
        "# guide for multiouput moldel for one hot encoded data: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "cat = Y.columns.values.tolist() # labels\n",
        "\n",
        "SVM_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(SGDClassifier(loss='hinge', random_state=42, alpha=0.0001, max_iter=40, early_stopping=True))),\n",
        "            ])\n",
        "for category in cat:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    SVM_pipeline.fit(train_feature_matrix, Y[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = SVM_pipeline.predict(test_feature_matrix)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(true_labels[category], prediction)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... Processing AV\n",
            "Test accuracy is 0.9960343688037012\n",
            "... Processing CB\n",
            "Test accuracy is 0.969596827495043\n",
            "... Processing CM\n",
            "Test accuracy is 0.9940515532055518\n",
            "... Processing DF\n",
            "Test accuracy is 0.9557171183079973\n",
            "... Processing DP\n",
            "Test accuracy is 0.9881031064111038\n",
            "... Processing DS\n",
            "Test accuracy is 0.923331130204891\n",
            "... Processing DT\n",
            "Test accuracy is 0.9458030403172505\n",
            "... Processing EB\n",
            "Test accuracy is 0.992729676140119\n",
            "... Processing EN\n",
            "Test accuracy is 0.9795109054857898\n",
            "... Processing FA\n",
            "Test accuracy is 0.9973562458691342\n",
            "... Processing FC\n",
            "Test accuracy is 0.992729676140119\n",
            "... Processing FS\n",
            "Test accuracy is 0.998678122934567\n",
            "... Processing HA\n",
            "Test accuracy is 0.9940515532055518\n",
            "... Processing HI\n",
            "Test accuracy is 0.9616655651024455\n",
            "... Processing IB\n",
            "Test accuracy is 0.9900859220092532\n",
            "... Processing ID\n",
            "Test accuracy is 0.9405155320555189\n",
            "... Processing IG\n",
            "Test accuracy is 0.9120951751487112\n",
            "... Processing IN\n",
            "Test accuracy is 0.8631857237276933\n",
            "... Processing IP\n",
            "Test accuracy is 0.9920687376074026\n",
            "... Processing IT\n",
            "Test accuracy is 0.9953734302709848\n",
            "... Processing JD\n",
            "Test accuracy is 0.9973562458691342\n",
            "... Processing LT\n",
            "Test accuracy is 0.9940515532055518\n",
            "... Processing LY\n",
            "Test accuracy is 0.9973562458691342\n",
            "... Processing MT\n",
            "Test accuracy is 0.9894249834765367\n",
            "... Processing NA\n",
            "Test accuracy is 0.7918043621943159\n",
            "... Processing NE\n",
            "Test accuracy is 0.8962326503635162\n",
            "... Processing OA\n",
            "Test accuracy is 0.9590218109715797\n",
            "... Processing OB\n",
            "Test accuracy is 0.9709187045604759\n",
            "... Processing OP\n",
            "Test accuracy is 0.9173826834104428\n",
            "... Processing OS\n",
            "Test accuracy is 0.9894249834765367\n",
            "... Processing PB\n",
            "Test accuracy is 0.9279576999339062\n",
            "... Processing PO\n",
            "Test accuracy is 0.9980171844018506\n",
            "... Processing QA\n",
            "Test accuracy is 0.9933906146728354\n",
            "... Processing RA\n",
            "Test accuracy is 0.9940515532055518\n",
            "... Processing RE\n",
            "Test accuracy is 0.9973562458691342\n",
            "... Processing RP\n",
            "Test accuracy is 0.9920687376074026\n",
            "... Processing RS\n",
            "Test accuracy is 0.9788499669530734\n",
            "... Processing RV\n",
            "Test accuracy is 0.9742233972240582\n",
            "... Processing SL\n",
            "Test accuracy is 0.9993390614672836\n",
            "... Processing SP\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py:75: UserWarning: Label not 0 is present in all training examples.\n",
            "  str(classes[c]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy is 0.9940515532055518\n",
            "... Processing SR\n",
            "Test accuracy is 0.984137475214805\n",
            "... Processing TB\n",
            "Test accuracy is 0.9947124917382684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r4xxibWuZm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inner_cv = KFold(n_splits=5, shuffle=True, random_state=4) \n",
        "# outer_cv = KFold(n_splits=5, shuffle=True, random_state=4)\n",
        "\n",
        "# # Inner CV: optimize number of neighbours aka hyperparameter\n",
        "# # Set up possible values of parameters to optimize over\n",
        "# alpha = [0.0001, 0.001, 0.01, 0.1]\n",
        "# params = dict(alpha = alpha)\n",
        "# gscv = GridSearchCV(estimator = model, param_grid=params, return_train_score=True, cv=inner_cv)\n",
        "\n",
        "# gscv.fit(XG, YG)\n",
        "\n",
        "# # for C in (0.001,0.01,0.1,1,10,100):\n",
        "# #     classifier =  sklearn.svm.LinearSVC(C=C)\n",
        "# #     classifier.fit(feature_matrix_train, labels_train)\n",
        "# #     print(\"C=\",C,\"     \",classifier.score(feature_matrix_dev, labels_dev))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F_UWKlUf9p6",
        "colab_type": "text"
      },
      "source": [
        "## Bert (multi-LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxV7Lhkwn8Tx",
        "colab_type": "text"
      },
      "source": [
        "Tutorial: https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\n",
        "\n",
        "We use the one hot encoded data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsTCKCcthHT5",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9OdoQjFF4ZU",
        "colab_type": "text"
      },
      "source": [
        "#### Feature tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDISZvoOf8wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX_EXAMPLES and INPUT_LENGTH limited to avoid going out of memory\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 1000\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWZ2Ni5dVq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "4b2d6638-d791-459d-c928-49dcbb668df1"
      },
      "source": [
        "# To avoid OOM, truncate data\n",
        "\n",
        "train = truncate_data(train, MAX_EXAMPLES)\n",
        "dev = truncate_data(dev, MAX_EXAMPLES)\n",
        "test = truncate_data(test, MAX_EXAMPLES)\n",
        "\n",
        "frames = [train, dev, test]\n",
        "for d in frames:\n",
        "  print(d.shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: truncating examples from 5295 to 1000\n",
            "Note: truncating examples from 1513 to 1000\n",
            "(1000, 45)\n",
            "(756, 45)\n",
            "(1000, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIR1xUNihUZw",
        "colab_type": "text"
      },
      "source": [
        "We need to convert our data into a format that BERT understands. Some utility functions are provided to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wWJ3csi8k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remember, INPUT_LENGTH is a parameter for make_model_inputs -function. \n",
        "# We also use BERT tokenizer here.\n",
        "\n",
        "X = make_model_inputs(train['text'])\n",
        "x = make_model_inputs(dev['text'])\n",
        "test_inp = make_model_inputs(test['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg0A3B4FcLbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "7d6bc95f-b809-453b-a18a-411976f94ff2"
      },
      "source": [
        "test_inp"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[  102, 41061,  8448, ...,   145, 29245,   103],\n",
              "        [  102,   261, 16634, ...,  2857,  3870,   103],\n",
              "        [  102, 14011, 17660, ..., 24441, 15398,   103],\n",
              "        ...,\n",
              "        [  102, 14540, 13897, ...,   111,   142,   103],\n",
              "        [  102,  3586,  1620, ...,   134,   119,   103],\n",
              "        [  102,   218, 50051, ...,  1138, 16523,   103]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9SmUCcoPt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74c2f783-16c2-4c2b-cea1-3ba87fdbb67a"
      },
      "source": [
        "# Print some examples (looks the same as in milestone 2)\n",
        "print('Token indices:')\n",
        "print(X[0][:2])\n",
        "print('Decoded:')\n",
        "for i in X[0][:2]:\n",
        "    print(tokenizer.decode(list(i)))\n",
        "print('Segment ids:')\n",
        "print(X[1][:2])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices:\n",
            "[[  102 20410   719   328 15022 38416 45766  4192 25413 14442 10448  7814\n",
            "   4009 24085 20599 11165  1330 50013 27722 11025 21917 13129  3452  4834\n",
            "   4942  1099   255  4942  1099   255  2599   145 46150   203  1640 48326\n",
            "    142  7026 32871  9890   111 36800 13873   403  6534   119  9183   142\n",
            "   9659   912   922 19078 12655 50009   119 22670  6729   119 12082 27949\n",
            "    612 12191   111 44695  1388 44626  8905  1555  7411   142 15996 14280\n",
            "   3817  2304 12213  9598  5463 29231   142 29403  4004   111 22423  1138\n",
            "   4942  1099   255  2599   145 35083 10080   203  8905  3369  5514 13758\n",
            "   2834 11166   119   374  1354 10772   109  3679  3332  2214   111 13742\n",
            "    697  4942  1099  6916 45571  1248  4942  1099   255  2599   145  6821\n",
            "    142 46150   203 21984 10900   407 23909   111  2604   145  5016  8905\n",
            "   5828 20150   239 24975  1922  2955   142 35576  1305   106 43029   111\n",
            "  36800 24975   106   142 32334  1252 10366  2767  1445 12310  3305 13914\n",
            "  21984 10900   170   111 36192   269   704   166 35917  4942  1099   866\n",
            "  46594   120 38731   166 41465  5087  1444  6408 15878 14482   356 49878\n",
            "  33640   142   166 42082   677  9364 26547 44137 18887  1929 50062 21062\n",
            "    427   111 47330  1388   133   240  4442   462   193   257 38731   166\n",
            "  20818 33046  1883 21403  8700   111 47330  1388  4819  5741   481  4942\n",
            "   1099   255  2599   145 15149 30909 15674 20753  4876   407 28988   144\n",
            "    119   975  2500 29403   473   145  5412   142  4354  5886   111 45081\n",
            "    106   119  8355  1499   142  6697   333  7483  5962   103]\n",
            " [  102  2208  3043  3043 49155   187 21069   348 15497   308   101 43609\n",
            "  27819  3013  3013 11185 18895  3902   145 16616 19099  7751  1121 24888\n",
            "    119 13405 29882 22322   111 28277   145  2596   943  2106   119   683\n",
            "  27114 23350   236  2208  3043  3043 49155   187 21069   348 15497   308\n",
            "    101 43609 27819  3013  3013 11185 18895  3902   145 16616 19099  7751\n",
            "   1121 24888   119 13405 29882 22322   111 28277   145  2596   943  2106\n",
            "    119   683 27114 23350   236   166  3043  3043   555  3835   101   555\n",
            "    859  4711  3013  3013  1462 46032 30795 36222  9094   142  1462 18252\n",
            "   3904 43269   149 21260   483   111 31943 23015   913 15258  7986 10331\n",
            "    337 12356 42657  4337  5966 40321   119  4215  6166  5503  6378   111\n",
            "  17842  1462  4254 21390   106 30617 10233 21260 20502  1385   146   111\n",
            "   2177  3043  3043   555  3835   101   555   859  4711  3013  3013  1462\n",
            "  46032 30795 36222  9094   142  1462 18252  3904 43269   149 21260   483\n",
            "    111  3043  3043  7210 16554 21260   123   101 31943 23015  3013  3013\n",
            "    913 15258  7986 10331   337 12356 42657  4337  5966 40321   119  4215\n",
            "   6166  5503  6378   111 17842  1462  4254 21390   106 30617 10233 21260\n",
            "  20502  1385   146   111   166 36183 50050   145 11627   111   737 12665\n",
            "    142   145   534  3225  1229 19242  4299 16280   353   111   950  2464\n",
            "  34523 50017  4101   984 45002   269   119 10314   145  1660 34523 50017\n",
            "    142  1660 42429 50011   119   374  5107 29641   119  3043  3043  7528\n",
            "    348  2305   308   101  7528   492  3013  3013   142   103]]\n",
            "Decoded:\n",
            "['Log', '##isti', '##ikka', 'Jenni', 'Lindholm', 'Lasku', '##tus', 'Ritva', 'Lie', '##vonen', 'Ota', 'yhteyttä', 'Nimi', 'Puhelin', '##nu', '##mer', '##o', 'Sähköposti', 'Yritys', 'Viesti', 'Sul', '##je', 'x', 'Sip', '##ari', '##la', 'Sip', '##ari', '##la', 'Oy', 'on', 'palvele', '##va', 'puut', '##oimittaja', 'ja', 'pinta', '##käsittelyn', 'mestari', '.', 'Tarjoamme', 'asiakkaille', '##mme', 'tuotteita', ',', 'ratkaisuja', 'ja', 'palveluja', 'ulko', '##ver', '##hoi', '##luu', '##n', ',', 'sisus', '##tukseen', ',', 'sauna', '##tiloihin', 'sekä', 'pihalle', '.', 'Inno', '##st', '##umme', 'puun', 'mahdollis', '##uuksista', 'ja', 'kehit', '##ämme', 'jatkuvasti', 'uusia', 'tapoja', 'hyödyntää', 'puuta', 'rakentamisessa', 'ja', 'sisusta', '##misessa', '.', 'Vastuu', '##llisuus', 'Sip', '##ari', '##la', 'Oy', 'on', 'perinteitä', 'kunnioitta', '##va', 'puun', '##jal', '##ostus', '##teollisuuden', 'perhe', '##yritys', ',', 'joka', 'ottaa', 'toiminnassa', '##an', 'huomioon', 'vastuu', '##llisuuden', '.', 'Tö', '##ihin', 'Sip', '##ari', '##laan', 'Ammattila', '##isille', 'Sip', '##ari', '##la', 'Oy', 'on', 'kokenut', 'ja', 'palvele', '##va', 'puutu', '##otte', '##iden', 'valmistaja', '.', 'Meillä', 'on', 'kokemusta', 'puun', 'käytöstä', 'vaativ', '##issa', 'arkkitehti', '##koh', '##teissa', 'ja', 'unelmien', 'koti', '##en', 'toteuttamisesta', '.', 'Tarjoamme', 'arkkitehti', '##en', 'ja', 'projektia', '##sia', '##kkaiden', 'käyttöön', 'oman', 'asiantunte', '##mu', '##ksemme', 'puutu', '##otte', '##ista', '.', 'Suorit', '##usta', '##so', '-', 'ilmoitukset', 'Sip', '##ari', '##lan', 'paneel', '##it', 'CE', '-', 'merkitään', 'yhtenä', '##iste', '##tyn', 'massiiv', '##ipu', '##up', '##ane', '##eleita', 'ja', '-', 'verho', '##uksia', 'koskevan', 'eurooppalaisen', 'standardin', 'EN', '14', '##9', '##15', 'mukaan', '.', 'Sisu', '##st', '##us', '##li', '##sto', '##ille', 'ei', 'ole', 'CE', '-', 'merkintä', 'vaatimusta', 'Euroopan', 'Unionin', 'toimesta', '.', 'Sisu', '##st', '##usko', '##ht', '##eet', 'Sip', '##ari', '##la', 'Oy', 'on', 'luonut', 'kattavan', 'sisustus', '##pane', '##ele', '##iden', 'mallist', '##on', ',', 'jonka', 'avulla', 'sisusta', '##minen', 'on', 'helppoa', 'ja', 'täynnä', 'mahdollisuuksia', '.', 'Väri', '##en', ',', 'pin', '##tojen', 'ja', 'muoto', '##jen', 'yhdistä', '##misessä']\n",
            "['Tässä', '[', '[', 'Ortodoks', '##inen', 'seminaari', '(', 'Joensuu', ')', '[UNK]', 'ortodoksisen', 'seminaarin', ']', ']', 'kirkossa', 'Joensuussa', 'joulu', 'on', 'kuvattu', 'suureen', 'maala', '##ukseen', 'kattoon', ',', 'tuonne', 'pääni', 'yläpuolelle', '.', 'Kuvassa', 'on', 'useita', 'eri', 'asioita', ',', 'jotka', 'kaipaavat', 'selitystä', ':', 'Tässä', '[', '[', 'Ortodoks', '##inen', 'seminaari', '(', 'Joensuu', ')', '[UNK]', 'ortodoksisen', 'seminaarin', ']', ']', 'kirkossa', 'Joensuussa', 'joulu', 'on', 'kuvattu', 'suureen', 'maala', '##ukseen', 'kattoon', ',', 'tuonne', 'pääni', 'yläpuolelle', '.', 'Kuvassa', 'on', 'useita', 'eri', 'asioita', ',', 'jotka', 'kaipaavat', 'selitystä', ':', '-', '[', '[', 'En', '##keli', '[UNK]', 'En', '##kel', '##eistä', ']', ']', 'toinen', 'ylistää', 'ihmiseksi', 'tullutta', 'Jumalaa', 'ja', 'toinen', 'julistaa', 'ilo', '##sanom', '##aa', 'paimen', '##elle', '.', 'Pai', '##menet', 'taas', 'edustavat', 'tavallista', 'väkeä', 'tai', 'pikemminkin', 'eräällä', 'tavoin', 'yhteiskunnan', 'reunalla', ',', 'ellei', 'peräti', 'ulkopuolella', 'olevia', '.', 'Heistä', 'toinen', 'liittyy', 'enkeli', '##en', 'ylis', '##tykseen', 'paimen', '##hui', '##lu', '##lla', '.', '+', '[', '[', 'En', '##keli', '[UNK]', 'En', '##kel', '##eistä', ']', ']', 'toinen', 'ylistää', 'ihmiseksi', 'tullutta', 'Jumalaa', 'ja', 'toinen', 'julistaa', 'ilo', '##sanom', '##aa', 'paimen', '##elle', '.', '[', '[', 'Kristuksen', 'nähneet', 'paimen', '##et', '[UNK]', 'Pai', '##menet', ']', ']', 'taas', 'edustavat', 'tavallista', 'väkeä', 'tai', 'pikemminkin', 'eräällä', 'tavoin', 'yhteiskunnan', 'reunalla', ',', 'ellei', 'peräti', 'ulkopuolella', 'olevia', '.', 'Heistä', 'toinen', 'liittyy', 'enkeli', '##en', 'ylis', '##tykseen', 'paimen', '##hui', '##lu', '##lla', '.', '-', 'Joose', '##f', 'on', 'sivussa', '.', 'Hän', 'epäilee', 'ja', 'on', 'itse', 'asiassa', 'pah', '##olaisen', 'kiusa', '##ttava', '##na', '.', 'Kun', 'ensimmäinen', 'Aada', '##m', 'tehtiin', 'maan', 'tom', '##usta', ',', 'Kristus', 'on', 'uusi', 'Aada', '##m', 'ja', 'uusi', 'luomu', '##s', ',', 'joka', 'syntyy', 'taivaasta', ',', '[', '[', 'Isä', '(', 'Jumala', ')', '[UNK]', 'Isä', '##stä', ']', ']', 'ja']\n",
            "Segment ids:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpqqKGfeQdl",
        "colab_type": "text"
      },
      "source": [
        "Extract one hot label encodings from the truncated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5SgLu9xePxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "Y = train.drop(['label','text', 'data'], axis=1) # label\n",
        "\n",
        "# dev\n",
        "y = dev.drop(['label','text', 'data'], axis=1)\n",
        "\n",
        "# test\n",
        "true_labels= test.drop(['label','text', 'data'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ze-mp5bpO5",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyz0YK_kbtBU",
        "colab_type": "text"
      },
      "source": [
        "This is the same as in milestone 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwQmd2F_boLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# search for optimal EPOCHS, LEARNING_RATE and BATCH_SIZE for serious training!\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 16\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 8 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sypxw1aJbyix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(train_['text']),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaQNy6Uptzw",
        "colab_type": "text"
      },
      "source": [
        "### Wrap the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjV59es9jyX6",
        "colab_type": "text"
      },
      "source": [
        "Steps needed for loading pretrained BERT model have been taken during milestone 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAyXXiqCpeG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8c930989-21c9-460b-aa32-c7f4a4bf772d"
      },
      "source": [
        "pretrained_model.inputs"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Input-Token:0' shape=(?, 250) dtype=float32>,\n",
              " <tf.Tensor 'Input-Segment:0' shape=(?, 250) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_iquZgFqmZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "41d37bcb-b693-449a-a022-56ab17a8e83d"
      },
      "source": [
        "pretrained_model.output"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Encoder-12-FeedForward-Norm/add_1:0' shape=(?, 250, 768) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJsIwzVmukO7",
        "colab_type": "text"
      },
      "source": [
        "Those dimensions are (minibatch-size, sequence-length, hidden-dim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS7bRChHubSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6a950e6d-b3b4-4ae0-9a26-6ea67e65d44b"
      },
      "source": [
        "# model.outputs is a list, here with a single item. Here\n",
        "# pretrained_model.outputs[0] just grabs that item (the output tensor).\n",
        "# Indxing that tensor with [:,0] gives the first position in the sequence\n",
        "# for all elements in the batch (the `:`).\n",
        "\n",
        "bert_out = pretrained_model.outputs[0][:,0]\n",
        "\n",
        "print(bert_out)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice_2:0\", shape=(?, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvbwRaMgE35m",
        "colab_type": "text"
      },
      "source": [
        "Wrap the pretrained model. **In multi-label classification instead of softmax(), we use sigmoid() to get the probabilities.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P3irmGsE2wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labels = len(mlb.classes_) # size of the output layer\n",
        "\n",
        "out = Dense(num_labels, activation='sigmoid')(bert_out) # by calling the output layer we end up calling all the other layers (the model)\n",
        "model = Model(                                          # sigmoid gives us independent probabilities for each class\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9_cBMhQW-df",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "422c079c-c5c6-4680-a12c-75f56b8686a6"
      },
      "source": [
        "model.output"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense_2/Sigmoid:0' shape=(?, 42) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OouJ_0jzYw1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy', # one hot encoded labels and multilabel!\n",
        "    metrics=['categorical_accuracy']\n",
        ")\n",
        "\n",
        "# An important choice to make is the loss function. We use the binary_crossentropy loss and not the usual in multi-class classification \n",
        "# used categorical_crossentropy loss. This might seem unreasonable, but we want to penalize each output node independently. So we pick \n",
        "# a binary loss and model the output of the network as a independent Bernoulli distributions per label.\n",
        "# source: https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIeGmqi-iXGL",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSwPhGPMZFqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "108a11d5-976d-4007-fc0a-e56f646c5405"
      },
      "source": [
        "stop_cb = EarlyStopping(monitor = 'val_categorical_accuracy', patience=8, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X,\n",
        "    Y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x, y)\n",
        ")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1000 samples, validate on 756 samples\n",
            "Epoch 1/16\n",
            "1000/1000 [==============================] - 62s 62ms/sample - loss: 0.2816 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 2/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2813 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 3/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2818 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 4/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2810 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 5/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2821 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 6/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2816 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 7/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2819 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 8/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2808 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 9/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2812 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 10/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2820 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 11/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2809 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 12/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2814 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 13/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2814 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 14/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2827 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 15/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2814 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n",
            "Epoch 16/16\n",
            "1000/1000 [==============================] - 47s 47ms/sample - loss: 0.2811 - categorical_accuracy: 0.2880 - val_loss: 0.2906 - val_categorical_accuracy: 0.3056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtoBHqFyiM7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb8tvqruaIJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_inp) # np.argmax gives the index which has the highest value e.g. class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNLiSNOrj3f_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "483481f1-4634-4c1a-dd96-5b966393b28a"
      },
      "source": [
        "print(predictions) # what is wrong here! same prediction for all!!!!\n",
        "len(predictions)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00497639 0.09345984 0.01151597 ... 0.02556935 0.09179869 0.03212136]\n",
            " [0.00497639 0.09345984 0.01151597 ... 0.02556935 0.09179869 0.03212136]\n",
            " [0.00497639 0.09345981 0.01151597 ... 0.02556938 0.09179866 0.03212136]\n",
            " ...\n",
            " [0.00497642 0.09345984 0.011516   ... 0.02556932 0.09179872 0.03212136]\n",
            " [0.00497648 0.09345981 0.01151597 ... 0.02556932 0.09179869 0.03212136]\n",
            " [0.00497642 0.09345981 0.01151597 ... 0.02556932 0.09179869 0.03212136]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X7dkit5l-ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "79b3dd9d-5f83-4c66-8a6f-087bae2b9f57"
      },
      "source": [
        "print(sum(predictions[0]))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.789755791425705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yauRbUmrRwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "39c9be1e-f4ce-4fab-aa42-847396036a56"
      },
      "source": [
        "# Should we set some treshold to accept label?\n",
        "# How to select an appropriate threshold for the model? This is normally done by optimizing the threshold on a holdout set. \n",
        "# According to https://www.depends-on-the-definition.com/unet-keras-segmenting-images/ No instrucion provided how to do it!\n",
        "\n",
        "preds = (predictions > 0.08).astype(np.uint8) # set the limit for reasonable/acceptable probability for belonging to a class\n",
        "preds [1]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0],\n",
              "      dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l49oGfGjlys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "b1e557d0-ea7d-4195-b2bc-bbedefea6f26"
      },
      "source": [
        "predicted_labels = mlb.inverse_transform(preds)\n",
        "for pred in predicted_labels[:10]: # WHAT!?!?!?\n",
        "  print(pred)\n",
        "print()\n",
        "print(\"Classification accuracy: \", round(accuracy_score(true_labels, preds)*100,1), \"percent\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "('CB', 'DF', 'DS', 'DT', 'EN', 'HI', 'ID', 'IG', 'IN', 'MT', 'NA', 'NE', 'OA', 'OB', 'OP', 'OS', 'PB', 'RS', 'RV', 'SR')\n",
            "\n",
            "Classification accuracy:  0.0 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1bZmgfNnWlm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 3.2: Model comparison\n",
        "Compare the results of these two classifiers. Do the two models predict in the same way? Analyze the predictions in terms of label-specific differences.\n"
      ]
    }
  ]
}